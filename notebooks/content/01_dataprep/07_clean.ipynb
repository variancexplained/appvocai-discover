{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "\n",
    "if \"jbook\" in os.getcwd():\n",
    "    os.chdir(os.path.abspath(os.path.join(\"../..\")))\n",
    "FORCE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "In the previous section, we provided an overview of the AppVoCAI dataset, evaluating its structure, features, and validity. This section is about duplicate deleting, language filtering, text encoding, artifact removing, PII masking, character normalizing data cleaning. By removing or masking artifacts that could compromise the integrity and reliability of downstream analysis, we ensure that the dataset will support nuanced, rich, and insightful discovery and model performance.\n",
    "\n",
    "## Data Cleaning Context\n",
    "Downstream tasks such as sentiment analysis, classification, text summarization, and generation will leverage transformer-based models (like BERT, RoBERTa, and GPT), which have proven to be highly robust in handling various data anomalies and linguistic variations. Unlike traditional models such as Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM) networks, and Convolutional Neural Networks (CNNs)—which process data sequentially—transformers operate on entire sequences in parallel. This allows them to capture long-range dependencies and uncover subtle nuances and contextual relationships in language with near-human precision.\n",
    "\n",
    "However, research demonstrates that preprocessing can still significantly improve the performance of transformer models {cite}`siinoTextPreprocessingStill2024`. This synergy between preprocessing and model architecture suggests that a balanced approach is ideal. Our data preparation methodology focuses on addressing critical data quality issues that could undermine the integrity of downstream analyses, while preserving the text as close to its original form as possible. By adopting this conservative approach, we tackle key issues without sacrificing the nuance and representativeness of the data, ensuring the models are presented with rich, authentic input.\n",
    "\n",
    "## Data Cleaning Key Evaluation Questions (KEQs)\n",
    "Although, this data cleaning approach comprises many of the preprocessing techniques commonly found in the literature {cite}`symeonidisComparativeEvaluationPreprocessing2018`, the following data cleaning approach is motivated by three guiding questions.\n",
    "\n",
    "1. What’s essential to remove, and what can be left intact to preserve meaning?\n",
    "2. How do we best preserve text richness and nuance?\n",
    "3. How can the data cleaning process best exploit model strengths towards optimal model performance?\n",
    "\n",
    "These Key Evaluation Questions (KEQs) crystallized our approach which balanced data quality with model sophistication.\n",
    "\n",
    "## Data Cleaning Strategy\n",
    "The following describes our data cleaning process and steps, executed in the order listed. We begin with 'safe' techniques that carry minimal risk of compromising downstream cleaning tasks. For instance, UTF-8 encoding can impact the accuracy of language detection algorithms, especially if characters carry language-specific information. Removing special characters may compromise the detection of Personally Identifiable Information (PII) such as URLs and email addresses. As the process progresses, steps carry a greater impact on the data, its expressiveness, and representation.\n",
    "\n",
    "Our minimalist, *leave-as-is* approach can depart from data cleaning orthodoxy and standard practice. In such cases, we are transparent with our rationale. With that, our process is as follows:\n",
    "\n",
    "### Basic Interventions\n",
    "These measures ensure storage, I/O, and memory efficiency and provide reliable access to the data.\n",
    "\n",
    "1. **Type Casting**: Data types are cast for optimal storage efficiency, memory utilization, and processing speed within the `pandas` framework. This ensures efficient handling and manipulation of the data throughout the pipeline.\n",
    "2. **Remove Newlines from Review Text**: Newline characters in text can cause errors and unpredictable behavior in I/O operations and parsing within `pandas` DataFrames. Our first task removes these artifacts to ensure data stability and prevent disruptions in subsequent processing steps.\n",
    "\n",
    "### Privacy\n",
    "We ensure personally identifiable information is masked to protect data privacy.\n",
    "\n",
    "3. **URL Masking**: Using regular expressions, we find and replace URLs with the placeholder '[URL]'. This allows us to uphold privacy standards while retaining the lexical context of the text.\n",
    "4. **Email Address Masking**: Email addresses are similarly masked with '[EMAIL]', ensuring that sensitive contact information is not exposed.\n",
    "5. **Phone Number Masking**: Phone numbers are detected and masked with '[PHONE]' using regular expressions. These masking protocols enable content analysis while meeting data privacy obligations and minimizing re-identification risks.\n",
    "\n",
    "### Noise Removal\n",
    "We remove or normalize artifacts that distort content or add little value to understanding sentiment, intent, or behavior.\n",
    "\n",
    "6. **Control Characters**: We remove non-printable characters from the Unicode and ASCII character sets that are used to control text flow or hardware devices (e.g., newline, tab, or carriage return). These characters have no analytical value and can interfere with text processing.\n",
    "7. **Accents and Diacritics**: We normalize accented characters (e.g., converting `é` to `e`) to reduce unnecessary text variation, which simplifies analysis without compromising the meaning of the content.\n",
    "8. **HTML Characters**: Common in scraped data, HTML entities (e.g., `&amp;`, `&#39;`) are removed as they do not convey meaningful content. This ensures that the text is clean and ready for analysis.\n",
    "9. **Excessive Whitespace**: Extra whitespace is condensed into a single space to ensure text consistency and improve readability. This also facilitates more efficient text parsing and analysis.\n",
    "\n",
    "### Language and Expression\n",
    "These steps enhance linguistic consistency and preserve the expressive elements of the text.\n",
    "\n",
    "10. **Remove Non-English Text**: We systematically identify and remove non-English app names and reviews to maintain linguistic uniformity within the dataset, which is crucial for consistent language-based analysis.\n",
    "11. **Elongation Handling**: Elongated words (e.g., \"soooo\") convey emphasis in informal text, which is valuable for sentiment analysis. We use a threshold approach to limit characters that appear four or more times consecutively to a maximum of three (e.g., \"soooo\" becomes \"sooo\"), preserving emphasis while maintaining readability.\n",
    "12. **Special Characters**: Excessive special characters can indicate SPAM, emotional intensity, or nonsensical content. We apply a threshold: if special characters make up more than 30% of the review text, the review is removed. This helps maintain the quality and relevance of the dataset.\n",
    "\n",
    "### Data Integrity\n",
    "We implement measures to ensure the integrity and uniqueness of the data.\n",
    "\n",
    "13. **Review ID Deduplication**: For duplicate review IDs, our policy for retention is based on several criteria: the most recent review date is prioritized, followed by the longest review text, and, if all else is equal, the review with the lowest row index is retained. This ensures that we keep the most informative and relevant reviews.\n",
    "\n",
    "### Encoding\n",
    "Encoding steps are performed last to standardize the text format while preserving language-specific features throughout earlier processing.\n",
    "\n",
    "14. **Unicode Normalization**: We normalize the text using `unicodedata.normalize` to standardize characters and ensure consistency, particularly for languages where characters have multiple valid representations.\n",
    "15. **UTF-8 Encoding**: After normalization, we encode the text in UTF-8 format. This step converts the text into a consistent byte representation, suitable for storage or transmission, and ensures proper character encoding.\n",
    "16. **Non-ASCII Character Removal**: Finally, we remove non-ASCII characters from the review text. This simplifies the text and ensures compatibility with systems that may not handle non-ASCII characters well.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Cleaning Techniques Not Implemented\n",
    "In natural language processing (NLP), text cleaning measures such as lower-casing, contraction and abbreviation expansion, spelling correction, and the removal of emoticons, emojis, and other artifacts are considered standard practice. **However, our data cleaning strategy deliberately omits certain conventional techniques to maximize the quality and authenticity of app reviews for aspect-based sentiment analysis (ABSA).** We focus on leveraging the strengths of modern transformer models to handle natural language variations effectively.\n",
    "\n",
    "### Emoticons and Emojis Conversion\n",
    "Emoticons and emojis are frequently used in app reviews to express emotions, sentiments, or nuanced meanings that words alone may not fully capture. In the context of aspect-based sentiment analysis (ABSA) for app reviews, the decision to retain or convert emojis plays a significant role in shaping the accuracy and efficiency of downstream natural language processing (NLP) tasks. Given that modern transformer models are highly adept at handling a wide variety of tokens, including emojis, we take a **leave emojis as-is** approach during data cleaning unless the number of special characters exceeds a threshold. This strategy leverages the inherent strengths of transformer models in dealing with diverse text elements while maintaining the integrity and natural flow of user-generated content.\n",
    "\n",
    "#### Justification:\n",
    "1. **Transformer Models’ Capabilities**:\n",
    "   Transformer-based models, such as BERT and GPT, employ advanced tokenization techniques like Byte Pair Encoding (BPE) and WordPiece, which are designed to recognize emojis as distinct tokens. These models are pretrained on large corpora that include emojis, making them well-equipped to process and learn from the contextual meanings that emojis convey. Therefore, emojis can be treated as standard tokens without the need for conversion, preserving computational efficiency while ensuring sentiment and meaning are captured.\n",
    "\n",
    "2. **Preservation of Natural Language Flow**:\n",
    "   App reviews are inherently informal and often rely on emojis to express sentiment or emphasis succinctly. By retaining emojis in their original form, the natural tone and expressiveness of the reviews are preserved, allowing the model to analyze real-world feedback in its most authentic form. Converting emojis to text (e.g., \"😊\" to \"happy face\") could introduce unnecessary verbosity and disrupt the concise style typical in user reviews, potentially leading to a loss of nuance in the analysis.\n",
    "\n",
    "3. **Efficiency in ABSA**:\n",
    "   In ABSA, the focus is on extracting sentiment related to specific app features or aspects. Since transformer models can already interpret the sentiment behind common emojis based on the surrounding context, converting them to text is unlikely to provide substantial improvements in analysis. For example, the positive sentiment conveyed by \"😊\" or the negative tone of \"😡\" are naturally inferred from their usage alongside relevant aspects of the review (e.g., \"support team\" or \"performance\"). Retaining emojis allows the model to focus on both the aspect and the associated sentiment, without needing additional processing steps.\n",
    "\n",
    "For app reviews processed in transformer-based models, our strategy is to leave emojis as-is during data cleaning. This approach capitalizes on the transformer’s robust tokenization and contextual learning capabilities while preserving the authentic, emotion-rich nature of user feedback. By retaining emojis in their original form, the model can efficiently capture sentiment and meaning without the unnecessary complexity introduced by conversion, ensuring both accuracy and interpretability in aspect-based sentiment analysis.\n",
    "\n",
    "### Spelling, Abbreviations, and Acronyms in App Reviews\n",
    "In processing user-generated content such as app reviews, the handling of spelling variations, abbreviations, and acronyms is a critical component of the data cleaning strategy. **Given the powerful contextual understanding of transformer-based models, a leave-as-is approach for both spelling errors and abbreviations/acronyms is taken.** This approach takes advantage of the transformer’s strengths in dealing with noisy text data while preserving the natural language flow and intent found in app reviews.\n",
    "\n",
    "#### Justification:\n",
    "1. **Transformer Models' Robustness to Spelling Variations and Abbreviations**:\n",
    "   Transformer models, particularly those using **Byte Pair Encoding (BPE)** or **WordPiece** tokenization, are designed to handle subword units, allowing them to process incomplete or misspelled words as well as abbreviations and acronyms effectively. By breaking down words and abbreviations into smaller components, transformers can use surrounding context to infer meaning. For example:\n",
    "   - **Misspelled words** such as \"exellent\" will be split into recognizable subwords like \"excel\" and \"ent,\" allowing the model to infer the correct meaning from context.\n",
    "   - **Abbreviations or acronyms**, such as \"AI\" (Artificial Intelligence) or \"UX\" (User Experience), are commonly recognized by transformers due to their pretraining on vast datasets that include these forms of shorthand. The context of the sentence often clarifies the meaning, rendering explicit expansion unnecessary.\n",
    "\n",
    "2. **Pretraining on Diverse, Noisy Data**:\n",
    "   Transformer models like **BERT** and **GPT** are pretrained on large, diverse datasets that encompass various forms of natural language, including informal writing styles with abbreviations, slang, and misspellings. These models are already equipped to understand user-generated content that is not perfectly clean, making it redundant to introduce correction or expansion processes that could introduce unnecessary complexity without providing significant performance gains.\n",
    "\n",
    "3. **Preserving Authenticity and User Intent**:\n",
    "   User-generated app reviews often reflect natural, informal communication, where spelling variations, abbreviations, and acronyms contribute to the user’s tone and intent. Correcting or expanding these forms may risk altering the tone or authenticity of the review, particularly in cases where users employ creative or emphatic language. For instance, abbreviating \"awesome\" as \"awsome\" or \"UX\" as shorthand reflects natural usage, and correcting these may remove key aspects of user expression that are crucial for sentiment analysis.\n",
    "\n",
    "4. **Focus on Context and Meaning over Perfection**:\n",
    "   The strength of transformers lies in their **contextual learning**—the ability to understand the meaning of words in relation to their surrounding context. In sentiment analysis, particularly for **aspect-based sentiment analysis (ABSA)**, the focus is on extracting user opinions related to specific app features, not on perfecting the language. Transformers excel in this task by interpreting the overall sentiment, even when the input includes abbreviations or spelling errors. Expanding abbreviations or correcting spelling would likely introduce marginal improvements, if any, at the cost of altering the text’s natural flow.\n",
    "\n",
    "For app reviews analyzed using transformer-based models, the **leave-as-is approach** for spelling correction, abbreviations, and acronyms is the most effective strategy. This approach leverages the strengths of transformer models—particularly their ability to tokenize subword units and learn from context—while preserving the natural, authentic nature of user-generated content. By maintaining spelling variations in the dataset, the data cleaning process remains streamlined and avoids unnecessary complexity, allowing the model to focus on capturing meaningful sentiment and insights without sacrificing user intent or tone.\n",
    "\n",
    "---\n",
    "\n",
    "With our data cleaning rationale established, we now move on to the implementation: the following code runs the entire data cleaning pipeline, automating the steps described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from discover.container import DiscoverContainer\n",
    "from discover.infra.config.flow import FlowConfigReader\n",
    "from discover.flow.data_prep.clean.stage import DataCleaningStage\n",
    "from discover.core.flow import PhaseDef, DataPrepStageDef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "## Dependency Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "container = DiscoverContainer()\n",
    "container.init_resources()\n",
    "container.wire(\n",
    "    modules=[\n",
    "        \"discover.flow.data_prep.stage\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Pipeline\n",
    "This code snippet demonstrates how to set up and run a data cleaning stage based on a configuration obtained from a configuration reader. Here’s a breakdown of the steps:\n",
    "\n",
    "1. **Obtain the Configuration**: \n",
    "   - A `FlowConfigReader` instance (`reader`) is used to load the configuration. \n",
    "   - The `get_stage_config` method retrieves the stage configuration for data preparation and cleaning.\n",
    "\n",
    "2. **Build the Data Cleaning Stage**:\n",
    "   - The `DataCleaningStage.build` method initializes the data cleaning stage with the provided `stage_config`. Setting `force=False` ensures the stage is only built if the endpoint doesn't already exists.\n",
    "\n",
    "3. **Run the Data Cleaning Stage**:\n",
    "   - Finally, the `run` method executes the data cleaning stage and returns an `asset_id`, which likely identifies the cleaned dataset or asset generated by this stage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# ============================================================================== #\n",
      "#                              Data Cleaning Stage                               #\n",
      "# ============================================================================== #\n",
      "\n",
      "\n",
      "\n",
      "                                CastDataTypeTask                                \n",
      "                                ----------------                                \n",
      "                          Start Datetime | Sat, 09 Nov 2024 18:28:52\n",
      "                       Complete Datetime | Sat, 09 Nov 2024 18:30:24\n",
      "                                 Runtime | 1.0 minutes and 32.65 seconds\n",
      "                                 Summary | Modified 0 cells.\n",
      "\n",
      "\n",
      "                               RemoveNewlinesTask                               \n",
      "                               ------------------                               \n",
      "                          Start Datetime | Sat, 09 Nov 2024 18:30:26\n",
      "                       Complete Datetime | Sat, 09 Nov 2024 18:31:42\n",
      "                                 Runtime | 1.0 minutes and 15.61 seconds\n",
      "                                 Summary | Modified 1693576 cells.\n",
      "\n",
      "\n",
      "                                  URLMaskTask                                   \n",
      "                                  -----------                                   \n",
      "                          Start Datetime | Sat, 09 Nov 2024 18:31:43\n",
      "                       Complete Datetime | Sat, 09 Nov 2024 18:35:56\n",
      "                                 Runtime | 4.0 minutes and 13.15 seconds\n",
      "                                 Summary | Modified 428184 cells.\n",
      "\n",
      "\n",
      "                              EmailAddressMaskTask                              \n",
      "                              --------------------                              \n",
      "                          Start Datetime | Sat, 09 Nov 2024 18:36:01\n",
      "                       Complete Datetime | Sat, 09 Nov 2024 18:38:22\n",
      "                                 Runtime | 2.0 minutes and 21.36 seconds\n",
      "                                 Summary | Modified 2 cells.\n",
      "\n",
      "\n",
      "                              PhoneNumberMaskTask                               \n",
      "                              -------------------                               \n",
      "                          Start Datetime | Sat, 09 Nov 2024 18:38:28\n",
      "                       Complete Datetime | Sat, 09 Nov 2024 18:41:45\n",
      "                                 Runtime | 3.0 minutes and 17.33 seconds\n",
      "                                 Summary | Modified 8247 cells.\n",
      "\n",
      "\n",
      "                             RemoveControlCharsTask                             \n",
      "                             ----------------------                             \n",
      "                          Start Datetime | Sat, 09 Nov 2024 18:41:50\n",
      "                       Complete Datetime | Sat, 09 Nov 2024 18:43:45\n",
      "                                 Runtime | 1.0 minutes and 55.76 seconds\n",
      "                                 Summary | Modified 4650 cells.\n",
      "\n",
      "\n",
      "                               RemoveAccentsTask                                \n",
      "                               -----------------                                \n",
      "                          Start Datetime | Sat, 09 Nov 2024 18:43:50\n",
      "                       Complete Datetime | Sat, 09 Nov 2024 18:47:05\n",
      "                                 Runtime | 3.0 minutes and 15.33 seconds\n",
      "                                 Summary | Modified 470262 cells.\n",
      "\n",
      "\n",
      "                              RemoveHTMLCharsTask                               \n",
      "                              -------------------                               \n",
      "                          Start Datetime | Sat, 09 Nov 2024 18:47:10\n",
      "                       Complete Datetime | Sat, 09 Nov 2024 18:49:03\n",
      "                                 Runtime | 1.0 minutes and 53.12 seconds\n",
      "                                 Summary | Modified 296 cells.\n",
      "\n",
      "\n",
      "                         RemoveExcessiveWhitespaceTask                          \n",
      "                         -----------------------------                          \n",
      "                          Start Datetime | Sat, 09 Nov 2024 18:49:08\n",
      "                       Complete Datetime | Sat, 09 Nov 2024 18:51:13\n",
      "                                 Runtime | 2.0 minutes and 4.8 seconds\n",
      "                                 Summary | Modified 2872579 cells.\n",
      "\n",
      "\n",
      "                              RemoveNonEnglishTask                              \n",
      "                              --------------------                              \n",
      "                          Start Datetime | Sat, 09 Nov 2024 18:51:18\n",
      "                       Complete Datetime | Sat, 09 Nov 2024 18:56:10\n",
      "                                 Runtime | 4.0 minutes and 52.34 seconds\n",
      "                                 Summary | Removed 612595 rows.\n",
      "\n",
      "\n",
      "                              RemoveNonEnglishTask                              \n",
      "                              --------------------                              \n",
      "                          Start Datetime | Sat, 09 Nov 2024 18:56:12\n",
      "                       Complete Datetime | Sat, 09 Nov 2024 19:34:43\n",
      "                                 Runtime | 38.0 minutes and 30.55 seconds\n",
      "                                 Summary | Removed 560974 rows.\n",
      "\n",
      "\n",
      "                        RemoveExcessiveSpecialCharsTask                         \n",
      "                        -------------------------------                         \n",
      "                          Start Datetime | Sat, 09 Nov 2024 19:34:45\n",
      "                       Complete Datetime | Sat, 09 Nov 2024 19:36:39\n",
      "                                 Runtime | 1.0 minutes and 54.48 seconds\n",
      "                                 Summary | Removed 115429 rows.\n",
      "\n",
      "\n",
      "                          RemoveDuplicateReviewIdTask                           \n",
      "                          ---------------------------                           \n",
      "                          Start Datetime | Sat, 09 Nov 2024 19:36:41\n",
      "                       Complete Datetime | Sat, 09 Nov 2024 19:38:47\n",
      "                                 Runtime | 2.0 minutes and 5.93 seconds\n",
      "                                 Summary | Removed 110 rows.\n",
      "\n",
      "\n",
      "                                DelongationTask                                 \n",
      "                                ---------------                                 \n",
      "                          Start Datetime | Sat, 09 Nov 2024 19:38:49\n",
      "                       Complete Datetime | Sat, 09 Nov 2024 19:43:56\n",
      "                                 Runtime | 5.0 minutes and 7.5 seconds\n",
      "                                 Summary | Modified 742021 cells.\n",
      "\n",
      "\n",
      "                            NormalizeUnicodeTextTask                            \n",
      "                            ------------------------                            \n",
      "                          Start Datetime | Sat, 09 Nov 2024 19:44:13\n",
      "                       Complete Datetime | Sat, 09 Nov 2024 19:47:54\n",
      "                                 Runtime | 3.0 minutes and 40.6 seconds\n",
      "                                 Summary | Modified 5518735 cells.\n",
      "\n",
      "\n",
      "                               VerifyEncodingTask                               \n",
      "                               ------------------                               \n",
      "                          Start Datetime | Sat, 09 Nov 2024 19:48:02\n",
      "                       Complete Datetime | Sat, 09 Nov 2024 19:51:07\n",
      "                                 Runtime | 3.0 minutes and 5.16 seconds\n",
      "                                 Summary | Modified 0 cells.\n",
      "\n",
      "\n",
      "                            RemoveNonASCIICharsTask                             \n",
      "                            -----------------------                             \n",
      "                          Start Datetime | Sat, 09 Nov 2024 19:51:12\n",
      "                       Complete Datetime | Sat, 09 Nov 2024 19:54:41\n",
      "                                 Runtime | 3.0 minutes and 28.69 seconds\n",
      "                                 Summary | Modified 0 cells.\n",
      "\n",
      "\n",
      "                              Data Cleaning Stage                               \n",
      "                              ===================                               \n",
      "                           Stage Started | Sat, 09 Nov 2024 18:28:16\n",
      "                         Stage Completed | Sat, 09 Nov 2024 19:57:04\n",
      "                           Stage Runtime | 1.0 hours 28.0 minutes and 47.91 seconds\n",
      "\n",
      "\n",
      "# ============================================================================ #\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obtain the configuration\n",
    "reader = FlowConfigReader()\n",
    "stage_config = reader.get_stage_config(\n",
    "    phase=PhaseDef.DATAPREP, stage=DataPrepStageDef.CLEAN\n",
    ")\n",
    "\n",
    "# Build and run Data Ingestion Stage\n",
    "stage = DataCleaningStage.build(stage_config=stage_config, force=FORCE)\n",
    "asset_id = stage.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing\n",
    "With data cleaning complete, we have addressed the critical anomalies that could impact downstream tasks, such as duplicates, non-ASCII text, and unwanted control characters, while preserving the natural richness and variability of user-generated content. This approach aligns with the guiding Key Evaluation Questions (KEQs) to ensure both data quality and the nuanced representation required for transformer-based models, which will be central to the AppVoCAI analyses. By addressing essential data quality issues without over-processing, we optimize the dataset to leverage transformers' strengths in handling subtle linguistic variations and context.\n",
    "\n",
    "In the next section, we turn our attention to data enrichment. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appvocai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
