{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if \"jbook\" in os.getcwd():\n",
    "    os.chdir(os.path.abspath(os.path.join(\"../..\")))\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AppVoCAI Data Quality Analysis\n",
    "In this section, we evaluate the overall quality of the AppVoCAI dataset across key dimensions: **completeness**, **validity**, **relevance**, **uniqueness**, and **data privacy**. Our analysis unfolds over the following three subsections.\n",
    "\n",
    "1. **Data Quality Scoring Framework**: We establish a methodology for quantifying data quality by integrating these dimensions into an overall quality score.\n",
    "3. **Data Quality Analysis**: We examine overall data quality provide a detailed breakdown of each data quality dimension, highlighting strengths and areas for improvement.\n",
    "3. **Data Cleaning Implications**: We translate the insights from the quality assessment into actionable requirements for the data cleaning stage.\n",
    "\n",
    "This structured approach ensures that the AppVoCAI dataset is primed for downstream aspect-based sentiment analysis.\n",
    "\n",
    "## Data Quality Scoring Framework\n",
    "Integrating aspects of **completeness**, **validity**, **relevance**, **uniqueness**, and **privacy and compliance**, our composite **Quality Score** is computed as follows:\n",
    "\n",
    "$$\n",
    "\\text{Quality Score} = w_c \\times X_{\\text{Completeness}} + w_v \\times X_{\\text{Validity}} + w_r \\times X_{\\text{Relevance}} + w_u \\times X_{\\text{Uniqueness}} + w_p \\times X_{\\text{Data Privacy}}\n",
    "$$\n",
    "\n",
    "and our weights are as follows:\n",
    "\n",
    "| Dimension    | Variable | Weight | Rationale                                                                                      |\n",
    "|--------------|----------|--------|------------------------------------------------------------------------------------------------|\n",
    "| Completeness | $w_c$    | 0.25   | Ensures the dataset is representative and balanced, critical for unbiased   model training.    |\n",
    "| Validity     | $w_v$    | 0.25   | Guarantees the data conforms to expected formats and patterns, ensuring   reliable processing. |\n",
    "| Relevance    | $w_r$    | 0.2    | Ensures all text is English to prevent noise and confusion from   multilingual data.           |\n",
    "| Uniqueness   | $w_u$    | 0.15   | Removes duplicates to avoid overfitting and biased weightage toward   repeated samples.        |\n",
    "| Privacy      | $w_p$    | 0.15   | Ensures no PII, avoiding ethical and legal risks while maintaining   dataset integrity.        |\n",
    "\n",
    "\n",
    "### Completeness\n",
    "We evaluate **completeness** as the degree to which the dataset is representative, and contains all required data values. Our composite score is a weighted average of **Row Completeness**, **Category Class Balance**, and **Sentiment Class Balance**.\n",
    "\n",
    "$$\n",
    "\\text{Completeness Score} = w_r \\times X_{\\text{Row Completeness}} + w_c \\times X_{\\text{Category Class Balance}} + w_s \\times X_{\\text{Sentiment Class Balance}}\n",
    "$$\n",
    "\n",
    "where the weights are assigned as follows:\n",
    "\n",
    "| Component               | Variable | Weight | Rationale                                                                 |\n",
    "|-------------------------|----------|--------|---------------------------------------------------------------------------|\n",
    "| Row Completeness        | $w_r$    | 0.5    | Ensuring rows are complete is foundational to data quality and usability. |\n",
    "| Category Class Balance  | $w_c$    | 0.25   | Category balance ensures fair representation across different categories. |\n",
    "| Sentiment Class Balance | $w_s$    | 0.25   | Sentiment balance avoids bias and ensures equitable model training.       |\n",
    "\n",
    "Each component is defined as follows:\n",
    "\n",
    "#### Row Completeness\n",
    "Row completeness is the number of complete rows $N_c$ normalized by the number of rows in the dataset $N$.\n",
    "\n",
    "$$\\text{Row Completeness} = \\frac{N_c}{N}$$\n",
    "\n",
    "#### Category Class Balance\n",
    "We define category class balance in terms of the deviation of record counts in each category from the average of record counts in each category. \n",
    "$$\\text{Category Class Balance} = 1-\\frac{\\displaystyle\\sum_{i=1}^C{|n_i-\\bar{n_c}|}}{2\\times{N}}$$\n",
    "\n",
    "where:\n",
    "- $C$ is the number of categories,\n",
    "- $N$ is the number of observations in the dataset,\n",
    "- $n_i$ is the number of observations in category $i$, and \n",
    "- $\\bar{n_c} = \\frac{\\displaystyle\\sum_{i=1}^C n_i}{C}$ is the average of the number of rows in each category,\n",
    "\n",
    "#### Sentiment Class Balance \n",
    "Sentiment class balance reflects representation of sentiment classes relative to the average number of observations for each sentiment class. \n",
    "$$\\text{Sentiment Class Balance} = 1-\\frac{\\displaystyle\\sum_{i=1}^S{|n_i-\\bar{n_s}|}}{2\\times{N}}$$\n",
    "\n",
    "where:\n",
    "- $S$ is the number of sentiment classes,\n",
    "- $N$ is the number of observations in the dataset,\n",
    "- $n_i$ is the number of observations assigned sentiment $i$, and \n",
    "- $\\bar{n_s} = \\frac{\\displaystyle\\sum_{i=1}^S n_i}{S}$ is the average of the number of rows in each sentiment class\n",
    "\n",
    "### Validity\n",
    "**Validity** refers to the degree to which the data complies with pre-defined business rules, such as valid values, patterns, ranges, and types. Concretely, we define validity in terms of:\n",
    "\n",
    " - **Ratings**: Valid ratings are integers in [1,5]\n",
    " - **Category**:  The dataset categories are among the 14 valid categories included in this study.\n",
    " - **Review Date**: Review dates should be between January 1, 2020, and September of 2023. \n",
    " - **Review Text**: Valid review text is free of:\n",
    "    - control, HTML, accented, and diacritic characters, \n",
    "    - excessive special characters that comprise greater than 0.3 of all characters in a review,\n",
    "    - excessive sequence, word and phrase repetition \n",
    " \n",
    "Integrating these components, we define validity score as follows:\n",
    "\n",
    "$$\\text{Validity Score} = w_r \\times{\\frac{N_r}{N}} + w_c \\times{\\frac{N_c}{N}}+ w_d \\times{\\frac{N_d}{N}}+ w_t \\times{\\frac{N_t}{N}}$$  \n",
    "\n",
    "where:\n",
    "- $N_r$ are the number of rows with valid ratings, \n",
    "- $N_c$ are the observations with valid categories, \n",
    "- $N_d$ indicates review dates in range, \n",
    "- $N_t$ is the count of rows that pass the text data quality anomaly detection checks, and\n",
    "- $N$ is the number of observations in the dataset.\n",
    "\n",
    "Weights for each of the components are:\n",
    "\n",
    "| Dimension   | Variable | Weight | Rationale                                                                                         |\n",
    "|-------------|----------|--------|---------------------------------------------------------------------------------------------------|\n",
    "| Review Text | $w_t$    | 0.6    | Critical to NLP tasks; central to downstream model performance and requires complex validation. |\n",
    "| Ratings     | $w_r$    | 0.2    | Ensures valid supervised labels but less impactful than text validity.                            |\n",
    "| Category    | $w_c$    | 0.1    | Filters to relevant dataset categories; errors here may be less frequent or impactful.          |\n",
    "| Review Date | $w_d$    | 0.1    | Ensures review dates are in expected range, but has minimal impact on NLP tasks.             |\n",
    "\n",
    "The weights reflect the importance of review text quality in the overall validity score. \n",
    "\n",
    "### Relevance\n",
    "**Relevance** is the degree to which specific data points or sets of data aligns with the intended purpose or business need, essentially indicating whether the data is meaningful and useful for the analysis at hand. For our analysis, we define relevance in terms of:\n",
    "\n",
    " - **Language**: Are the data free of non-English text in app names and reviews. \n",
    " - **Review Length**: Aspect-based sentiment analysis requires an aspect and an opinion word. Single word reviews have questionable relevance to the task.\n",
    "\n",
    "Our relevance score is computed as:\n",
    "$$\\text{Relevance Score} = w_e \\times{\\frac{N_e}{N}} + w_l \\times{\\frac{N_l}{N}}$$  \n",
    "\n",
    "where:\n",
    "- $N$ is the number of observations in the dataset,\n",
    "- $N_e$ are the number of rows containing English language `app_name`s  and review `content`,\n",
    "- $N_l$ are the number of reviews of length 2 or greater,\n",
    "\n",
    "and the weights are:\n",
    "\n",
    "| Measure       | Variable | Weight | Rationale                                                                                    |\n",
    "|---------------|----------|--------|----------------------------------------------------------------------------------------------|\n",
    "| Language      | $w_e$    | 0.4    | Non-English text is irrelevant and can degrade model performance.                            |\n",
    "| Review Length | $w_l$    | 0.6    | ABSA requires an aspect and an opinion, so overly short reviews are inherently irrelevant. |\n",
    "\n",
    "\n",
    "### Uniqueness\n",
    "We evaluate uniquness in terms of: \n",
    "- **Row Uniqueness**: The degree to which rows are unique, \n",
    "- **Review Id Uniqueness**: Indicates uniqueness of review `id`s.\n",
    "- **Review Uniqueness**: The degree of uniqueness considering `app_id`, `author`, and review `content`. \n",
    "\n",
    "Combining these components, we compute **Uniqueness Score** as:\n",
    "\n",
    "$$\\text{Uniqueness Score} = w_u \\times{\\frac{N_u}{N}} + w_i \\times{\\frac{N_i}{N}}+ w_r \\times{\\frac{N_r}{N}}$$  \n",
    "\n",
    "where:\n",
    "- $N$ is the number of observations in the dataset,\n",
    "- $N_u$ is the number of unique rows, \n",
    "- $N_i$ is the number of rows with unique review `id`s, \n",
    "- $N_r$ is the number of unique reviews in terms of `app_id`, `author`, and review `content`.\n",
    "\n",
    "\n",
    "and the weights are defined as:\n",
    "\n",
    "| Criterion            | Variable | Weight | Rationale                                                                                       |\n",
    "|----------------------|----------|--------|-------------------------------------------------------------------------------------------------|\n",
    "| Row Uniqueness       | $w_u$    | 0.3    | Ensures no exact duplicate rows, critical for data cleanliness and   avoiding redundancy.       |\n",
    "| Review Id Uniqueness | $w_i$    | 0.2    | Tracks unique reviews via IDs, useful but less critical than content   uniqueness.              |\n",
    "| Review Uniqueness    | $w_r$    | 0.5    | Avoids duplicate reviews based on app_id, author, and content, ensuring   quality for analysis. |\n",
    "\n",
    "\n",
    "### Data Privacy\n",
    "Personally Identifiable Information (PII) in datasets raises a number of ethical, privacy, and legal concerns. Here, we measure the degree to which the data are free of PII such as URLs, phone numbers, and email addresses. \n",
    "$$\\text{Data Privacy}=1-\\frac{N_p}{N}$$\n",
    "\n",
    "where:\n",
    "- $N_p$ are the number of observations containing PII, \n",
    "- $N$ is the number of observations in the dataset.\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from discover.app.dqa import DQA\n",
    "from discover.container import DiscoverContainer\n",
    "from discover.assets.idgen import AssetIDGen\n",
    "from discover.core.flow import PhaseDef, StageDef\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "### Dependency Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "container = DiscoverContainer()\n",
    "container.init_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_id = AssetIDGen.get_asset_id(\n",
    "    asset_type=\"dataset\", phase=PhaseDef.DATAPREP, stage=StageDef.DQD, name=\"review\"\n",
    ")\n",
    "repo = container.repo.dataset_repo()\n",
    "df = repo.get(asset_id=asset_id, distributed=False, nlp=False).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqa = DQA(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['dqd_validity_contains_excess_character_repetition'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdqa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_quality\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/appvocai-discover/discover/app/dqa.py:199\u001b[0m, in \u001b[0;36mDQA.analyze_quality\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze_quality\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m--> 199\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummarize_quality\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot_quality(df\u001b[38;5;241m=\u001b[39mdf)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/projects/appvocai-discover/discover/app/dqa.py:214\u001b[0m, in \u001b[0;36mDQA.summarize_quality\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msummarize_quality\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m    204\u001b[0m     d \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimension\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m    206\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleteness\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    207\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidity\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    208\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelevance\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    209\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUniqueness\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    210\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrivacy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    211\u001b[0m         ],\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m    213\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompleteness,\n\u001b[0;32m--> 214\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidity\u001b[49m,\n\u001b[1;32m    215\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelevance,\n\u001b[1;32m    216\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muniqueness,\n\u001b[1;32m    217\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprivacy,\n\u001b[1;32m    218\u001b[0m         ],\n\u001b[1;32m    219\u001b[0m     }\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39md)\n",
      "File \u001b[0;32m~/projects/appvocai-discover/discover/app/dqa.py:120\u001b[0m, in \u001b[0;36mDQA.validity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidity\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validity:\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_validity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validity\n",
      "File \u001b[0;32m~/projects/appvocai-discover/discover/app/dqa.py:341\u001b[0m, in \u001b[0;36mDQA._compute_validity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compute_validity\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validity \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mvalidity\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mrating \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrating_validity\n\u001b[1;32m    340\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mvalidity\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mcategory \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategory_validity\n\u001b[0;32m--> 341\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mvalidity\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mreview \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreview_validity\u001b[49m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mvalidity\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mdate \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdate_validity\n\u001b[1;32m    343\u001b[0m     )\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validity\n",
      "File \u001b[0;32m~/projects/appvocai-discover/discover/app/dqa.py:144\u001b[0m, in \u001b[0;36mDQA.review_validity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreview_validity\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_review_validity:\n\u001b[0;32m--> 144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_review_validity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_review_validity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_review_validity\n",
      "File \u001b[0;32m~/projects/appvocai-discover/discover/app/dqa.py:366\u001b[0m, in \u001b[0;36mDQA._compute_review_validity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    363\u001b[0m columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mvalidity\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mreview_validity\n\u001b[1;32m    365\u001b[0m \u001b[38;5;66;03m# Calculate rows where all specified columns are False\u001b[39;00m\n\u001b[0;32m--> 366\u001b[0m valid_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m~\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39many(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# Compute the proportion of valid rows\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_review_validity \u001b[38;5;241m=\u001b[39m valid_rows\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_N\n",
      "File \u001b[0;32m~/miniconda3/envs/appvocai/lib/python3.10/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/appvocai/lib/python3.10/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/appvocai/lib/python3.10/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['dqd_validity_contains_excess_character_repetition'] not in index\""
     ]
    }
   ],
   "source": [
    "dqa.analyze_quality()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " While **completeness**, **relevance**, **uniqueness**, and **privacy** are strong, the **validity** dimension shows notable gaps. In the context of review text, this likely reflects issues such as noise introduced by control characters, excessive special characters, elongated text, non-ASCII content, accents, or embedded HTML. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqa.analyze_completeness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **completeness** breakdown shows high **Row Completeness**, with minimal missing data in reviews. Let's take a closer look at **Category Balance** and **Sentiment Balance**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Category Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqa.plot_category_class_balance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Category Balance** distribution illustrates notable variability in representation across app categories. While some categories like \"Health & Fitness\" and \"Utilities\" dominate, others such as \"Finance,\" \"Productivity,\" and \"Book\" are significantly underrepresented. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqa.plot_sentiment_class_balance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Sentiment Balance** distribution reveals a skew towards \"Very Positive\" and \"Neutral\" sentiments, with fewer \"Negative\" and \"Very Negative\" reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While **Category Balance** and **Sentiment Balance**  may bias sentiment analysis models, it is not a cleaning issue and can be addressed during pre-modeling with techniques such as class weighting, undersampling, or SMOTE. For now, this observation serves as a consideration for downstream tasks rather than immediate intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqa.analyze_validity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **validity** scores reveal strong conformance to expected rules for **Rating**, **Category**, and **Review Date**, with all achieving perfect scores. However, **Review Validity** is significantly lower, indicating pervasive issues in the text quality of reviews. This likely reflects noise such as control characters, excessive special characters, elongation, non-ASCII content, accents, and embedded HTML. Let's take a closer look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review Validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqa.analyze_review_validity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Review Validity** analysis highlights various forms of noise in the text, such as non-ASCII characters, excess whitespace, elongation, special characters, accents, control characters, and HTML artifacts. These issues reflect the presence of artifacts that do not carry meaningful information and can distort downstream text processing and analysis. Remediation efforts should prioritize removing or normalizing such noise to enhance the clarity and usability of the reviews while preserving the semantic integrity of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqa.analyze_relevance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Relevance** analysis focuses on ensuring that reviews are in English and meet a minimum length requirement. Both **Language Relevance** and **Review Length Relevance** scores are high, indicating that the majority of reviews meet these criteria. However, a small proportion of reviews fall short, potentially introducing irrelevant or low-quality data into the analysis. Addressing these issues by filtering out non-English reviews and excessively short reviews will ensure the dataset remains focused on meaningful and interpretable content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqa.analyze_uniqueness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Uniqueness** analysis shows strong overall performance. **Row Uniqueness** and **Review Uniqueness** are perfect, ensuring no duplicate rows or identical reviews exist in the dataset. However, **Review ID Uniqueness** is slightly below perfect, suggesting a small number of duplicate or reused identifiers. While this is a minor issue, it should be resolved to ensure accurate tracking and association of reviews with their metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqa.analyze_privacy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Privacy** analysis indicates minimal violations of sensitive information; yet, remediation efforts should prioritize removal of URLs, email addresses and phone numbers identified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Implications\n",
    "\n",
    "- **Completeness**: Ensure removal of incomplete rows to maintain dataset integrity. Category and sentiment imbalance will be addressed during pre-modeling through techniques such as undersampling, class weighting, or SMOTE.\n",
    "- **Validity**: Focus on eliminating textual noise, such as excessive special characters, elongation, and control characters, while retaining meaningful artifacts (e.g., emojis or domain-specific symbols) that contribute to the review's semantics.\n",
    "- **Relevance**: Filter out non-English reviews and app names to ensure the dataset focuses on relevant content for analysis.\n",
    "- **Uniqueness**: Remove duplicate review IDs to preserve data accuracy and consistency.\n",
    "- **Privacy**: Strip URLs, phone numbers, and any other sensitive information to maintain compliance with privacy standards.\n",
    "\n",
    "Next, we will implement these cleaning steps to prepare the dataset for downstream analysis and modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appvocai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
