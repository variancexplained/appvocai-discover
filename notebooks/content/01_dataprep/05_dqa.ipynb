{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if \"jbook\" in os.getcwd():\n",
    "    os.chdir(os.path.abspath(os.path.join(\"../..\")))\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AppVoCAI Data Quality Analysis\n",
    "In this section, we evaluate the quality of the AppVoCAI dataset along seven dimensions of data quality.\n",
    "\n",
    "## Completeness\n",
    "**Completeness** represents the degree to which all required data values are present. The completeness metric is defined as:\n",
    "$$X_{Completeness}=\\frac{N_c}{N}$$ \n",
    "where $N_c$ is the number of complete rows, and $N$ is the total number of rows. \n",
    "\n",
    "## Validity\n",
    "**Validity** refers to the degree to which the data complies with pre-defined business rules, such as valid values, ranges, and types. Concretely, we define validity in terms of:\n",
    "\n",
    " - **Ratings**: Valid ratings are integers in [1,5]\n",
    " - **Category**:  The dataset categories are among the 14 valid categories included in this study.\n",
    " - **Review Date**: Review dates should be between January 1, 2020, and September of 2023. \n",
    " - **Review Text**: Review text must be free of personally identifiable information, such as URLs, phone numbers, and email addresses, and patterns that signal invalid reviews, such as exessive repetition in sequences, words, and phrases.\n",
    " \n",
    "Validity scores are computed as follows:\n",
    "\n",
    "$$X_{Validity}=\\frac{N_r}{N}*w_1 + \\frac{N_c}{N}*w_2 + \\frac{N_d}{N}*w_3+ \\frac{N_t}{N}*w_4$$  \n",
    "where:\n",
    "- $N_r$ are the number of rows with valid ratings, \n",
    "- $N_c$ are the observations with valid categories, \n",
    "- $N_d$ indicates review dates in range, \n",
    "- $N_t$ is the count of rows that pass the text data quality anomaly detection checks, and\n",
    "- $N$ is the number of observations in the  dataset.\n",
    "\n",
    "Each component is weighted and summed to produce the $X_{\\text{Validity}}$ score:\n",
    "- **Valid Rate Weight**: $w_1=0.2$\n",
    "- **Valid Category Weight**: $w_2=0.2$\n",
    "- **Valid Review Date Weight**: $w_3=0.2$\n",
    "- **Valid Review Text Weight**: $w_4=0.4$\n",
    "\n",
    "The weights reflect the importance of review text quality in the overall validity score. \n",
    "\n",
    "## Relevance\n",
    "**Relevance** is the degree to which specific data points or sets of data aligns with the intended purpose or business need, essentially indicating whether the data is meaningful and useful for the analysis at hand. For our analysis, we define relevance in terms of:\n",
    "\n",
    " - **Language**: Are the data free of non-English text in app names and reviews. \n",
    " - **Review Length**: \n",
    "\n",
    "3. **Uniqueness**: Uniqueness in the dataset is defined as:\n",
    "$$X_{Uniqueness}=\\frac{N_r}{N}*w_1+\\frac{N_{id}}{N}*w_2$$\n",
    "where $N_r$ is the number of unique reviews by app_id, $N_{id}$ is the number of unique review identifiers, $N$ is the total number of observations, and $w_1=0.3$, and $w_2=0.7$ are weights reflecting the importance of each component to the integrity of the dataset.  \n",
    "\n",
    "4. **Balance**: The degree to which sentiments are in balance across the dataset. Balance is computed as:\n",
    "$$X_{Balance}=1-\\frac{\\sum_{i=1}^N|x_i-\\bar{x}|}{N}$$\n",
    "where $x_i$ is the sentiment for the $i^{th}$ observation, $\\bar{x}$ is the average sentiment for the dataset, and $N$ is the total number of observations.\n",
    "\n",
    "\n",
    "\n",
    "6. **Data Privacy**: Personally Identifiable Information (PII) in datasets raises a number of ethical, privacy, and legal concerns. Here, we measure the degree to which the data are free of PII such as URLs, phone numbers, and email addresses. \n",
    "$$X_{Data Privacy}=1-\\frac{N_p}{N}$$\n",
    "where $N_p$ is the number of observations containing PII, and $N$ is the number of observations in the dataset. \n",
    "\n",
    "7. **Text Quality**: Our text quality metric is a weighted sum of syntactic complexity measures and a perplexity-based coherence score to arrive at a quality score for each rating {ref}`appendix:tqa`. \n",
    "\n",
    "## Overall Data Quality Score\n",
    "The overall data quality score is a weighted sum of the data quality dimensions, calculated as follows:\n",
    "\n",
    "$$\n",
    "\\text{Overall Quality Score} = 0.10 \\times X_{\\text{Completeness}} + 0.15 \\times X_{\\text{Validity}} + 0.10 \\times X_{\\text{Uniqueness}} + 0.10 \\times X_{\\text{Balance}} + 0.15 \\times X_{\\text{Accuracy}} + 0.05 \\times X_{\\text{Data Privacy}} + 0.05 \\times X_{\\text{Interpretability}} + 0.30 \\times X_{\\text{Text Quality}}\n",
    "$$\n",
    "\n",
    "### Justification for Weights\n",
    "1. **Completeness (10%)**: Ensures that a sufficient proportion of data values are present. While essential, its impact is balanced with other dimensions.\n",
    "2. **Validity (15%)**: Critical for ensuring the integrity of numerical values like ratings and identifying outliers in review lengths, influencing analysis reliability.\n",
    "3. **Uniqueness (10%)**: Maintains data integrity by verifying the distinctiveness of reviews and identifiers, though not as heavily weighted as other metrics.\n",
    "4. **Balance (10%)**: Important for maintaining an unbiased distribution of sentiments across the dataset, reducing the risk of skewed analytical insights.\n",
    "5. **Accuracy (15%)**: Essential for minimizing distortions caused by noise or extraneous characters in the text, preserving the fidelity of the data.\n",
    "6. **Data Privacy (5%)**: Measures the presence of Personally Identifiable Information (PII) and ensures compliance with privacy standards. Although vital, it has a lower weight due to the focus on textual analysis.\n",
    "7. **Interpretability (5%)**: Assesses the linguistic appropriateness of the data, such as ensuring reviews are in the correct language. This metric is important but weighted lower in comparison.\n",
    "8. **Text Quality (30%)**: Given the focus on natural language processing, Text Quality receives the highest weight. This dimension emphasizes lexical richness, syntactical diversity, and coherence, which are paramount for accurately analyzing and interpreting complex textual data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from discover.app.dqa import DQA\n",
    "from discover.container import DiscoverContainer\n",
    "from discover.assets.idgen import AssetIDGen\n",
    "from discover.core.flow import PhaseDef, StageDef\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "## Dependency Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "container = DiscoverContainer()\n",
    "container.init_resources()\n",
    "container.wire(\n",
    "    modules=[\n",
    "        \"discover.app.base\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_id = AssetIDGen.get_asset_id(\n",
    "    asset_type=\"dataset\", phase=PhaseDef.DATAPREP, stage=StageDef.DQD, name=\"review\"\n",
    ")\n",
    "repo = container.repo.dataset_repo()\n",
    "df = repo.get(asset_id=asset_id, distributed=False, nlp=False).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dqd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dqa \u001b[38;5;241m=\u001b[39m \u001b[43mDQA\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/appvocai-discover/discover/app/dqa.py:52\u001b[0m, in \u001b[0;36mDQA.__init__\u001b[0;34m(self, name, config_reader_cls, printer_cls)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     47\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     48\u001b[0m     config_reader_cls: \u001b[38;5;28mtype\u001b[39m[AppConfigReader] \u001b[38;5;241m=\u001b[39m AppConfigReader,\n\u001b[1;32m     49\u001b[0m     printer_cls: Type[Printer] \u001b[38;5;241m=\u001b[39m Printer,\n\u001b[1;32m     50\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config \u001b[38;5;241m=\u001b[39m \u001b[43mconfig_reader_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43msection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdqd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer \u001b[38;5;241m=\u001b[39m Printer()\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Obtain the dataset asset id\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/appvocai-discover/discover/infra/config/base.py:86\u001b[0m, in \u001b[0;36mConfigReader.get_config\u001b[0;34m(self, section, namespace)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03mRetrieves configuration data either as a dictionary or as a `NestedNamespace` object.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    If the requested section is not found in the configuration.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_config()\n\u001b[0;32m---> 86\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m[\u001b[49m\u001b[43msection\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m section \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_namespace(config) \u001b[38;5;28;01mif\u001b[39;00m namespace \u001b[38;5;28;01melse\u001b[39;00m config\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dqd'"
     ]
    }
   ],
   "source": [
    "dqa = DQA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqa.plot_quality()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section highlights key areas where our data quality demands further scrutiny, emphasizing **Balance**, **Accuracy**, and **Text Quality** as focal points for improvement.\n",
    "\n",
    "### 1. **Text Quality** (Score: Needs Significant Attention)\n",
    "- **Current Status**: Scoring the lowest among all dimensions, Text Quality highlights serious concerns regarding the richness and diversity of our data. Deficiencies may include limited lexical variation, insufficient syntactic sophistication, or a lack of textual coherence.\n",
    "- **Implications**: Poor text quality poses challenges for natural language processing tasks that require depth and nuance, potentially degrading model performance and insight generation. \n",
    "- **Next Steps**: Evaluate the distribution of text quality scores vis-a-vis quality score thresholds to identify the highest-quality reviews for modeling self-training and fine-tuning. \n",
    "\n",
    "### 2. **Accuracy** (Score: Needs Improvement)\n",
    "- **Current Status**: The Accuracy score points to issues like noisy, inconsistent data. Examples could involve spelling errors, irregular sentence structures, or extraneous characters that interfere with data reliability.\n",
    "- **Implications**: Inconsistent or error-prone data compromises the effectiveness of NLP models, leading to potential misinterpretations or unreliable outcomes.\n",
    "- **Next Steps**:  Inspect the data, focusing on artifacts, extraneous characters, patterns and noise requiring treatment during the data cleaning stage.\n",
    "\n",
    "### 3. **Balance** (Score: Moderate)\n",
    "- **Current Status**: The Balance score is markedly lower compared to dimensions like Completeness, Validity, and Uniqueness.\n",
    "- **Implications**: Imbalances within the dataset, such as uneven distributions of sentiment classes or user feedback, may bias analyses and affect the generalizability of models. These imbalances complicate efforts to build models that perform consistently across all data segments.\n",
    "- **Next Steps**: Examine the sentiment distribution patterns to better understand where imbalances lie. Insights gleaned from this analysis will guide potential downstream data augmentation techniques, ensuring a more representative dataset for modeling.\n",
    "\n",
    "Completeness, Validity, and Uniqueness exhibit strong performance, yet the focus shifts to Balance, Text Quality, and Accuracy. Let's take a closer look. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Dimension Analysis\n",
    "Let's dive in to explore each quality dimension, identifying key issues and the necessary interventions to ensure we deliver high-quality data for the upcoming exploratory and modeling stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Quality\n",
    "Aspect-based sentiment analysis (ABSA), a more fine-grained approach compared to traditional document or sentence-level sentiment analysis, predicts sentiment polarities for specific aspects or entities mentioned in the text. This method relies heavily on rich syntactic and lexical structures to accurately capture the sentiment nuances associated with different aspects. High-quality text that exhibits syntactic diversity and lexical complexity provides a fertile ground for ABSA, enabling models to discern and interpret relationships between words and phrases more effectively.\n",
    "\n",
    "Moreover, texts that are syntactically diverse and lexically rich facilitate better extraction of semantic features, such as aspect terms and opinion expressions. These features are critical in tasks where understanding subtle sentiment shifts or complex opinion structures is paramount. Well-structured text with varied syntax aids in parsing dependency relationships, which models use to link sentiment-laden words to their corresponding aspects. Likewise, a broad lexical range enhances the model’s ability to generalize across different expressions of sentiment, capturing nuances that would otherwise be missed in simpler texts.\n",
    "\n",
    "Additionally, high-quality text minimizes noise and ambiguity, leading to clearer sentiment patterns and more reliable sentiment predictions. The interplay between syntactic and lexical richness and ABSA performance underscores the importance of maintaining rigorous text quality standards throughout preprocessing and analysis. Thus, ensuring text quality not only bolsters sentiment analysis accuracy but also enhances the overall robustness of downstream NLP applications, such as customer feedback analysis, brand monitoring, and nuanced opinion mining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqa.plot_text_quality()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of **Text Quality Assessment (TQA) scores** reveals important characteristics with implications for the ABSA modeling stage.\n",
    "\n",
    "1. **Bimodal Distribution**: The density plot displays a **bimodal shape**, with two distinct peaks. One prominent cluster occurs in the mid-range of scores, while a second, sharper peak appears closer to the higher end. This suggests the presence of two main groups of text quality within the dataset: a substantial portion of moderately well-structured reviews and another segment of high-quality text.\n",
    "\n",
    "2. **Long Tail on the Lower End**: The distribution stretches out toward the lower end, with a longer tail indicating a smaller but notable number of low-quality reviews. These lower scores represent text that may lack coherence, contain errors, or exhibit reduced linguistic complexity.\n",
    "\n",
    "3. **Concentration of High-Quality Reviews**: The plots show a clear concentration of reviews around the higher peak, reinforcing the idea that there is a significant portion of high-quality text. This visually emphasizes that a focused sampling strategy can target these reviews to enhance self-training and fine-tuning efforts.\n",
    "\n",
    "The bimodal distribution of Text Quality Scores implies the need for a strategic selection process when preparing data for self-training and fine-tuning. Ensuring that the model is exposed to a diverse yet high-quality set of reviews is crucial for optimal performance. Here’s why:\n",
    "\n",
    "1. **Diversity**: The variation in text quality, as represented by the two peaks, suggests that the dataset contains a mix of both lower- and higher-quality reviews. By carefully selecting samples from both groups, we can expose the model to a wide range of linguistic structures and styles. This exposure enhances the model's robustness and its ability to generalize across diverse real-world text inputs.\n",
    "\n",
    "2. **Quality**: At the same time, prioritizing higher-quality reviews ensures that the model learns from well-formed, coherent text. This is especially important for fine-tuning, where the goal is to refine the model’s understanding and boost performance on specific NLP tasks. By incorporating quality text into the training set, we can strike a balance between maintaining linguistic diversity and promoting strong language comprehension.\n",
    "\n",
    "3. **Self-Training Implications**: When leveraging self-training techniques, selecting a balanced subset of text data—capturing both the richness of high-quality examples and the linguistic challenges of lower-quality text—can improve the model's ability to self-correct and learn effectively from its own predictions.\n",
    "\n",
    "4. **Fine-Tuning Strategy**: For fine-tuning, it may be beneficial to apply a filtering mechanism that emphasizes text quality, ensuring the model is polished on the best possible data. This targeted approach can help refine the model’s performance, particularly for tasks that are sensitive to text coherence and fluency.\n",
    "\n",
    "Overall, this TQA score analysis suggests that a hybrid strategy combining both **quality filtering** and **diversity preservation** is key to building models that are not only accurate but also adaptable to real-world, varied text data. Let me know if you'd like to discuss how to implement this strategy or explore any additional aspects!   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "In the context of NLP and Aspect-Based Sentiment Analysis (ABSA), accuracy refers to the degree to which the text faithfully represents real-world linguistic structures and meanings. Previously, we identified seven facets of *harmful noise*—artifacts that either lack meaningful content or obscure the intended message of the text. These noise elements can degrade the quality and interpretability of the data, ultimately impacting model performance.\n",
    "\n",
    "The following reveals the extent to which these noise artifacts are present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqa.plot_noise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most prevalent issues in the dataset include **non-ASCII characters** and **excess whitespace**, followed by a smaller percentage of non-English text, text elongation, accented characters, special characters, control characters, and HTML characters. \n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **Prevalence of Non-ASCII Characters**:\n",
    "   - **Highest Density**: Non-ASCII characters account for the largest noise category, affecting over **40%** of the data. These characters include emojis, symbols, and elements from non-Latin scripts, making them critical to text analysis, especially for sentiment-related tasks.\n",
    "   - **Nuanced Handling Required**: Non-ASCII characters are not mere noise; they often convey important context. For example, **emojis** can reflect strong emotional sentiment, while non-Latin script characters are vital for accurately representing multilingual text. For transformer models, which are well-equipped to handle diverse character sets, careful consideration must be given to retain these elements rather than filtering them out.\n",
    "\n",
    "2. **Excess Whitespace**:\n",
    "   - **Significant Impact**: Excess whitespace affects over **10%** of the dataset. While this type of noise does not influence meaning, it disrupts text formatting and consistency. Removing excess whitespace is a straightforward preprocessing task that enhances data quality without altering semantic content.\n",
    "\n",
    "3. **Non-English Text**:\n",
    "   - **Linguistic and Semantic Complexity**: The presence of non-English text introduces an additional layer of complexity. Depending on the goals of the analysis, this content may need to be preserved for multilingual modeling or excluded if the focus is on a specific language. Handling non-English text requires a context-specific approach to ensure data relevance.\n",
    "\n",
    "4. **Other Noise Types**:\n",
    "   - **Text Elongation and Accents**: These features, though less prevalent, carry meaning in many contexts. Text elongation (e.g., “sooo good”) emphasizes sentiment or emphasis, particularly in informal communication, while accented characters are crucial for representing non-English words accurately.\n",
    "   - **Non-English App Names**: These may add to the dataset's complexity and need to be carefully evaluated for their relevance to the analysis.\n",
    "   - **Special, Control, and HTML Characters**: These defects are relatively rare but can interfere with text parsing. HTML tags and control characters will be sanitized or replaced to maintain text clarity.\n",
    "\n",
    "### Implications for Data Processing\n",
    "\n",
    "A **one-size-fits-all** approach to noise removal is inadequate given the complexity and potential value of certain noise elements:\n",
    "- **Non-ASCII Characters**: Rather than removing these elements indiscriminately, it is crucial to evaluate their relevance to the task at hand. Emojis, for instance, can enrich sentiment analysis, while non-Latin scripts are indispensable for multilingual text processing.\n",
    "- **Non-English Text**: A strategic approach is needed to either retain or filter this content based on the scope of the analysis. Multilingual NLP applications would benefit from retaining and properly labeling these data points.\n",
    "- **Whitespace and Control Characters**: These can be safely normalized or eliminated, as they generally do not contribute semantic value.\n",
    "- **Text Elongation and Accents**: Handling these features contextually can preserve emphasis and linguistic nuance. Elongation may enhance sentiment analysis, and accents are important for accurate representation in multilingual datasets.\n",
    "\n",
    "The diversity and complexity of noise in this dataset highlight the need for a **carefully tailored preprocessing strategy**. While standard cleaning practices are effective for simple noise like whitespace, handling non-ASCII, accented characters, and non-English text demands a more thoughtful approach. By preserving meaningful elements, especially for transformer-based models, we can ensure that the data retains its richness and supports robust natural language understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance\n",
    "Class imbalance, in which one or multiple classes (**minority classes** ) are considerably less frequent than other **majority classes** {cite}`henningSurveyMethodsAddressing2022`, tend to bias models towards the majority class(es), neglecting the minority class(s). A common problem in NLP, class imbalance often leads to underperformance and misclassification of the minority class. Let's examine the frequencies of sentiments in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqa.plot_balance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of sentiment classifications has a class balance score of **0.72**, suggestig a moderate imbalance among the classes.\n",
    "\n",
    "### Class Balance Observations:\n",
    "1. **Dominant Classes**: The classes **Neutral** and **Very Positive** have the highest counts, each close to 25,000. This indicates that a significant portion of the dataset falls into these two sentiment categories, which may skew the model's learning focus toward these sentiments.\n",
    "2. **Underrepresented Class**: The **Negative** class is clearly the least represented, with a count much lower than the other classes. This imbalance could lead to challenges in accurately predicting negative sentiment, as the model may struggle to learn sufficient features for this underrepresented category.\n",
    "3. **Moderate Classes**: The **Positive** and **Very Negative** classes have intermediate counts, providing a reasonable representation but still less than the Neutral and Very Positive classes.\n",
    "\n",
    "### Implications for Model Performance:\n",
    "- **Potential Bias**: The imbalance, particularly the underrepresentation of the Negative class, could introduce bias into your sentiment analysis model, making it more likely to misclassify or underpredict negative sentiments.\n",
    "- **Mitigation Strategies**: To address this class imbalance, consider techniques like oversampling the Negative class, undersampling the dominant classes, or using class-weighted loss functions to ensure the model pays equal attention to all sentiment categories.\n",
    "- **Data Augmentation**: Another approach could be augmenting the dataset with additional negative sentiment examples if possible, to improve the representation of underrepresented classes.\n",
    "\n",
    "Overall, while the class balance score of 0.72 indicates a moderate imbalance, special attention will be given to ensuring the model can generalize well across all sentiment classes, especially the underrepresented ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Privacy\n",
    "The presence of personally identifiable information (PII) in AI datasets is a significant concern with both ethical and legal implications. Removing PII from analysis and training data is essential to mitigate risks of **privacy violations, data breaches, and non-compliance with regulations such as GDPR or CCPA**. Ensuring that PII is properly handled and anonymized safeguards user confidentiality, upholds data ethics standards, and protects organizations from legal liabilities. Moreover, it fosters trust with users and stakeholders, reinforcing the responsible use of AI technologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqa.plot_privacy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart provides an overview of the presence of different types of **Personally Identifiable Information (PII)** in the dataset. Here’s an analysis of the findings:\n",
    "\n",
    "### Key Observations:\n",
    "1. **High Prevalence of URLs**: The most significant PII issue is the presence of URLs, which appear in over 2% of the data. URLs can contain sensitive or identifying information, such as user profiles or specific website interactions. This prevalence underscores the need for targeted strategies to detect and remove or anonymize URLs to protect user privacy.\n",
    "\n",
    "2. **Moderate Presence of Phone Numbers**: The presence of phone numbers is notably lower than URLs but still represents a meaningful percentage of the data. Phone numbers are highly sensitive, and even a small amount of this data can pose serious privacy risks. Ensuring these are effectively identified and removed is critical for compliance and ethical data handling.\n",
    "\n",
    "3. **Absence or Minimal Presence of Email Addresses**: Email addresses are either completely absent or present at a negligible level, as suggested by the empty or near-zero bar. While this is encouraging from a privacy standpoint, it’s still important to have mechanisms in place to identify and handle any email addresses that might appear in future data.\n",
    "\n",
    "### Implications:\n",
    "- **Focus on URL Removal**: The high prevalence of URLs necessitates robust mechanisms for detecting and anonymizing or removing them from the dataset. Special consideration will be given to parsing and handling URLs effectively to prevent any residual identifying information.\n",
    "- **Stringent Measures for Phone Numbers**: Although less common, phone numbers will be treated with heightened sensitivity. Incorporating regex patterns or other detection methods for comprehensive coverage is essential.\n",
    "- **Preparedness for Email Detection**: Even though emails appear to be minimal, maintaining vigilant detection measures ensures that any future data additions do not inadvertently expose sensitive information.\n",
    "\n",
    "The chart highlights the importance of prioritizing PII mitigation strategies, especially for URLs and phone numbers, to maintain data privacy and integrity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity \n",
    "In the context of data quality, **validity** refers to the extent to which the data accurately and appropriately represents the phenomenon it is intended to capture. Distinct from an *unhelpful* or *non-informative* review, an *invalid* review as one that fails to represent *any* phenomenon. To isolate and separate the **valid** from the **invalid** reviews, we'll apply four measures that we hypothesize to be determinates of review validity.\n",
    "\n",
    "1. **Text Quality Score**: A composite of syntactic and lexical complexity and coherence\n",
    "2. **Perplexity**: Raw perplexity scores indicate the degree of language model uncertainty in predicting the next word in a sequence.\n",
    "3. **Review Length**: Short or excessively long reviews may indicate low quality or invalid reviews.\n",
    "4. **Repetition**: High character, word, and phrase repretition often indicates text validity issues.\n",
    "\n",
    "To what degree is validity determined by these measures? Let's see what we can induce from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Quality and Validity\n",
    "We selected the 20 lowest quality English reviews. Since short reviews tend to score in the lower extremes, we'll filter reviews of 5 words or greater. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 200\n",
    "dqa.select(\n",
    "    n=10,\n",
    "    exclude=\"tqd_non_english_text\",\n",
    "    sort_by=\"tqa_score\",\n",
    "    ascending=False,\n",
    "    cols=[\"id\", \"app_name\", \"tqa_score\", \"content\"],\n",
    "    x=\"review_length\",\n",
    "    condition=lambda x: x > 4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis of `tqa_score` at a threshold of 0.0899 highlights notable limitations in its ability to serve as a reliable predictor of review validity. Here’s how it plays out across our 20-sample dataset:\n",
    "\n",
    "Starting from observation 0, the score classifies a genuinely valid review—\"Like 10 out of 10\"—as valid, yielding a true positive. However, as we proceed, the score correctly flags “Hgdwtt Ty Inc vvv Ct,” a nonsensical entry, as invalid, marking it as a true positive. The model continues with some successes, like correctly identifying the repetitive “Ben Better Ben Better…” as invalid. Yet, the pattern quickly falters: simple but clear reviews like “Thankyou for who you are” and “I am nothing without god” fall victim to misclassification, counted as false positives.\n",
    "\n",
    "The metrics tell a similar story. Precision sits at 0.67, meaning only 67% of the reviews flagged as valid truly are. Recall, also at 0.67, reflects that the classifier managed to capture just two-thirds of all genuine valid reviews. The F1 score of 0.67 encapsulates this moderate performance, highlighting the score’s struggle to balance precision and recall.\n",
    "\n",
    "It’s crucial to acknowledge that these figures represent an upper bound. By filtering out reviews with fewer than five words, we’ve artificially limited the potential for false positives. If shorter, valid reviews had been included, the precision would likely drop, as `tqa_score` tends to misclassify brief but meaningful content.\n",
    "\n",
    "In a nutshell, even with fine-tuned thresholds, `tqa_score` delivers erratic and unreliable classification, underscoring the need for additional or alternative measures to enhance validity assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqa.show_review(review_id=\"5971658415\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perplexity and Validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqa.select(\n",
    "    n=20,\n",
    "    exclude=\"tqd_non_english_text\",\n",
    "    sort_by=\"an_perplexity\",\n",
    "    ascending=True,\n",
    "    cols=[\"id\", \"app_name\", \"an_perplexity\", \"content\"],\n",
    "    x=\"review_length\",\n",
    "    condition=lambda x: x > 4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review Length Analysis\n",
    "Again, we take a conservative approach by defining review length outliers as those $3\\times\\text{IQR}$. Let's examine the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqa.plot_review_length_validity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistics provided for the **review_length** feature give insight into the distribution and potential presence of outliers:\n",
    "\n",
    "### Key Observations:\n",
    "1. **Central Tendency**: \n",
    "   - The **mean review length** is 142.7 characters, while the **median (50th percentile)** is 120 characters. The difference between the mean and median suggests a right-skewed distribution, indicating the presence of longer reviews that pull the average up.\n",
    "  \n",
    "2. **Spread and Variability**:\n",
    "   - The **standard deviation** is relatively high at 71.2 characters, reflecting considerable variability in review lengths. This wide spread indicates that there are substantial differences in how long reviews tend to be, with some being much longer than others.\n",
    "  \n",
    "3. **Range and Potential Outliers**:\n",
    "   - The minimum review length is 90 characters, while the maximum is a striking 1016 characters. Given that the 75th percentile value is 157 characters, reviews significantly longer than this (towards the upper end of the range) are likely outliers. These extreme values may represent detailed, comprehensive reviews that deviate from the norm or potential issues such as spam or overly verbose entries.\n",
    "\n",
    "### Implications for Data Quality:\n",
    "- **Identifying Outliers**: Reviews that are much longer than the 75th percentile (e.g., those closer to or exceeding 1000 characters) will be flagged for further examination. These could either be valid, detailed reviews or invalid entries, such as spam or irrelevant content.\n",
    "- **Handling Strategy**: \n",
    "  - **Invalid Outliers**: If these outliers are found to be spam, irrelevant, or errors, they will be removed or corrected to improve the overall data quality and ensure model reliability.\n",
    "  - **Valid Outliers**: If these longer reviews provide valuable and legitimate feedback, they will be retained, as they can offer important insights for tasks like aspect-based sentiment analysis.\n",
    "\n",
    "Overall, the distribution suggests a need for targeted outlier analysis to distinguish between useful and problematic reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Review Length Outlier Samples\n",
    "Questions. To what degree does extreme review length indicate gibberish, nonsense, SPAM, or *harmful* noise, and what are the characteristics that distinguish that from verbosity? We hypothesize that excessive repetition is a factor that distinguishes SPAM from wordinesss.  What do the data say? Here, we've listed the worst offenders in descending order of review length, together with indicators of extreme repetition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqa.get_defects(\n",
    "    defect=\"review_length\",\n",
    "    sort_by=\"review_length\",\n",
    "    ascending=False,\n",
    "    cols=\"repetition\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perplexity Outliers\n",
    "We use perplexity as a proxy indicator of giberish. Reviews with a perpleixy above $3\\times\\text{IQR}$ is suspect for gibberish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqa.plot_perplexity_validity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Data Quality Analysis\n",
    "The Data Quality Assessment identified key problem areas, including **Balance**, **Accuracy**, and **Text Quality**. Each of these dimensions presents unique challenges with implications for subsequent analysis and modeling. Imbalances in the data may lead to biased outcomes, inaccuracies could disrupt model performance, and lower text quality threatens the linguistic richness needed for robust NLP tasks.\n",
    "\n",
    "In the next section, we transition to the **Data Cleaning** phase, where we will apply targeted strategies to mitigate these issues, ensuring that the dataset is well-prepared for reliable and effective modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appvocai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
