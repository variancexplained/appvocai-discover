{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "\n",
    "if \"jbook\" in os.getcwd():\n",
    "    os.chdir(os.path.abspath(os.path.join(\"../..\")))\n",
    "FORCE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "In the previous section, we analyzed the AppVoCAI dataset, evaluating its validity, completeness, uniqueness, relevance, and adherence to data privacy concerns. This section is about duplicate deleting, language filtering, artifact removing, PII masking, character normalizing data cleaning. Our data cleaning methodology focuses on addressing critical data quality issues that could undermine the integrity of downstream analyses, while preserving the text as close to its original form as possible. By adopting this conservative approach, we tackle key issues without sacrificing the nuance and representativeness of the data, ensuring the models are presented with rich, authentic input.\n",
    "\n",
    "### Data Cleaning Key Evaluation Questions (KEQs)\n",
    "Although, this data cleaning approach comprises many of the preprocessing techniques commonly found in the literature {cite}`symeonidisComparativeEvaluationPreprocessing2018`, the following data cleaning approach is motivated by three guiding questions.\n",
    "\n",
    "1. What’s essential to remove, and what can be left intact to preserve meaning?\n",
    "2. How do we best preserve text richness and nuance?\n",
    "3. How can the data cleaning process best exploit model strengths towards optimal model performance?\n",
    "\n",
    "These Key Evaluation Questions (KEQs) crystallized our approach which balanced data quality with model sophistication.\n",
    "\n",
    "### Data Cleaning Strategy\n",
    "The following describes our data cleaning process and steps, executed in the order listed. We begin with 'safe' techniques that carry minimal risk of compromising downstream cleaning tasks. For instance, UTF-8 encoding can impact the accuracy of language detection algorithms, especially if characters carry language-specific information. Removing special characters may compromise the detection of Personally Identifiable Information (PII) such as URLs and email addresses. As the process progresses, steps carry a greater impact on the data, its expressiveness, and representation.\n",
    "\n",
    "Our minimalist, *leave-as-is* approach can depart from data cleaning orthodoxy and standard practice. In such cases, we are transparent with our rationale. With that, our process is as follows:\n",
    "\n",
    "1. **Review ID Deduplication**: For duplicate review IDs, our policy for retention is based on several criteria: the most recent review date is prioritized, followed by the longest review text, and, if all else is equal, the review with the lowest row index is retained. This ensures that we keep the most informative and relevant reviews.\n",
    "2. **Control Characters**: We remove non-printable characters from the Unicode and ASCII character sets that are used to control text flow or hardware devices (e.g., newline, tab, or carriage return). These characters have no analytical value and can interfere with text processing.\n",
    "3. **Privacy**: URLs, email addresses and phone numbers are removed from the dataset to ensure adherence to data privacy and minimal information policies.\n",
    "4. **Remove Non-English Text**: We identify and remove non-English app names and reviews to maintain linguistic uniformity within the dataset, which is crucial for consistent language-based analysis.\n",
    "5. **HTML Characters**: Common in scraped data, HTML entities (e.g., `&amp;`, `&#39;`) are removed as they do not convey meaningful content. This ensures that the text is clean and ready for analysis.\n",
    "6. **Accents and Diacritics**: We normalize accented characters (e.g., converting `é` to `e`) to reduce unnecessary text variation, which simplifies analysis without compromising the meaning of the content.\n",
    "7. **Repetition**: Excess character, sequence, word and phrase repetition is reduced, but not eliminated to perserve artifacts that may signal emphasis.\n",
    "8. **Non-Informative Reviews**: Reviews that don't match minimum length criteria are removed.\n",
    "9. **Special Characters**: Excessive special characters can indicate SPAM, emotional intensity, or nonsensical content. We apply a threshold: if special characters make up more than 35% of the review text, the review is removed. \n",
    "10. **Elongation Handling**: Elongated words (e.g., \"soooo\") convey emphasis in informal text, which is valuable for sentiment analysis. We use a threshold approach to limit characters that appear four or more times consecutively to a maximum of three (e.g., \"soooo\" becomes \"sooo\"), preserving emphasis while maintaining readability.\n",
    "11. **Excessive Whitespace**: Excessive whitespace is removed.\n",
    "\n",
    "### Data Cleaning Techniques Not Implemented\n",
    "In natural language processing (NLP), text cleaning measures such as lower-casing, contraction and abbreviation expansion, spelling correction, and the removal of emoticons, emojis, and other artifacts are considered standard practice. Given that transformer models are fine-tuned on large user generated content datasets such as IMDB Movie Reviews and SemEval Laptop Reviews dataset, they are highly adept at handling a wide variety of tokens, including emojis, spelling variations, abbreviations and contractions. So, we take a **leave emojis as-is** approach. By leveraging the inherent strengths of transformer - particularly their ability to tokenize subword units and learn from context — we preserve the natural, authentic nature of user-generated content.\n",
    "\n",
    "Hey Siri, play my Data Cleaning playlist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from discover.app.dqa import DQA\n",
    "from discover.container import DiscoverContainer\n",
    "from discover.infra.config.flow import FlowConfigReader\n",
    "from discover.flow.stage.data_prep.clean import DataCleaningStage\n",
    "from discover.flow.stage.data_prep.quality import DataQualityStage\n",
    "from discover.core.flow import PhaseDef, StageDef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "### Dependency Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "container = DiscoverContainer()\n",
    "container.init_resources()\n",
    "container.wire(\n",
    "    modules=[\n",
    "        \"discover.flow.stage.base\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Pipeline\n",
    "This code snippet demonstrates how to set up and run a data cleaning stage based on a configuration obtained from a configuration reader. Here’s a breakdown of the steps:\n",
    "\n",
    "1. **Obtain the Configuration**: \n",
    "   - A `FlowConfigReader` instance (`reader`) is used to load the configuration. \n",
    "   - The `get_stage_config` method retrieves the stage configuration for data preparation and cleaning.\n",
    "\n",
    "2. **Build the Data Cleaning Stage**:\n",
    "   - The `DataCleaningStage.build` method initializes the data cleaning stage with the provided `stage_config`. Setting `force=False` ensures the stage is only built if the endpoint doesn't already exists.\n",
    "\n",
    "3. **Run the Data Cleaning Stage**:\n",
    "   - Finally, the `run` method executes the data cleaning stage and returns an `asset_id`, which likely identifies the cleaned dataset or asset generated by this stage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# ============================================================================== #\n",
      "#                              Data Cleaning Stage                               #\n",
      "# ============================================================================== #\n",
      "\n",
      "____________________________________________________________________________\n",
      "Data Cleaning Stage                     16:46:05    16:46:05    0.01 seconds\n",
      "\n",
      "\n",
      "                          Data Cleaning Stage Summary                           \n",
      "                          ---------------------------                           \n",
      "                         Original       Clean  % Change\n",
      "Rows                      8670.00     7787.00     10.18\n",
      "Rows Modified                0.00     7787.00     89.82\n",
      "Average Review Length       31.43       33.94      8.00\n",
      "Size (Mb)              7166883.00  6594555.00      7.99\n",
      "\n",
      "\n",
      "# ============================================================================ #\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obtain the configuration\n",
    "reader = FlowConfigReader()\n",
    "stage_config = reader.get_stage_config(phase=PhaseDef.DATAPREP, stage=StageDef.CLEAN)\n",
    "\n",
    "# Build and run Data Cleaning Stage\n",
    "stage = DataCleaningStage.build(\n",
    "    stage_config=stage_config, return_dataframe_type=None, force=FORCE\n",
    ")\n",
    "asset_id = stage.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Verification Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# ============================================================================== #\n",
      "#                        Data Cleaning Verification Stage                        #\n",
      "# ============================================================================== #\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task                                    Start       End         Runtime     \n",
      "----------------------------------------------------------------------------\n",
      "DetectOrRepairURLTask                   16:46:20    16:46:20    0.15 seconds\n",
      "DetectOrRepairEmailAddressTask          16:46:20    16:46:20    0.06 seconds\n",
      "DetectOrRepairPhoneNumberTask           16:46:20    16:46:20    0.05 seconds\n",
      "DetectOrRepairControlCharsTask          16:46:20    16:46:21    0.05 seconds\n",
      "DetectOrRepairAccentedCharsTask         16:46:21    16:46:21    0.04 seconds\n",
      "DetectOrRepairHTMLCharsTask             16:46:21    16:46:21    0.04 seconds\n",
      "DetectOrRepairExcessiveWhitespaceTask   16:46:21    16:46:21    0.04 seconds\n",
      "DetectOrRepairNonEnglishTask            16:46:21    16:46:21    0.17 seconds\n",
      "DetectOrRepairNonEnglishTask            16:46:21    16:46:21    0.13 seconds\n",
      "DetectOrRepairExcessiveSpecialCharsTask 16:46:21    16:46:21    0.46 seconds\n",
      "DetectOrRepairUniquenessTask            16:46:21    16:46:22    0.19 seconds\n",
      "DetectOrRepairUniquenessTask            16:46:22    16:46:22    0.11 seconds\n",
      "DetectOrRepairUniquenessTask            16:46:22    16:46:22    0.1 seconds \n",
      "DetectOrRepairElongationTask            16:46:22    16:46:22    0.03 seconds\n",
      "DetectOrRepairRepeatedSequenceTask      16:46:22    16:46:22    0.1 seconds \n",
      "DetectOrRepairRepeatedCharactersTask    16:46:22    16:46:22    0.02 seconds\n",
      "DetectOrRepairRepeatedWordsTask         16:46:22    16:46:22    0.1 seconds \n",
      "DetectOrRepairRepeatedPhraseTask        16:46:22    16:46:22    0.11 seconds\n",
      "DetectOrRepairGibberishTask             16:46:22    16:46:22    0.03 seconds\n",
      "DetectOrRepairShortReviewsTask          16:46:22    16:46:22    0.05 seconds\n",
      "DetectOrRepairCategoryAnomalyTask       16:46:22    16:46:22    0.08 seconds\n",
      "DetectOrRepairRatingAnomalyTask         16:46:22    16:46:22    0.07 seconds\n",
      "DetectOrRepairReviewDateAnomalyTask     16:46:22    16:46:23    0.21 seconds\n",
      "DropColumnsTask                         16:46:23    16:46:23    0.02 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                        (0 + 14) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________\n",
      "Data Cleaning Verification Stage        16:46:06    16:46:29    23.2 seconds\n",
      "\n",
      "\n",
      "# ============================================================================ #\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "FORCE = True  #\n",
    "# Obtain the configuration\n",
    "reader = FlowConfigReader()\n",
    "stage_config = reader.get_stage_config(phase=PhaseDef.DATAPREP, stage=StageDef.DQV)\n",
    "\n",
    "# Build and run the Data Cleaning Verification Stage\n",
    "stage = DataQualityStage.build(\n",
    "    stage_config=stage_config, return_dataframe_type=\"pandas\", force=FORCE\n",
    ")\n",
    "asset_id = stage.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'convert_dtypes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dqa \u001b[38;5;241m=\u001b[39m \u001b[43mDQA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m dqa\u001b[38;5;241m.\u001b[39manalyze_quality()\n",
      "File \u001b[0;32m~/projects/appvocai-discover/discover/app/dqa.py:42\u001b[0m, in \u001b[0;36mDQA.__init__\u001b[0;34m(self, df, config_reader_cls)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     39\u001b[0m     df: pd\u001b[38;5;241m.\u001b[39mDataFrame,\n\u001b[1;32m     40\u001b[0m     config_reader_cls: Type[AppConfigReader] \u001b[38;5;241m=\u001b[39m AppConfigReader,\n\u001b[1;32m     41\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_reader \u001b[38;5;241m=\u001b[39m config_reader_cls()\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_reader\u001b[38;5;241m.\u001b[39mget_config(section\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdqa\u001b[39m\u001b[38;5;124m\"\u001b[39m, namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/projects/appvocai-discover/discover/app/analysis.py:55\u001b[0m, in \u001b[0;36mAnalysis.__init__\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, df: pd\u001b[38;5;241m.\u001b[39mDataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df \u001b[38;5;241m=\u001b[39m df\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtypes\u001b[49m(convert_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_visualizer \u001b[38;5;241m=\u001b[39m Visualizer()\n",
      "File \u001b[0;32m~/miniconda3/envs/appvocai/lib/python3.10/site-packages/pyspark/sql/dataframe.py:3129\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3096\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   3097\u001b[0m \n\u001b[1;32m   3098\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3126\u001b[0m \u001b[38;5;124;03m+---+\u001b[39;00m\n\u001b[1;32m   3127\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 3129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   3130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[1;32m   3131\u001b[0m     )\n\u001b[1;32m   3132\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   3133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'convert_dtypes'"
     ]
    }
   ],
   "source": [
    "dqa = DQA(df=dataset.content)\n",
    "dqa.analyze_quality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dca.analyze_quality()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalize Data Cleaning\n",
    "Drop intermediate data preparation columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing\n",
    "With data cleaning complete, we have addressed the critical anomalies without over-processing.\n",
    "\n",
    "In the next section, we turn our attention to data enrichment. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appvocai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
