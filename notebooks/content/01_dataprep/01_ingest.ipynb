{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if \"jbook\" in os.getcwd():\n",
    "    os.chdir(os.path.abspath(os.path.join(\"../..\")))\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "FORCE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AppVoCAI Dataset Ingestion\n",
    "In this section, we unbox the dataset, survey its key characteristics, profile its structure, format, and data types, then register it as an asset, prior to downstream data quality assessment, cleaning, enrichment and analysis activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from discover.analytics.base import Analysis\n",
    "from discover.setup import auto_wire_container\n",
    "from discover.infra.config.flow import FlowConfigReader\n",
    "from discover.core.flow import DataPrepStageEnum, PhaseEnum\n",
    "from discover.flow.stage.data_prep.ingest import IngestionStage\n",
    "\n",
    "# Wire container\n",
    "container = auto_wire_container()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest Data\n",
    "The IngestionStage loads the raw data, and performs encoding verification, data type casting, and removal of newlines from the review text, ensuring data accessibility for downstream processing and analysis.   \n",
    "\n",
    "The following orchestrates the initialization and execution of the Ingestion pipeline within a broader data preparation workflow. It begins with the retrieval of a specific configuration through the `FlowConfigReader`. This reader is tasked with pulling the configuration that defines the parameters for the pipeline, specifically targeting the `INGEST` stage within the `DataPrep` phase. This configuration encapsulates the structure, settings, and tasks required for ingesting the data.\n",
    "\n",
    "With the configuration in hand, the code then proceeds to the next step: the construction of the `IngestionStage` pipeline. Using the `build` method of the `IngestionStage` class, it dynamically assembles the pipeline, injecting the necessary configurations and optionally setting a flag to force the execution of the pipeline, even if the destination data already exists. This enables flexibility, allowing the pipeline to rerun when necessary, or to skip execution if the data is up to date.\n",
    "\n",
    "Finally, the pipeline is executed by invoking the `run()` method, triggering the ingestion workflow, applying the defined tasks and operations. Upon successful execution, the method returns an asset ID, serving as a reference to the processed data. This asset ID can be used for subsequent stages or for tracking the output of the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# ============================================================================== #\n",
      "#                              Data Ingestion Stage                              #\n",
      "# ============================================================================== #\n",
      "\n",
      "\n",
      "Task                                    Start       End         Runtime     \n",
      "----------------------------------------------------------------------------\n",
      "VerifyEncodingTask                      01:47:00    01:47:00    0.02 seconds\n",
      "CastDataTypeTask                        01:47:00    01:47:00    0.02 seconds\n",
      "RemoveNewlinesTask                      01:47:00    01:47:00    0.0 seconds \n",
      "____________________________________________________________________________\n",
      "Data Ingestion Stage                    01:47:00    01:47:00    0.33 seconds\n",
      "\n",
      "\n",
      "# ============================================================================ #\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obtain the configuration\n",
    "reader = FlowConfigReader()\n",
    "stage_config = reader.get_stage_config(phase=PhaseEnum.DATAPREP, stage=DataPrepStageEnum.INGEST)\n",
    "\n",
    "# Build and run Data Ingestion Stage\n",
    "stage = IngestionStage.build(stage_config=stage_config, force=FORCE)\n",
    "dataset = stage.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AppVoCAI Dataset Summary\n",
    "Let's  load and summarize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                            AppVoCAI Dataset Summary                            \n",
      "                       Number of Reviews | 8,670\n",
      "                     Number of Reviewers | 8,668\n",
      "              Number of Repeat Reviewers | 2 (0.0%)\n",
      "         Number of Influential Reviewers | 575 (6.6%)\n",
      "                          Number of Apps | 2,636\n",
      "                    Number of Categories | 14\n",
      "                 Average Reviews per App | 3.3\n",
      "                                Features | 11\n",
      "                        Memory Size (Mb) | 6.74\n",
      "                    Date of First Review | 2020-01-01 11:06:50\n",
      "                     Date of Last Review | 2023-08-29 13:25:46\n"
     ]
    }
   ],
   "source": [
    "reviews = Analysis(df=dataset.to_pandas())\n",
    "reviews.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evidenced here, we've extracted 8.7 million reviews dated between January of 2020, and September of 2023. These reviews were contributed by over 7 million unique users, with approximately 14% identified as repeat reviewers. Notably, over 7% of the reviewers qualify as influencers, defined by having at least one review with a non-zero vote count. The dataset encompasses nearly 35,000 apps spanning 14 distinct categories. On average, each review contains 32 words, while the average number of reviews per app nears 250. With a memory footprint of 5.8 GB, the dataset is of a moderately large scale, posing both opportunities and challenges for data processing and analysis.\n",
    "\n",
    "In the next stage, we assess data quality, illuminating interventions for data cleaning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appvocai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
