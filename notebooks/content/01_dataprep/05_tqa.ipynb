{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "\n",
    "if \"jbook\" in os.getcwd():\n",
    "    os.chdir(os.path.abspath(os.path.join(\"../..\")))\n",
    "FORCE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"Let us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do.\" (Donald Knuth. \"Literate Programming (1984)\" in Literate Programming. CSLI, 1992, pg. 99)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Quality Analysis (TQA) \n",
    "Review text quality is an indicator of the content's richness, coherence, and informativeness. In this section, we integrate two complementary quality assessment measures—a lexical/syntactic complexity score and a perplexity-based coherence score—into a weighted sum. This approach provides a balanced evaluation, capturing both the structural diversity and natural language fluency of the reviews.\n",
    "\n",
    "## Syntactic and Lexical Complexity Assessment\n",
    "The lexical and syntactic quality assessment (TQA) evaluates review quality using a composite score derived from multiple syntactic and lexical measures. These measures are computed with specific weights:\n",
    "\n",
    "- **Syntactic Diversity Score** (30%): Captures variety in language using an entropy-based calculation.\n",
    "- **Syntactic Density Score** (30%): Assesses the density of key parts of speech relative to total word count.\n",
    "- **Lexical Complexity Score** (40%): Evaluates text complexity using unique word proportion, special character usage, and word length variation.\n",
    "\n",
    "\n",
    "A high Lexical and Syntactic Complexity Score typically indicates a text rich in linguistic features, with varied sentence structures and a well-balanced mix of nouns, verbs, and modifiers (like adjectives and adverbs). This variety is particularly valuable for tasks like Aspect-Based Sentiment Analysis (ABSA), where structural complexity can signal content with nuanced aspects and sentiments.\n",
    "\n",
    "## Perplexity-Based Coherence Scoring\n",
    "This measure evaluates review quality by applying 14 linguistic and structural filters, each assigned a weight derived from relative perplexity differences between the full dataset and filtered subsets. The filters assess features like adjective presence, punctuation ratios, word repetition, and special character use. Weights are computed to emphasize filters that most reduce perplexity, thus enhancing text fluency and coherence. The final score is a weighted sum of these filter indicators.\n",
    "\n",
    "Lower perplexity implies higher fluency, coherence, and grammatical correctness, which are key indicators of text quality. This component is useful for flagging low-quality or noisy text that may be unpredictable or deviate significantly from standard linguistic norms.\n",
    "\n",
    "## Weighted Scoring Approach\n",
    "To create a balanced quality score, the Syntactic Complexity Score and Perplexity-Based Score are combined with tailored weights that emphasize their respective strengths.\n",
    "\n",
    "- **Lexical and Syntactic Complexity Weight**: Typically given more weight when the task demands detailed and linguistically rich text, such as ABSA, where richer syntactic content improves aspect and sentiment extraction.\n",
    "- **Perplexity-Based Weight**: Often assigned a moderate weight to capture coherence and fluency, ensuring that only grammatically sound and predictable text is prioritized without sacrificing syntactic diversity.\n",
    "\n",
    "The final **Text Quality Score** is a weighted average of these two components, providing a single score that balances both syntactic richness and linguistic fluency. The remainder of this notebook will execute the text quality scoring pipeline, computing and integrating the two quality measures into a final text quality score. The specifics of the measures are provided in {ref}`appendix:tqa`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from discover.container import DiscoverContainer\n",
    "from discover.infra.config.flow import FlowConfigReader\n",
    "from discover.core.flow import StageDef\n",
    "from discover.flow.data_processing.data_prep.tqa.stage import TQAStage\n",
    "from discover.core.flow import PhaseDef, StageDef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "## Dependency Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "container = DiscoverContainer()\n",
    "container.init_resources()\n",
    "container.wire(\n",
    "    modules=[\n",
    "        \"discover.flow.data_processing.base.stage\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Quality Analysis Pipeline\n",
    "You know the script. We obtain the configuration, instantiate and run the `TQAStage` stage object. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Obtain the configuration\n",
    "reader = FlowConfigReader()\n",
    "stage_config = reader.get_stage_config(\n",
    "    phase=PhaseDef.DATAPREP, stage=StageDef.TQA\n",
    ")\n",
    "\n",
    "# Build and run Text Quality Analysis Pipeline\n",
    "stage = TQAStage.build(stage_config=stage_config, force=FORCE)\n",
    "asset_id = stage.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code transparency is the DNA of reproducibility. It encodes the essential instructions that allow research and software processes to be faithfully replicated, understood, and verified. Just as DNA must be legible and robust, enabling organisms to sustain and reproduce their biological functions, transparent code must eschew the allure of overly elaborate abstractions that obfuscate its core logic. These abstractions often conceal critical details beneath layers of complexity, making the underlying processes opaque to readers. The art of transparency lies in maintaining a clear, discoverable architecture, one that reveals the logic and data flows without extraneous, labyrinthine constructs. This disciplined simplicity, much like the genetic elegance of a functional genome, empowers others to inspect, reproduce, and extend the work, ensuring the longevity and reliability of the knowledge encoded within...so I am told.\n",
    "\n",
    "Our (plural form included for... obfuscation) tendency to abstract away \"*implementation details*\" in favor of narrative coherence and focus on what *we* believe has significance manifests in: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from abc import ABC, abstractmethod\n",
    "class Human(ABC):\n",
    "    @abstractmethod\n",
    "    def fly(self, elevation: float, distance: float) -> None:\n",
    "        # 1. Clear your mind, 2. Visualize flight, 3. Levitate. TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well before Perez, Granger, and Ragan-Kelly {cite}`IpythonIpython2024`, Donald Knuth {cite}`KnuthLiterateProgramming` introduced Literate Programming - source code as literature, processed by computers, exposed to, and *critically* understood by humans. \n",
    "\n",
    "The pipeline pattern realized in the **Text Quality Analysis Pipeline** above reflects our orchestration approach, achieving correctness vis-a-vis its functional design and efficiency, with a worst-case time complexity of linearithmic $(\\text{O}(\\text{n log n}))$, given $n$ as the size of the OpenWebTextCorpus {cite}Download used for perplexity computations. Yet, it is *not* literate and lacks transparency. It is indiscoverable, unrepeatable, and impossible to reproduce.\n",
    "\n",
    "In this section, we prioritize code transparency over abstraction, and expressivity over brevity. First, we'll outline in detail, the **Text Quality Analysis** tasks, then construct the pipeline from configuration to orchestration. To support narrative coherence and flow, code cells will be collapsed, but accessible. By emphasizing program literacy, we hope to present our methods with greater visibility and accessibility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Quality Analysis Pipeline Tasks\n",
    "The tasks which we will build span three distinct stages:\n",
    "\n",
    "1. **Syntactic and Lexical Complexity Scoring**: This stage evaluates each review's structural and lexical intricacy, assigning scores based on sentence complexity, vocabulary richness, and grammatical diversity.\n",
    "2. **Perplexity-Based Coherence Scoring**: Here, we assess the coherence of each review using perplexity metrics, providing a measure of how well the text flows and adheres to linguistic norms.\n",
    "3. **Text Quality Scoring**: Finally, we integrate the complexity and coherence scores into a composite measure. This weighted sum offers a comprehensive evaluation of the overall quality of each review, balancing structural sophistication with textual clarity.\n",
    "\n",
    "This pipeline will be implemented in [PySpark](https://spark.apache.org/docs/latest/api/python/index.html), leveraging the NLP capabilities (tokenization, part-of-speech tagging) of the [SparkNLP](https://sparknlp.org/) Package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntactic and Lexical Complexity Scoring Tasks\n",
    "The **Syntactic and Lexical Complexity Score** is computed through a series of tasks:\n",
    "\n",
    "1. **NLPTask**: Leverages SparkNLP to tokenize the review text and assign part-of-speech (POS) tags to each word.\n",
    "2. **ComputeSyntacticStatsTask**: Calculates syntactic metrics, such as raw counts of each POS and the proportions of review text represented by each POS.\n",
    "3. **ComputeLexicalStatsTask**: Computes lexical complexity statistics, including unique word counts, word proportions, and word length metrics.\n",
    "4. **ComputeSyntacticLexicalComplexityScore**: Combines the syntactic and lexical metrics to produce a comprehensive complexity score.\n",
    "\n",
    "Let's dive in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLPTask\n",
    "The **NLPTask** class utilizes a Spark ML Pipeline to streamline NLP preprocessing on text data within a Spark DataFrame. It begins by assembling raw text into a structured document format using SparkNLP's `DocumentAssembler`. The input is then tokenized, preparing it for POS tagging with a pretrained `PerceptronModel`. Finally, the `Finisher` stage converts the processed NLP annotations into plain lists of tokens and POS tags, making them easily accessible and usable in the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -r 19-142 discover/flow/data_prep/tqa/task.py\n",
    "import math\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "import seaborn as sns\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from sparknlp.annotator import PerceptronModel, Tokenizer\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "\n",
    "from discover.flow.base.task import Task\n",
    "from discover.infra.service.logging.task import task_logger\n",
    "from discover.infra.utils.file.io import IOService\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "sns.set_style(\"white\")\n",
    "sns.set_palette(\"Blues_r\")\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "#                                       NLP TASK                                                   #\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "class NLPTask(Task):\n",
    "    \"\"\"\n",
    "    A class to perform NLP preprocessing on a specified content column in a Spark DataFrame.\n",
    "    This task includes tokenization, POS tagging, and formatting of the output as plain lists.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    column : str\n",
    "        The name of the column containing content data to process (default is \"content\").\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    run(data: DataFrame) -> DataFrame\n",
    "        Executes the NLP pipeline on the provided DataFrame, adding token and POS tag columns.\n",
    "\n",
    "    _build_pipeline() -> Pipeline\n",
    "        Constructs a Spark ML Pipeline with stages for document assembly, tokenization,\n",
    "        POS tagging, and output formatting using a Finisher.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, column: str = \"content\") -> None:\n",
    "        \"\"\"\n",
    "        Initializes NLPTask with the column to process.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        column : str, optional\n",
    "            The name of the column containing the content data to process (default is \"content\").\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._column = column\n",
    "\n",
    "    @task_logger\n",
    "    def run(self, data: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Executes the NLP pipeline on the input DataFrame, applying tokenization and POS tagging,\n",
    "        and returns the transformed DataFrame with additional columns for tokens and POS tags.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : DataFrame\n",
    "            The Spark DataFrame containing the content data column specified during initialization.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        DataFrame\n",
    "            A transformed Spark DataFrame with new columns: 'tokens' and 'pos', containing lists\n",
    "            of tokens and POS tags, respectively.\n",
    "        \"\"\"\n",
    "        pipeline = self._build_pipeline()\n",
    "        return pipeline.fit(data).transform(data)\n",
    "\n",
    "    def _build_pipeline(self) -> Pipeline:\n",
    "        \"\"\"\n",
    "        Builds and returns a Spark ML Pipeline with stages for document assembly, tokenization,\n",
    "        POS tagging, and a Finisher for output formatting.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Pipeline\n",
    "            A configured Spark Pipeline that performs NLP tasks including tokenization, POS tagging,\n",
    "            and result formatting for easy integration into a DataFrame.\n",
    "        \"\"\"\n",
    "        # Assembles raw content data into a Spark NLP document\n",
    "        document_assembler = (\n",
    "            DocumentAssembler().setInputCol(self._column).setOutputCol(\"document\")\n",
    "        )\n",
    "\n",
    "        # Tokenizer splits words for NLP processing\n",
    "        tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"tokens\")\n",
    "\n",
    "        # POS Tagging with a pretrained model\n",
    "        pos = (\n",
    "            PerceptronModel.pretrained(\"pos_ud_ewt\", \"en\")\n",
    "            .setInputCols([\"document\", \"tokens\"])\n",
    "            .setOutputCol(\"pos_tags\")\n",
    "        )\n",
    "\n",
    "        # Finisher converts annotations to plain lists for DataFrame output\n",
    "        finisher = (\n",
    "            Finisher()\n",
    "            .setInputCols([\"tokens\", \"pos_tags\"])\n",
    "            .setOutputCols([\"tp_tokens\", \"tp_pos\"])\n",
    "        )\n",
    "\n",
    "        # Create and return Pipeline with the defined stages\n",
    "        pipeline = Pipeline(\n",
    "            stages=[\n",
    "                document_assembler,\n",
    "                tokenizer,\n",
    "                pos,\n",
    "                finisher,\n",
    "            ]\n",
    "        )\n",
    "        return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Syntactic Complexity Measures\n",
    "This code processes a Spark DataFrame containing lists of part-of-speech (POS) tags for each text entry. First, it counts the total number of POS tags per entry. It then calculates the counts of specific POS types, such as nouns, verbs, adjectives, and adverbs. Next, it computes the proportion of each POS type relative to the total POS count, ensuring these ratios are only calculated when the total count is greater than zero. Finally, the code cleans up by removing any intermediate columns that are no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -r 146-241 discover/flow/data_prep/tqa/task.py\n",
    "class ComputeSyntacticStatsTask(Task):\n",
    "    \"\"\"\n",
    "    A task to compute Part-of-Speech (POS) statistics for a specified column in a PySpark DataFrame.\n",
    "\n",
    "    This task generates counts and proportions for specific POS tags (nouns, verbs, adjectives, adverbs, determiners)\n",
    "    based on POS tags available in the input DataFrame. These statistics are useful for analyzing the linguistic\n",
    "    characteristics of the text in each row.\n",
    "\n",
    "    Attributes:\n",
    "        column (str): The name of the column containing the text or POS data to analyze. Defaults to \"content\".\n",
    "\n",
    "    Methods:\n",
    "        run(data: DataFrame) -> DataFrame:\n",
    "            Executes the POS statistics calculations on the specified column of the input DataFrame and returns\n",
    "            the DataFrame with the new POS statistics columns.\n",
    "\n",
    "    POS Statistics Columns:\n",
    "        pos_n_nouns (int): The number of noun tags in the text.\n",
    "        pos_n_verbs (int): The number of verb tags in the text.\n",
    "        pos_n_adjectives (int): The number of adjective tags in the text.\n",
    "        pos_n_adverbs (int): The number of adverb tags in the text.\n",
    "        pos_n_determiners (int): The number of determiner tags in the text.\n",
    "        pos_p_nouns (float): The proportion of noun tags relative to the total POS tags.\n",
    "        pos_p_verbs (float): The proportion of verb tags relative to the total POS tags.\n",
    "        pos_p_adjectives (float): The proportion of adjective tags relative to the total POS tags.\n",
    "        pos_p_adverbs (float): The proportion of adverb tags relative to the total POS tags.\n",
    "        pos_p_determiners (float): The proportion of determiner tags relative to the total POS tags.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, column: str = \"content\") -> None:\n",
    "        \"\"\"\n",
    "        Initializes the ComputeSyntacticStatsTask with the specified text or POS column.\n",
    "\n",
    "        Args:\n",
    "            column (str): The name of the column containing the POS data. Defaults to \"content\".\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._column = column\n",
    "\n",
    "    @task_logger\n",
    "    def run(self, data: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Executes the POS statistics calculations on the specified column.\n",
    "\n",
    "        The function calculates the counts and proportions of specific POS tags (nouns, verbs, adjectives, adverbs,\n",
    "        determiners) within each entry of the specified column. The resulting statistics are added as new columns\n",
    "        in the DataFrame.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The input PySpark DataFrame containing the POS tags as a list in the \"tp_pos\" column.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: The input DataFrame with additional POS statistics columns.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: Calculate the total POS tag count per review.\n",
    "            data = data.withColumn(\"pos_count\", F.size(\"tp_pos\"))\n",
    "\n",
    "            # Step 2: Calculate counts of specific POS tags\n",
    "            pos_tags = [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"DET\"]\n",
    "            pos_labels = [\"nouns\", \"verbs\", \"adjectives\", \"adverbs\", \"determiners\"]\n",
    "            for i, tag in enumerate(pos_tags):\n",
    "                try:\n",
    "                    data = data.withColumn(\n",
    "                        f\"pos_n_{pos_labels[i]}\",\n",
    "                        F.expr(f\"size(filter(tp_pos, x -> x = '{tag}'))\"),\n",
    "                    )\n",
    "                except AnalysisException as e:\n",
    "                    raise AnalysisException(f\"Error processing POS tag '{tag}': {e}\")\n",
    "\n",
    "            # Step 3: Calculate ratios/percentages of specific POS tags\n",
    "            for i, tag in enumerate(pos_tags):\n",
    "                try:\n",
    "                    data = data.withColumn(\n",
    "                        f\"pos_p_{pos_labels[i]}\",\n",
    "                        F.when(\n",
    "                            F.col(\"pos_count\") > 0,\n",
    "                            F.col(f\"pos_n_{pos_labels[i]}\") / F.col(\"pos_count\"),\n",
    "                        ).otherwise(0),\n",
    "                    )\n",
    "                except AnalysisException as e:\n",
    "                    raise AnalysisException(\n",
    "                        f\"Error calculating ratio for POS tag '{tag}': {e}\"\n",
    "                    )\n",
    "\n",
    "            # Drop intermediate column if not needed\n",
    "            data = data.drop(\"pos_count\")\n",
    "\n",
    "            return data\n",
    "\n",
    "        except AnalysisException as e:\n",
    "            raise AnalysisException(f\"Column 'tp_pos' not found: {e}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Unexpected error during POS statistics calculation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Lexical Complexity Measures\n",
    "This task calculates various lexical metrics for text data in a PySpark DataFrame. It starts by computing basic character-based statistics, such as the total character count, digit count, and punctuation count, and then calculates their respective proportions. Next, the text is tokenized into words to determine word counts, unique word counts, and the ratio of unique to total words, providing insights into vocabulary richness. The task also measures word length statistics, including the minimum, maximum, mean, and standard deviation, to capture the complexity of the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -r 245-470 discover/flow/data_prep/tqa/task.py\n",
    "class ComputeLexicalStatsTask(Task):\n",
    "    \"\"\"\n",
    "    A task to compute basic text statistics for a specified column in a PySpark DataFrame.\n",
    "\n",
    "    This task generates various statistics for text data, such as character count, digit and punctuation counts,\n",
    "    word count, unique word count, and word length statistics, which are useful for analyzing the content and structure\n",
    "    of text in each row.\n",
    "\n",
    "    Attributes:\n",
    "        column (str): The name of the column containing the text data to analyze. Defaults to \"content\".\n",
    "\n",
    "    Methods:\n",
    "        run(data: DataFrame) -> DataFrame:\n",
    "            Executes the basic statistics calculations on the specified column of the input DataFrame and returns\n",
    "            the DataFrame with the new statistics columns.\n",
    "\n",
    "    Basic Statistics Columns:\n",
    "        stats_char_count (int): The total number of characters in the text.\n",
    "        stats_digits_count (int): The total number of digits in the text.\n",
    "        stats_digits_proportion (float): The proportion of digits to total characters.\n",
    "        stats_special_chars_count (int): The total number of punctuation marks in the text.\n",
    "        stats_special_chars_proportion (float): The proportion of punctuation marks to total characters.\n",
    "        stats_word_count (int): The total number of words in the text.\n",
    "        stats_unique_word_count (int): The total number of unique words in the text.\n",
    "        stats_unique_word_proportion (float): The proportion of unique words to total words.\n",
    "        stats_word_length_min (int): The minimum word length in the text.\n",
    "        stats_word_length_max (int): The maximum word length in the text.\n",
    "        stats_word_length_mean (float): The mean word length in the text.\n",
    "        stats_word_length_std (float): The standard deviation of word lengths in the text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, column: str = \"content\") -> None:\n",
    "        \"\"\"\n",
    "        Initializes the ComputeLexicalStatsTask with the specified text column.\n",
    "\n",
    "        Args:\n",
    "            column (str): The name of the column containing the text data to analyze. Defaults to \"content\".\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._column = column\n",
    "\n",
    "    @task_logger\n",
    "    def run(self, data: DataFrame) -> DataFrame:\n",
    "        try:\n",
    "            # Ensure the column exists\n",
    "            try:\n",
    "                data = data.withColumn(\"stats_char_count\", F.length(self._column))\n",
    "            except AnalysisException as e:\n",
    "                raise AnalysisException(\n",
    "                    f\"Column '{self._column}' not found in DataFrame: {str(e)}\"\n",
    "                )\n",
    "\n",
    "            # 1. Character count\n",
    "            try:\n",
    "                data = data.withColumn(\"stats_char_count\", F.length(self._column))\n",
    "            except Exception as e:\n",
    "                raise ValueError(\n",
    "                    f\"Error calculating character count for column '{self._column}': {str(e)}\"\n",
    "                )\n",
    "\n",
    "            # 2. Digits count\n",
    "            try:\n",
    "                data = data.withColumn(\n",
    "                    \"stats_digits_count\", F.expr(\"regexp_count(content, '[^0-9]')\")\n",
    "                )\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Error calculating digits count: {str(e)}\")\n",
    "\n",
    "            # 3. Digits proportion\n",
    "            try:\n",
    "                data = data.withColumn(\n",
    "                    \"stats_digits_proportion\",\n",
    "                    F.when(\n",
    "                        F.col(\"stats_char_count\") > 0,\n",
    "                        F.col(\"stats_digits_count\") / F.col(\"stats_char_count\"),\n",
    "                    ).otherwise(0),\n",
    "                )\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Error calculating digits proportion: {str(e)}\")\n",
    "\n",
    "            # 4. Special chars count\n",
    "            try:\n",
    "                data = data.withColumn(\n",
    "                    \"stats_special_chars_count\",\n",
    "                    F.expr(\"regexp_count(content, r'[^\\\\w\\\\s]')\"),\n",
    "                )\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Error calculating special chars count: {str(e)}\")\n",
    "\n",
    "            # 5. Special chars proportion\n",
    "            try:\n",
    "                data = data.withColumn(\n",
    "                    \"stats_special_chars_proportion\",\n",
    "                    F.when(\n",
    "                        F.col(\"stats_char_count\") > 0,\n",
    "                        F.col(\"stats_special_chars_count\") / F.col(\"stats_char_count\"),\n",
    "                    ).otherwise(0),\n",
    "                )\n",
    "            except Exception as e:\n",
    "                raise ValueError(\n",
    "                    f\"Error calculating special chars proportion: {str(e)}\"\n",
    "                )\n",
    "\n",
    "            # 6. Punctuation count\n",
    "            try:\n",
    "                data = data.withColumn(\n",
    "                    \"stats_punctuation_count\",\n",
    "                    F.expr(\"regexp_count(content, '[.,!?;:]')\"),\n",
    "                )\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Error calculating punctuation count: {str(e)}\")\n",
    "\n",
    "            # 7. Punctuation proportion\n",
    "            try:\n",
    "                data = data.withColumn(\n",
    "                    \"stats_punctuation_proportion\",\n",
    "                    F.when(\n",
    "                        F.col(\"stats_char_count\") > 0,\n",
    "                        F.col(\"stats_punctuation_count\") / F.col(\"stats_char_count\"),\n",
    "                    ).otherwise(0),\n",
    "                )\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Error calculating punctuation proportion: {str(e)}\")\n",
    "\n",
    "            # 8. Split content into words\n",
    "            try:\n",
    "                data = data.withColumn(\"words\", F.split(F.col(self._column), \"\\\\s+\"))\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Error splitting content into words: {str(e)}\")\n",
    "\n",
    "            # 9. Word count\n",
    "            try:\n",
    "                data = data.withColumn(\"stats_word_count\", F.size(\"words\"))\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Error calculating word count: {str(e)}\")\n",
    "\n",
    "            # 10. Unique word count\n",
    "            try:\n",
    "                data = data.withColumn(\"unique_words\", F.array_distinct(\"words\"))\n",
    "                data = data.withColumn(\n",
    "                    \"stats_unique_word_count\", F.size(\"unique_words\")\n",
    "                )\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Error calculating unique word count: {str(e)}\")\n",
    "\n",
    "            # 11. Unique word proportion\n",
    "            try:\n",
    "                data = data.withColumn(\n",
    "                    \"stats_unique_word_proportion\",\n",
    "                    F.when(\n",
    "                        F.col(\"stats_word_count\") > 0,\n",
    "                        F.col(\"stats_unique_word_count\") / F.col(\"stats_word_count\"),\n",
    "                    ).otherwise(0),\n",
    "                )\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Error calculating unique word proportion: {str(e)}\")\n",
    "\n",
    "            # 12. Word repetition ratio\n",
    "            try:\n",
    "                data = data.withColumn(\n",
    "                    \"stats_word_repetition_ratio\",\n",
    "                    1 - F.col(\"stats_unique_word_proportion\"),\n",
    "                )\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Error calculating word repetition ratio: {str(e)}\")\n",
    "\n",
    "            # Drop intermediate columns\n",
    "            try:\n",
    "                data = data.drop(\"words\", \"unique_words\")\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Error dropping intermediate columns: {str(e)}\")\n",
    "\n",
    "            # 13. Word length statistics\n",
    "            try:\n",
    "                data = data.withColumn(\n",
    "                    \"word_lengths\",\n",
    "                    F.expr(\"transform(split(content, '\\\\\\\\s+'), x -> length(x))\"),\n",
    "                )\n",
    "\n",
    "                # Minimum word length\n",
    "                data = data.withColumn(\n",
    "                    \"stats_word_length_min\", F.array_min(\"word_lengths\")\n",
    "                )\n",
    "\n",
    "                # Maximum word length\n",
    "                data = data.withColumn(\n",
    "                    \"stats_word_length_max\", F.array_max(\"word_lengths\")\n",
    "                )\n",
    "\n",
    "                # Mean word length\n",
    "                data = data.withColumn(\n",
    "                    \"stats_word_length_mean\",\n",
    "                    F.expr(\n",
    "                        \"aggregate(transform(word_lengths, x -> CAST(x AS DOUBLE)), CAST(0.0 AS DOUBLE), (acc, x) -> acc + x) / size(word_lengths)\"\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "                # Standard deviation of word length\n",
    "                data = data.withColumn(\n",
    "                    \"stats_word_length_std\",\n",
    "                    F.when(\n",
    "                        F.size(\"word_lengths\") > 1,\n",
    "                        F.sqrt(\n",
    "                            F.expr(\n",
    "                                \"aggregate(transform(word_lengths, x -> CAST(x AS DOUBLE)), CAST(0.0 AS DOUBLE), (acc, x) -> acc + pow(x - stats_word_length_mean, 2)) / size(word_lengths)\"\n",
    "                            )\n",
    "                        ),\n",
    "                    ).otherwise(0),\n",
    "                )\n",
    "\n",
    "                # Drop intermediate column if not needed\n",
    "                data = data.drop(\"word_lengths\")\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Error calculating word length statistics: {str(e)}\")\n",
    "\n",
    "            return data\n",
    "\n",
    "        except AnalysisException as e:\n",
    "            raise AnalysisException(\n",
    "                f\"Column '{self._column}' not found in DataFrame: {str(e)}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Unexpected error during statistics calculation: {str(e)}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Syntactic/Lexical Complexity Score\n",
    "To evaluate the richness and complexity of text data, this method calculates a comprehensive **SyntacticLexical Complexity Score** composed of three main components:\n",
    "\n",
    "1. **POS Diversity Score**: Using an entropy-based approach, this score quantifies the variety of part-of-speech (POS) tags, such as nouns, verbs, adjectives, and adverbs. It highlights how diverse the grammatical structures are within the text.\n",
    "\n",
    "2. **POS Density Score**: By examining the ratio of total POS tags to the word count, this score captures the intensity of POS usage. It indicates how grammatically dense or varied the language is throughout the text.\n",
    "\n",
    "3. **Lexical Complexity Score**: This metric assesses vocabulary sophistication. It factors in the proportion of unique words, the prevalence of special characters, and the variability in word lengths, offering a detailed look at the text’s lexical intricacy.\n",
    "\n",
    "Together, these scores are combined in a weighted manner to form the overall **SyntacticLexical Complexity Score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -r 475-709 discover/flow/data_prep/tqa/task.py\n",
    "class ComputeSyntacticLexicalScoresTask(Task):\n",
    "    \"\"\"\n",
    "    A task to compute a Text Quality Assessment (TQA) score based on various components\n",
    "    such as POS count, POS diversity, lexical complexity, POS intensity, and TQA quality checks.\n",
    "\n",
    "    Attributes:\n",
    "        pos_diversity_weight (float): The weight assigned to the POS diversity component.\n",
    "        pos_density_weight (float): The weight assigned to the POS intensity component.\n",
    "        lexical_complexity_weight (float): The weight assigned to the lexical complexity component.\n",
    "        column (str): Column containing review text.\n",
    "        new_column (str): The name of the output column to store the computed TQA score.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pos_diversity_weight: float,\n",
    "        pos_density_weight: float,\n",
    "        lexical_complexity_weight: float,\n",
    "        column: str = \"content\",\n",
    "        new_column: str = \"tqa_syntactic_lexical_score\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the ComputeSyntacticLexicalScoresTask with specified weights and output column name.\n",
    "\n",
    "        Args:\n",
    "            pos_diversity_weight (float): Weight for the POS diversity component.\n",
    "            pos_density_weight (float): Weight for the POS intensity component.\n",
    "            lexical_complexity_weight (float): Weight for the lexical complexity component.\n",
    "            new_column (str): Name of the output column for the TQA score. Defaults to \"enrichment_tqa_score1\".\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._pos_diversity_weight = pos_diversity_weight\n",
    "        self._pos_density_weight = pos_density_weight\n",
    "        self._lexical_complexity_weight = lexical_complexity_weight\n",
    "        self._column = column\n",
    "        self._new_column = new_column\n",
    "\n",
    "    @task_logger\n",
    "    def run(self, data: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Executes the TQA score computation by applying several components as UDFs.\n",
    "\n",
    "        This method calculates the following:\n",
    "        - POS diversity score: Measures the diversity of parts of speech used.\n",
    "        - POS density score: Quantifies the density of POS tags in the content.\n",
    "        - Lexical complexity score: Measures the complexity of the language used.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The input PySpark DataFrame containing text data and related features.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: The input DataFrame with additional columns for each component score and the final TQA score.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Ensure the required text column exists\n",
    "            if self._column not in data.columns:\n",
    "                raise AnalysisException(\n",
    "                    f\"Column '{self._column}' not found in DataFrame\"\n",
    "                )\n",
    "\n",
    "            # Define UDFs for each computation\n",
    "            @F.udf(\"float\")\n",
    "            def compute_pos_diversity_score(\n",
    "                content, pos_p_nouns, pos_p_verbs, pos_p_adjectives, pos_p_adverbs\n",
    "            ):\n",
    "                \"\"\"\n",
    "                Computes the POS diversity score using an entropy-based calculation.\n",
    "\n",
    "                The POS diversity score is computed based on the proportions of different\n",
    "                parts of speech in the content, with a higher score indicating more variety\n",
    "                in the types of POS tags used.\n",
    "\n",
    "                Args:\n",
    "                    content (str): The text content of the review.\n",
    "                    pos_p_nouns (float): Proportion of nouns in the content.\n",
    "                    pos_p_verbs (float): Proportion of verbs in the content.\n",
    "                    pos_p_adjectives (float): Proportion of adjectives in the content.\n",
    "                    pos_p_adverbs (float): Proportion of adverbs in the content.\n",
    "\n",
    "                Returns:\n",
    "                    float: The computed POS diversity score.\n",
    "                \"\"\"\n",
    "                try:\n",
    "                    if content and len(content) > 2:\n",
    "                        pos_tags = [\n",
    "                            pos_p_nouns,\n",
    "                            pos_p_verbs,\n",
    "                            pos_p_adjectives,\n",
    "                            pos_p_adverbs,\n",
    "                        ]\n",
    "                        pos_diversity = -sum(p * math.log(p) for p in pos_tags if p > 0)\n",
    "                        return float(pos_diversity * self._pos_diversity_weight)\n",
    "                    return 0.0\n",
    "                except Exception as e:\n",
    "                    raise ValueError(f\"Error in compute_pos_diversity_score: {str(e)}\")\n",
    "\n",
    "            @F.udf(\"float\")\n",
    "            def compute_pos_density_score(\n",
    "                content,\n",
    "                pos_n_nouns,\n",
    "                pos_n_verbs,\n",
    "                pos_n_adjectives,\n",
    "                pos_n_adverbs,\n",
    "                stats_word_count,\n",
    "            ):\n",
    "                \"\"\"\n",
    "                Computes the POS intensity score based on the number of POS tags relative\n",
    "                to the word count.\n",
    "\n",
    "                This score quantifies the density of specific parts of speech in the content,\n",
    "                with a higher score indicating a more \"dense\" use of those parts of speech.\n",
    "\n",
    "                Args:\n",
    "                    content (str): The text content of the review.\n",
    "                    pos_n_nouns (int): Number of nouns in the content.\n",
    "                    pos_n_verbs (int): Number of verbs in the content.\n",
    "                    pos_n_adjectives (int): Number of adjectives in the content.\n",
    "                    pos_n_adverbs (int): Number of adverbs in the content.\n",
    "                    stats_word_count (int): Total word count in the content.\n",
    "\n",
    "                Returns:\n",
    "                    float: The computed POS intensity score.\n",
    "                \"\"\"\n",
    "                try:\n",
    "                    if content and len(content) > 2 and stats_word_count > 0:\n",
    "                        pos_density = (\n",
    "                            pos_n_nouns + pos_n_verbs + pos_n_adjectives + pos_n_adverbs\n",
    "                        ) / stats_word_count\n",
    "                        return float(pos_density * self._pos_density_weight)\n",
    "                    return 0.0\n",
    "                except Exception as e:\n",
    "                    raise ValueError(f\"Error in compute_pos_density_score: {str(e)}\")\n",
    "\n",
    "            @F.udf(\"float\")\n",
    "            def compute_lexical_complexity_score(\n",
    "                content,\n",
    "                stats_unique_word_proportion,\n",
    "                stats_special_chars_proportion,\n",
    "                stats_word_length_std,\n",
    "            ):\n",
    "                \"\"\"\n",
    "                Computes the lexical complexity score based on unique word proportion,\n",
    "                special character proportion, and word length standard deviation.\n",
    "\n",
    "                This score measures the complexity of the language used in the content,\n",
    "                with a higher score indicating more complexity (e.g., more unique words,\n",
    "                special characters, or varied word lengths).\n",
    "\n",
    "                Args:\n",
    "                    content (str): The text content of the review.\n",
    "                    stats_unique_word_proportion (float): Proportion of unique words in the content.\n",
    "                    stats_special_chars_proportion (float): Proportion of special characters in the content.\n",
    "                    stats_word_length_std (float): Standard deviation of word lengths in the content.\n",
    "\n",
    "                Returns:\n",
    "                    float: The computed lexical complexity score.\n",
    "                \"\"\"\n",
    "                try:\n",
    "                    if content and len(content) > 2:\n",
    "                        lexical_complexity = (\n",
    "                            0.4 * stats_unique_word_proportion\n",
    "                            + 0.3 * stats_special_chars_proportion\n",
    "                            + 0.3 * stats_word_length_std\n",
    "                        )\n",
    "                        return float(\n",
    "                            lexical_complexity * self._lexical_complexity_weight\n",
    "                        )\n",
    "                    return 0.0\n",
    "                except Exception as e:\n",
    "                    raise ValueError(\n",
    "                        f\"Error in compute_lexical_complexity_score: {str(e)}\"\n",
    "                    )\n",
    "\n",
    "            # Apply UDFs to create new columns\n",
    "            try:\n",
    "                data = data.withColumn(\n",
    "                    \"tqm_pos_diversity_score\",\n",
    "                    compute_pos_diversity_score(\n",
    "                        F.col(self._column),\n",
    "                        F.col(\"pos_p_nouns\"),\n",
    "                        F.col(\"pos_p_verbs\"),\n",
    "                        F.col(\"pos_p_adjectives\"),\n",
    "                        F.col(\"pos_p_adverbs\"),\n",
    "                    ),\n",
    "                )\n",
    "                data = data.withColumn(\n",
    "                    \"tqm_pos_density_score\",\n",
    "                    compute_pos_density_score(\n",
    "                        F.col(self._column),\n",
    "                        F.col(\"pos_n_nouns\"),\n",
    "                        F.col(\"pos_n_verbs\"),\n",
    "                        F.col(\"pos_n_adjectives\"),\n",
    "                        F.col(\"pos_n_adverbs\"),\n",
    "                        F.col(\"stats_word_count\"),\n",
    "                    ),\n",
    "                )\n",
    "                data = data.withColumn(\n",
    "                    \"tqm_lexical_complexity_score\",\n",
    "                    compute_lexical_complexity_score(\n",
    "                        F.col(self._column),\n",
    "                        F.col(\"stats_unique_word_proportion\"),\n",
    "                        F.col(\"stats_special_chars_proportion\"),\n",
    "                        F.col(\"stats_word_length_std\"),\n",
    "                    ),\n",
    "                )\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\n",
    "                    f\"Error applying UDFs to calculate lexical complexity score: {str(e)}\"\n",
    "                )\n",
    "\n",
    "            # Calculate the TQA score as a weighted combination of components\n",
    "            try:\n",
    "                data = data.withColumn(\n",
    "                    self._new_column,\n",
    "                    F.col(\"tqm_pos_diversity_score\")\n",
    "                    + F.col(\"tqm_lexical_complexity_score\")\n",
    "                    + F.col(\"tqm_pos_density_score\"),\n",
    "                )\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\n",
    "                    f\"Error calculating lexical complexity score: {str(e)}\"\n",
    "                )\n",
    "\n",
    "            return data\n",
    "\n",
    "        except AnalysisException as e:\n",
    "            raise AnalysisException(\n",
    "                f\"Column '{self._column}' not found in DataFrame: {str(e)}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Unexpected error during lexical complexity score calculation: {str(e)}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes the **Syntactic and Lexical Complexity Scoring** component. Next, we move on to **Perplexity-Based Coherence Scoring**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Perplexity-Based Coherence Score Pipeline\n",
    "The perplexity-based coherence scores are computed in three steps.\n",
    "\n",
    "1. **ComputePerplexityFilters**: Binary indicators for 14 data quality heuristics are added to the dataset. They indicate the presence of key POS tags that are important in sentiment analysis. We will denote the matrix of these indicators $I$, where $I_i{(\\text{review})}=1$ indicates that the review satisfies the $i^{th}$ filter criteria.  \n",
    "\n",
    "2. **ComputePerplexityWeights**: Each of the 14 filters are applied to the dataset, creating 14 subsets. The average perplexity of each subset is computed, and the perplexity weight reflects the degree to which each filter reduces complexity as follows:\n",
    "$$w_i=\\text{max}\\bigg(0,\\frac{PP_{all}-PP_i}{PP_{all}}\\bigg)$$\n",
    "where $PP_{all}$ is average perplexity of the full dataset, and $P_i$ is the average perplexity of the dataset with the $i^{th}$ filter applied.  \n",
    "\n",
    "3. **ComputeCoherenceScores**: Perplexity-based coherence scores are computed as a weighted sum of the indicator matrix $I$ and the weights $w$ created above as:\n",
    "$$\\text{score}_{review}=\\frac{\\displaystyle\\sum_{i=1}^Fw_iI_i(\\text{review})}{\\displaystyle\\sum_{i=1}^Fw_i}$$ \n",
    "\n",
    "Here's the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Perplexity Filters\n",
    "This task creates binary indicators for 14 filters, each selected from over 50+ filters based on consistent perplexity improvements {cite}`sharmaTextQualityBasedPruning2024`.\n",
    "\n",
    "1. **Linguistic Features**: New columns are created to indicate whether the text contains specific parts of speech:\n",
    "   - **Adjective**: `tqf_has_adjective` checks for at least one adjective.\n",
    "   - **Adverb**: `tqf_has_adverb` checks for at least one adverb.\n",
    "   - **Determiner**: `tqf_has_determiner` checks for at least one determiner.\n",
    "   - **Noun**: `tqf_has_noun` checks for at least one noun.\n",
    "   - **Verb**: `tqf_has_verb` checks for at least one verb.\n",
    "\n",
    "2. **Punctuation and Special Character Checks**:\n",
    "   - **Terminal Punctuation**: `tqf_has_terminal_punctuation` checks if the text ends with a period, exclamation mark, or question mark.\n",
    "   - **High Special Characters Ratio**: `tqf_high_special_chars_ratio` is `True` if special characters make up more than 25% of the text.\n",
    "   - **High Punctuation Ratio**: `tqf_high_punctuation_ratio` is `True` if punctuation makes up more than 25% of the text.\n",
    "\n",
    "3. **Content Length and Structure**:\n",
    "   - **Word Count Range**: `tqf_word_count_range` is `True` if the word count is between 4 and 255 words.\n",
    "   - **Stop Word Match**: `tqf_stop_word_match` checks if at least two common stop words are present in the text.\n",
    "   - **First Letter Capitalized**: `tqf_first_letter_cap` indicates if the first letter of the text is uppercase.\n",
    "   - **Not All Caps**: `tqf_no_all_caps` is `True` if the text is not written entirely in uppercase.\n",
    "\n",
    "4. **Word Repetition and Special Character Checks**:\n",
    "   - **High Word Repetition**: `tqf_high_word_repetition` flags text with a word repetition ratio of 20% or higher.\n",
    "   - **No Special Characters**: `tqf_no_special_chars` is `True` if the text contains no special characters beyond standard punctuation.\n",
    "\n",
    "Finally, the code drops unnecessary columns for tokens and POS tags, returning the enriched DataFrame with these new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -r 713-854 discover/flow/data_prep/tqa/task.py\n",
    "class ComputePerplexityFiltersTask(Task):\n",
    "    \"\"\"\n",
    "    A task to compute Text Quality Assessment (TQA) statistics for reviews in a PySpark DataFrame.\n",
    "\n",
    "    This task generates various boolean flags based on the presence of certain parts of speech, punctuation patterns,\n",
    "    and statistical ratios in the review text. These flags can be used to assess the quality and characteristics\n",
    "    of each review.\n",
    "\n",
    "    Methods:\n",
    "        run(data: DataFrame) -> DataFrame:\n",
    "            Executes the TQA statistics calculations on the specified columns of the input DataFrame and returns the\n",
    "            DataFrame with the new TQA columns.\n",
    "\n",
    "    TQA Filter Columns:\n",
    "        tqf_has_adjective (bool): True if the review has at least one adjective.\n",
    "        tqf_has_adverb (bool): True if the review has at least one adverb.\n",
    "        tqf_has_determiner (bool): True if the review has at least one determiner.\n",
    "        tqf_has_noun (bool): True if the review has at least one noun.\n",
    "        tqf_has_terminal_punctuation (bool): True if the review contains terminal punctuation (., !, or ?).\n",
    "        tqf_has_verb (bool): True if the review has at least one verb.\n",
    "        tqf_high_digit_ratio (bool): True if the ratio of digits to words is greater than 0.25.\n",
    "        tqf_high_punctuation_ratio (bool): True if the ratio of punctuation to words is greater than 0.25.\n",
    "        tqf_word_count_range (bool): True if the word count is between 3 and 256.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    @task_logger\n",
    "    def run(self, data: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Executes a series of transformations on the input DataFrame to compute various\n",
    "        textual quality features (TQF) based on parts of speech and other textual statistics.\n",
    "\n",
    "        The following TQF features are computed:\n",
    "        - Whether the review has at least one adjective, adverb, determiner, noun, or verb\n",
    "        - Whether the review contains terminal punctuation (., !, ?)\n",
    "        - Whether the special characters to word ratio is greater than 0.25\n",
    "        - Whether the punctuation to word ratio is greater than 0.25\n",
    "        - Whether the word count is within a specified range\n",
    "        - Whether stop words appear in the review\n",
    "        - Whether the first letter is capitalized\n",
    "        - Whether the content is in all caps\n",
    "        - Whether the word repetition ratio is high\n",
    "        - Whether the review contains special characters\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The input PySpark DataFrame containing text data and related features.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: The input DataFrame with additional columns for each computed TQF feature.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 1. Whether review has at least one adjective\n",
    "            data = data.withColumn(\"tqf_has_adjective\", F.col(\"pos_n_adjectives\") > 0)\n",
    "\n",
    "            # 2. Whether review has at least one adverb\n",
    "            data = data.withColumn(\"tqf_has_adverb\", F.col(\"pos_n_adverbs\") > 0)\n",
    "\n",
    "            # 3. Whether review has at least one determiner\n",
    "            data = data.withColumn(\"tqf_has_determiner\", F.col(\"pos_n_determiners\") > 0)\n",
    "\n",
    "            # 4. Whether review has at least one noun\n",
    "            data = data.withColumn(\"tqf_has_noun\", F.col(\"pos_n_nouns\") > 0)\n",
    "\n",
    "            # 5. Whether the review contains terminal punctuation (., !, or ?)\n",
    "            data = data.withColumn(\n",
    "                \"tqf_has_terminal_punctuation\", F.col(\"content\").rlike(\"[.!?]$\")\n",
    "            )\n",
    "\n",
    "            # 6. Whether review has at least one verb\n",
    "            data = data.withColumn(\"tqf_has_verb\", F.col(\"pos_n_verbs\") > 0)\n",
    "\n",
    "            # 7. Whether special characters to words ratio is greater than 0.25\n",
    "            data = data.withColumn(\n",
    "                \"tqf_high_special_chars_ratio\",\n",
    "                F.col(\"stats_special_chars_proportion\") > 0.25,\n",
    "            )\n",
    "\n",
    "            # 8. Whether punctuation to words ratio is greater than 0.25\n",
    "            data = data.withColumn(\n",
    "                \"tqf_high_punctuation_ratio\",\n",
    "                F.col(\"stats_punctuation_proportion\") > 0.25,\n",
    "            )\n",
    "\n",
    "            # 9. Whether word count is in the range > 3 and < 256\n",
    "            data = data.withColumn(\n",
    "                \"tqf_word_count_range\",\n",
    "                (F.col(\"stats_word_count\") > 3) & (F.col(\"stats_word_count\") < 256),\n",
    "            )\n",
    "\n",
    "            # 10. Stop word match\n",
    "            # List of stop words to search for\n",
    "            stop_words = [\"the\", \"be\", \"to\", \"of\", \"and\", \"that\", \"have\", \"with\"]\n",
    "\n",
    "            # Create conditions for each stop word\n",
    "            conditions = [\n",
    "                F.expr(f\"array_contains(split(content, ' '), '{word}')\").cast(\"int\")\n",
    "                for word in stop_words\n",
    "            ]\n",
    "\n",
    "            # Sum the conditions and check if at least 2 stop words are present\n",
    "            data = data.withColumn(\n",
    "                \"tqf_stop_word_match\",\n",
    "                F.when(sum(conditions) >= 2, True).otherwise(False),\n",
    "            )\n",
    "\n",
    "            # 11. Create a new column \"tqf_first_letter_cap\" based on the first letter being uppercase\n",
    "            data = data.withColumn(\n",
    "                \"tqf_first_letter_cap\",\n",
    "                F.expr(\"substring(content, 1, 1) rlike '^[A-Z]'\"),\n",
    "            )\n",
    "\n",
    "            # 12. Create a new column \"tqf_no_all_caps\" based on whether the content is all caps\n",
    "            data = data.withColumn(\n",
    "                \"tqf_no_all_caps\", ~F.col(\"content\").rlike(\"^[^a-z]*$\")\n",
    "            )\n",
    "\n",
    "            # 13. Create a new column \"tqf_high_word_repetition\" if 'stats_word_repetition_ratio' >= 0.2\n",
    "            data = data.withColumn(\n",
    "                \"tqf_high_word_repetition\", F.col(\"stats_word_repetition_ratio\") >= 0.2\n",
    "            )\n",
    "\n",
    "            # Define the regex pattern for special characters (non-alphanumeric, non-punctuation)\n",
    "            special_chars_pattern = r\"[^a-zA-Z0-9\\s.,!?;:'\\\"()\\-]\"\n",
    "\n",
    "            # Set tqf_no_special_chars to True if content has no special characters\n",
    "            data = data.withColumn(\n",
    "                \"tqf_no_special_chars\", ~F.col(\"content\").rlike(special_chars_pattern)\n",
    "            )\n",
    "\n",
    "            # Delete tokens and POS tags from dataset\n",
    "            data = data.drop(\"tp_tokens\", \"tp_pos\")\n",
    "\n",
    "            return data\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Unexpected error during perplexity filter computation: {str(e)}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute and Save Perplexity Weights\n",
    "This task computes weights based on the average perplexity of text reviews, leveraging a set of predefined filters. Here's how it works:\n",
    "\n",
    "1. **Check for Existing Weights**: The task first checks if the perplexity weights file already exists at the specified file path (`self._pp_filepath`). If the file exists, the task skips the computation.\n",
    "\n",
    "2. **Calculate Overall Average Perplexity**: The code computes the overall average perplexity (`pp_all`) across all reviews in the input DataFrame.\n",
    "\n",
    "3. **Identify and Sort Filters**: It gathers all columns in the DataFrame that start with a specific prefix (`self._pp_filter_prefix`), which correspond to different text quality filters. These filters are then sorted.\n",
    "\n",
    "4. **Compute Weights for Each Filter**:\n",
    "   - For each filter, the code filters the DataFrame to include only reviews where the filter condition is `True`.\n",
    "   - It then calculates the average perplexity for this subset of reviews (`avg_perplexity_i`).\n",
    "   - The weight for each filter is calculated as `(pp_all - avg_perplexity_i) / pp_all`, reflecting how much the filtered subset differs from the overall average perplexity.\n",
    "\n",
    "5. **Save Weights to File**: The computed weights are converted into a Pandas DataFrame, formatted, and saved to a file using `IOService.write()`.\n",
    "\n",
    "This task generates and saves weights that quantify the impact of various text quality filters on review perplexity, providing a basis for coherence assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -r 858-962 discover/flow/data_prep/tqa/task.py\n",
    "class ComputePerplexityWeights(Task):\n",
    "    \"\"\"\n",
    "    A class to compute and save perplexity weights for various filters in a PySpark DataFrame.\n",
    "\n",
    "    This task calculates the overall average perplexity and the average perplexity for each\n",
    "    filter column that starts with a specified prefix. It then computes weights for each filter\n",
    "    and writes the results to a specified file.\n",
    "\n",
    "    Attributes:\n",
    "        _column (str): The name of the column containing perplexity values.\n",
    "        _pp_filepath (str): The file path to save the perplexity weights.\n",
    "        _pp_filter_prefix (str): The prefix used to identify filter columns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        column: str = \"an_perplexity\",\n",
    "        pp_filepath: str = \"models/tqa/pp_weights.csv\",\n",
    "        pp_filter_prefix: str = \"tqf_\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the ComputePerplexityWeights class with the specified parameters.\n",
    "\n",
    "        Args:\n",
    "            column (str): The name of the column containing perplexity values. Defaults to 'an_perplexity'.\n",
    "            pp_filepath (str): The file path to save the perplexity weights. Defaults to 'models/tqa/pp_weights.csv'.\n",
    "            pp_filter_prefix (str): The prefix used to identify filter columns. Defaults to 'tqf_'.\n",
    "        \"\"\"\n",
    "        self._column = column\n",
    "        self._pp_filepath = pp_filepath\n",
    "        self._pp_filter_prefix = pp_filter_prefix\n",
    "\n",
    "    @task_logger\n",
    "    def run(self, data: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Computes and saves perplexity weights for filter columns in the DataFrame.\n",
    "\n",
    "        The method calculates the overall average perplexity and the average perplexity\n",
    "        for each filter column. It then computes a weight for each filter based on the\n",
    "        difference between the overall and individual perplexity values, and saves the\n",
    "        weights to a specified file.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The input PySpark DataFrame containing the perplexity values and filter columns.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: The input DataFrame (unmodified).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(self._pp_filepath):\n",
    "                # Calculate the overall average perplexity\n",
    "                pp_all = data.agg(F.avg(self._column)).first()[0]\n",
    "                if pp_all is None:\n",
    "                    raise ValueError(\n",
    "                        f\"Could not calculate the overall average perplexity for column '{self._column}'\"\n",
    "                    )\n",
    "\n",
    "                w_i = {}\n",
    "\n",
    "                # Get all columns that start with the filter prefix and sort them\n",
    "                filters = [\n",
    "                    col\n",
    "                    for col in data.columns\n",
    "                    if col.startswith(self._pp_filter_prefix)\n",
    "                ]\n",
    "                filters = sorted(filters)\n",
    "\n",
    "                # Compute the average perplexity and the weights for each filter\n",
    "                for filter in filters:\n",
    "                    try:\n",
    "                        filtered_data = data.filter(F.col(filter) == True)  # noqa\n",
    "                        avg_perplexity_i = filtered_data.agg(\n",
    "                            F.avg(self._column)\n",
    "                        ).first()[0]\n",
    "                        if avg_perplexity_i is None:\n",
    "                            raise ValueError(\n",
    "                                f\"Could not calculate the average perplexity for filter '{filter}'\"\n",
    "                            )\n",
    "\n",
    "                        w_i[filter] = (pp_all - avg_perplexity_i) / pp_all\n",
    "\n",
    "                    except Exception as filter_exception:\n",
    "                        raise RuntimeError(\n",
    "                            f\"Error calculating perplexity for filter '{filter}': {str(filter_exception)}\"\n",
    "                        )\n",
    "\n",
    "                # Convert the weights dictionary to a DataFrame and write to file\n",
    "                weights = pd.DataFrame.from_dict(\n",
    "                    w_i, orient=\"index\", columns=[\"weight\"]\n",
    "                ).reset_index(names=\"filter\")\n",
    "                try:\n",
    "                    IOService.write(data=weights, filepath=self._pp_filepath)\n",
    "                except Exception as write_exception:\n",
    "                    raise RuntimeError(\n",
    "                        f\"Error writing perplexity weights to file '{self._pp_filepath}': {str(write_exception)}\"\n",
    "                    )\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Unexpected error during perplexity weight computation: {str(e)}\"\n",
    "            )\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Perplexity-Based Coherence Score\n",
    "The ComputeCoherenceScoreTask class calculates a coherence score for text data based on weighted indicators. \n",
    "\n",
    "1. **Initialization**: \n",
    "   - The class is initialized with parameters for the output column name (`new_column`) and the file path to the perplexity weights (`pp_filepath`).\n",
    "   - It loads the weights from the specified file, which are used to compute the coherence score.\n",
    "\n",
    "2. **Loading Weights**:\n",
    "   - The weights are read from a file into a Pandas DataFrame and then converted to a PySpark DataFrame for further processing.\n",
    "   - These weights are collected into a list of dictionaries, making them iterable for constructing the weighted sum expression.\n",
    "\n",
    "3. **Score Calculation**:\n",
    "   - The total weight is computed by summing all individual weights.\n",
    "   - A weighted sum expression is constructed by multiplying each filter indicator (converted to double) by its corresponding weight and normalizing by the total weight.\n",
    "   - The coherence score is added to the DataFrame as a new column, specified by `new_column`.\n",
    "\n",
    "4. **Output**: \n",
    "   - The method returns the modified PySpark DataFrame, now enriched with the computed coherence score.\n",
    "\n",
    "This class calculates a coherence score that reflects the overall text quality by leveraging precomputed perplexity weights and binary filter indicators, providing a quantitative measure of coherence for each text entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -r 966-1050 discover/flow/data_prep/tqa/task.py\n",
    "class ComputeCoherenceScoreTask(Task):\n",
    "    \"\"\"\n",
    "    A task to compute a Text Quality Assessment (TQA) coherence score based on binary filter indicators\n",
    "    and pre-computed perplexity weights.\n",
    "\n",
    "    This task reads perplexity weights from a specified file, computes the weighted sum of binary filter indicators,\n",
    "    normalizes the result, and adds the computed TQA coherence score as a new column to the input DataFrame.\n",
    "\n",
    "    Attributes:\n",
    "        _new_column (str): The name of the new column to store the computed TQA coherence score.\n",
    "        _pp_filepath (str): The file path to the CSV containing perplexity weights.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        new_column: str = \"tqa_coherence_score\",\n",
    "        pp_filepath: str = \"models/tqa/pp_weights.csv\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._new_column = new_column\n",
    "        self._pp_filepath = pp_filepath\n",
    "\n",
    "    @task_logger\n",
    "    def run(self, data: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Computes the TQA score based on the binary indicators and the assigned weights.\n",
    "\n",
    "        This method loads perplexity weights from a CSV file, computes the weighted sum of binary filter indicators,\n",
    "        normalizes the result by the total weight, and adds the computed TQA score to the DataFrame.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The input PySpark DataFrame containing binary indicators for filters.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: The input DataFrame with an additional column for the computed TQA score.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the perplexity weights file cannot be loaded.\n",
    "            ValueError: If the total weight is zero, preventing TQA score computation.\n",
    "            RuntimeError: If an unexpected error occurs during TQA score computation.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load perplexity weights from the specified file\n",
    "            try:\n",
    "                self._weights_pandas = IOService.read(self._pp_filepath).reset_index(\n",
    "                    drop=True\n",
    "                )\n",
    "            except Exception as e:\n",
    "                raise FileNotFoundError(\n",
    "                    f\"Failed to load perplexity weights from '{self._pp_filepath}': {str(e)}\"\n",
    "                )\n",
    "\n",
    "            # Convert the Pandas DataFrame to a Spark DataFrame\n",
    "            weights_spark = ps.DataFrame(self._weights_pandas).to_spark()\n",
    "\n",
    "            # Collect weights into a list of dictionaries\n",
    "            weights_list = weights_spark.collect()\n",
    "\n",
    "            # Compute the total sum of the weights\n",
    "            total_weight = sum(item[\"weight\"] for item in weights_list)\n",
    "\n",
    "            if total_weight == 0:\n",
    "                raise ValueError(\"The total weight is zero. Cannot compute TQA score.\")\n",
    "\n",
    "            # Compute the weighted sum of filter indicators\n",
    "            filter_sum_expr = sum(\n",
    "                [\n",
    "                    F.col(item[\"filter\"]).cast(\"double\") * F.lit(item[\"weight\"])\n",
    "                    for item in weights_list\n",
    "                ]\n",
    "            ) / F.lit(\n",
    "                total_weight\n",
    "            )  # Normalize by the total sum of the weights\n",
    "\n",
    "            # Add the computed TQA score as a new column\n",
    "            data = data.withColumn(self._new_column, filter_sum_expr)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Unexpected error during TQA score computation: {str(e)}\"\n",
    "            )\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Text Quality Score\n",
    "Finally, we compute the text quality score as a composite of the **Syntactic/Lexical Complexity Score** and the **Perplexity-Based Coherence Score**. We first, scale each score to the range [0,1] using minmax scaling. Then the final score is weighted sum of both scaled scores as follows:\n",
    "\n",
    "$$\\text{score}_{review}=w_1\\times{\\text{SLC}}_{review} + w_2\\times{\\text{PBC}}_{review}$$ \n",
    "where $\\text{SLC}_{review}$ is the **Syntactic/Lexical Complexity Score** and $\\text{PBC}_{review}$ is the **Perplexity-Based Coherence Score**. Weights $w_1$ and $w_2$ are the weights for the **Syntactic/Lexical Complexity Score** and the **Perplexity-Based Coherence Score**, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -r 1054-1167 discover/flow/data_prep/tqa/task.py\n",
    "class ComputeTextQualityScore(Task):\n",
    "    \"\"\"\n",
    "    A task to compute a final Text Quality Assessment (TQA) score by normalizing and combining two TQA scores\n",
    "    using specified weights.\n",
    "\n",
    "    Attributes:\n",
    "        new_column (str): Column containing the final text quality score.\n",
    "        tqa_syntactic_weight (float): The weight assigned to the first TQA score. Defaults to 0.4.\n",
    "        tqa_perplexity_weight (float): The weight assigned to the second TQA score. Defaults to 0.6.\n",
    "        _data (DataFrame): The DataFrame holding the data after computation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        new_column: str = \"tqa_score\",\n",
    "        tqa_syntactic_weight: float = 0.4,\n",
    "        tqa_perplexity_weight: float = 0.6,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the ComputeTextQualityScore with specified weights for combining the two TQA scores.\n",
    "\n",
    "        Args:\n",
    "            new_column (str): The name of the output column for the final TQA score. Defaults to \"tqa_score\".\n",
    "            tqa_syntactic_weight (float): Weight for the first TQA score. Defaults to 0.4.\n",
    "            tqa_perplexity_weight (float): Weight for the second TQA score. Defaults to 0.6.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._new_column = new_column\n",
    "        self._tqa_syntactic_weight = tqa_syntactic_weight\n",
    "        self._tqa_perplexity_weight = tqa_perplexity_weight\n",
    "\n",
    "    @task_logger\n",
    "    def run(self, data: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Normalizes two TQA scores to the range [0, 1] and computes a final TQA score\n",
    "        using the specified weights.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The input PySpark DataFrame containing \"tqa_syntactic_lexical_score\"\n",
    "                              and \"tqa_coherence_score\" columns.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: The PySpark DataFrame with normalized scores and the final combined TQA score.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Normalize both scores to [0, 1]\n",
    "            min_max_enrichment_tqa_score1 = data.select(\n",
    "                F.min(\"tqa_syntactic_lexical_score\"),\n",
    "                F.max(\"tqa_syntactic_lexical_score\"),\n",
    "            ).first()\n",
    "\n",
    "            if min_max_enrichment_tqa_score1[1] == min_max_enrichment_tqa_score1[0]:\n",
    "                raise ValueError(\n",
    "                    \"Syntactic lexical score has no variation. Cannot normalize.\"\n",
    "                )\n",
    "\n",
    "            min_enrichment_tqa_score1, max_enrichment_tqa_score1 = (\n",
    "                min_max_enrichment_tqa_score1\n",
    "            )\n",
    "\n",
    "            min_max_enrichment_tqa_score2 = data.select(\n",
    "                F.min(\"tqa_coherence_score\"), F.max(\"tqa_coherence_score\")\n",
    "            ).first()\n",
    "\n",
    "            if min_max_enrichment_tqa_score2[1] == min_max_enrichment_tqa_score2[0]:\n",
    "                raise ValueError(\"Coherence score has no variation. Cannot normalize.\")\n",
    "\n",
    "            min_enrichment_tqa_score2, max_enrichment_tqa_score2 = (\n",
    "                min_max_enrichment_tqa_score2\n",
    "            )\n",
    "\n",
    "            # Apply Min-Max normalization to both scores\n",
    "            data = data.withColumn(\n",
    "                \"tqa_syntactic_lexical_score\",\n",
    "                (F.col(\"tqa_syntactic_lexical_score\") - min_enrichment_tqa_score1)\n",
    "                / (max_enrichment_tqa_score1 - min_enrichment_tqa_score1),\n",
    "            )\n",
    "\n",
    "            data = data.withColumn(\n",
    "                \"tqa_coherence_score\",\n",
    "                (F.col(\"tqa_coherence_score\") - min_enrichment_tqa_score2)\n",
    "                / (max_enrichment_tqa_score2 - min_enrichment_tqa_score2),\n",
    "            )\n",
    "\n",
    "            # Combine scores using weights\n",
    "            data = data.withColumn(\n",
    "                self._new_column,\n",
    "                self._tqa_syntactic_weight * F.col(\"tqa_syntactic_lexical_score\")\n",
    "                + self._tqa_perplexity_weight * F.col(\"tqa_coherence_score\"),\n",
    "            )\n",
    "\n",
    "            # Compute min and max of the new column\n",
    "            min_value = data.agg(F.min(self._new_column)).collect()[0][0]\n",
    "            max_value = data.agg(F.max(self._new_column)).collect()[0][0]\n",
    "\n",
    "            if max_value == min_value:\n",
    "                raise ValueError(\n",
    "                    f\"Final TQA score has no variation. Min and max values are both {min_value}.\"\n",
    "                )\n",
    "\n",
    "            # Apply Min-Max transformation to the final TQA score\n",
    "            data = data.withColumn(\n",
    "                self._new_column,\n",
    "                (F.col(self._new_column) - min_value) / (max_value - min_value),\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Unexpected error during TQA score computation: {str(e)}\"\n",
    "            )\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, we've defined the tasks that assess the quality of review text. Next, we'll construct the pipeline from configuration to execution.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Text Quality Analysis (TQA) Pipeline\n",
    "The orchestration architecture components integrate, manifesting configuration-driven pipelines of tasks dynamically parameterized and instantiated at runtime. \n",
    "\n",
    "- **Pipeline Blueprint**: The blueprints for pipelines are defined in YAML, and are comprised of source and destination dataset configurations, as well as a sequence parameterized tasks\n",
    "- **Dataset**: The central character in this story, `Dataset` objects encapsulate the core review data as it progresses through the pipeline and into analysis.\n",
    "- **DatasetRepo**: Responsible for `Dataset` persistence, the `DatasetRepo` ensures immutability, and dataset accessibility.\n",
    "- **Task**: Parameterized, atomic units of work, these classes define the core logic performed by the pipeline. \n",
    "- **TaskBuilder**: Injected into the `StageBuilder`, the TaskBuilder dynamically constructs Task objects from the module and class name obtained from the stage configuration.\n",
    "- **Stage**: Stage objects encapsulate the set of inputs, outputs, and sequentially executed tasks.\n",
    "- **DataPrepStage**: A `Stage` subclass, the DataPrepStage object encapsulates the orchestration of all data preparation stages. \n",
    "\n",
    "\n",
    "We'll examine how these components interact to materialze workflows by constructing  the **Text Quality Analysis Pipeline**, starting with its configuration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Blueprint\n",
    "The blueprint for the `tqa` pipeline has three principal compoents:\n",
    "1. **`source_config`**: Specifies the stage input dataset configuration.\n",
    "2. **`destination_config`**: Defines the configuration for the output dataset.\n",
    "3. **`tasks`**: A list of tasks (defined above) and their arguments.\n",
    "\n",
    "Below we have the source and destination configurations for the `tqa` pipeline. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": [
     "hide-cell"
    ],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# %load -r 197-211 config/base/flow.yaml\n",
    "tqa:\n",
    "  source_config:\n",
    "    asset_type: dataset\n",
    "    phase: dataprep\n",
    "    stage: tqd\n",
    "    name: review\n",
    "    nlp: True\n",
    "    distributed: True\n",
    "  destination_config:\n",
    "    asset_type: dataset\n",
    "    phase: dataprep\n",
    "    stage: tqa\n",
    "    name: review\n",
    "    nlp: True\n",
    "    distributed: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source and destination configurations specify:\n",
    "- **asset_type**: Asset types can be datasets, files, or models.\n",
    "- **phase**: The phase in which the asset was created. Examples include `dataprep`, `eda`, and `modeling`.\n",
    "- **stage**: Thee stage, within the phase in which the asset was created, i.e. `ingest`, or `tqd`.\n",
    "- **name**: Name of the asset.\n",
    "- **nlp**: Whether the dataset will be part of an NLP pipeline. \n",
    "- **distributed**: Whether the dataset should be a pandas or a distributed (PySpark) DataFrame.\n",
    "\n",
    "Next, we'll define the pipeline tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Text Quality Analysis Pipeline** is comprised of eight defined above:\n",
    "1. **NLPTask**: Performs tokenization and part-of-speech (POS) tagging.\n",
    "2. **ComputeSyntacticStatsTask**: Computes the syntactic complexity statistics.\n",
    "3. **ComputeLexicalStatsTask**: Computes the lexical complexity statistics.\n",
    "4. **ComputeSyntacticLexicalScoresTask**: Combines the syntactic and lexical scores into a composite score.\n",
    "5. **ComputePerplexityFiltersTask**: Creates the binary indicators for 14 heuristic filters for perplexity-based coherence scoring.\n",
    "6. **ComputePerplexityWeights**: Computes the weights for each filter based upon perplexity scores.\n",
    "7. **ComputeCoherenceScoreTask**: Computes perplexity-based coherence scores\n",
    "8. **ComputeTextQualityScore**: Computes the final Text Quality Score based upon the Syntactic/Lexical Complexity and Coherence scores.\n",
    "\n",
    "Here, we have added the configurations for each task, including the class name, its module path, and a dictionary containing task specific parameters."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": [
     "hide-cell"
    ],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# %load -r 197-252 config/base/flow.yaml\n",
    "tqa:\n",
    "  source_config:\n",
    "    asset_type: dataset\n",
    "    phase: dataprep\n",
    "    stage: tqd\n",
    "    name: review\n",
    "    nlp: True\n",
    "    distributed: True\n",
    "  destination_config:\n",
    "    asset_type: dataset\n",
    "    phase: dataprep\n",
    "    stage: tqa\n",
    "    name: review\n",
    "    nlp: True\n",
    "    distributed: True\n",
    "  tasks:\n",
    "  - class_name: NLPTask\n",
    "    module: discover.flow.data_processing.data_prep.tqa.task\n",
    "    params:\n",
    "      column: content\n",
    "  - class_name: ComputeSyntacticStatsTask\n",
    "    module: discover.flow.data_processing.data_prep.tqa.task\n",
    "    params:\n",
    "      column: content\n",
    "  - class_name: ComputeLexicalStatsTask\n",
    "    module: discover.flow.data_processing.data_prep.tqa.task\n",
    "    params:\n",
    "      column: content\n",
    "  - class_name: ComputeSyntacticLexicalScoresTask\n",
    "    module: discover.flow.data_processing.data_prep.tqa.task\n",
    "    params:\n",
    "      column: content\n",
    "      new_column: tqa_syntactic_lexical_score\n",
    "      pos_diversity_weight: 0.3\n",
    "      pos_density_weight: 0.3\n",
    "      lexical_complexity_weight: 0.4\n",
    "  - class_name: ComputePerplexityFiltersTask\n",
    "    module: discover.flow.data_processing.data_prep.tqa.task\n",
    "    params: {}\n",
    "  - class_name: ComputePerplexityWeights\n",
    "    module: discover.flow.data_processing.data_prep.tqa.task\n",
    "    params:\n",
    "      pp_filepath: models/tqa/pp_weights.csv\n",
    "      column: an_perplexity\n",
    "      pp_filter_prefix: tqf_\n",
    "  - class_name: ComputeCoherenceScoreTask\n",
    "    module: discover.flow.data_processing.data_prep.tqa.task\n",
    "    params:\n",
    "      new_column: tqa_coherence_score\n",
    "      pp_filepath: models/tqa/pp_weights.csv\n",
    "  - class_name: ComputeTextQualityScore\n",
    "    module: discover.flow.data_processing.data_prep.tqa.task\n",
    "    params:\n",
    "      tqa_syntactic_weight: 0.4\n",
    "      tqa_perplexity_weight: 0.6\n",
    "      new_column: tqa_score"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-2.png](attachment:image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, the **Text Quality Analysis Pipeline** is fully specified. Next, we'll introduce the `Dataset` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Datasets inherit from the Asset class, which encapsulates core attributes such as the `asset_id`, the `phase` and `stage` in which the asset was created, its `name`, `content`, and date time it was `created`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -r 20-119 discover/assets/base.py\n",
    "\n",
    "from dataclasses import dataclass, fields\n",
    "from datetime import datetime\n",
    "from typing import Any, Optional\n",
    "\n",
    "from discover.assets.idgen import AssetIDGen\n",
    "from discover.core.data_class import DataClass\n",
    "from discover.core.flow import PhaseDef, StageDef\n",
    "from discover.infra.utils.date_time.format import ThirdDateFormatter\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "dt4mtr = ThirdDateFormatter()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "@dataclass\n",
    "class Asset(DataClass):\n",
    "    \"\"\"\n",
    "    A base class representing an asset in a data pipeline, such as models, datasets, or other entities.\n",
    "\n",
    "    This class encapsulates common attributes and behaviors associated with assets,\n",
    "    including tracking their phase, stage, name, content, and creation time.\n",
    "    It also provides methods for serialization and deserialization, along with\n",
    "    properties for generating unique IDs and descriptions.\n",
    "\n",
    "    Attributes:\n",
    "        asset_id (str): Uniquely identifies an asset.\n",
    "        phase (PhaseDef): The phase to which the asset belongs.\n",
    "        stage (StageDef): The stage within the phase.\n",
    "        name (str): The name of the asset.\n",
    "        content (Any): The actual content or object the asset represents.\n",
    "        created (Optional[datetime]): The timestamp when the asset was created.\n",
    "            If not provided, it is set to the current datetime during initialization.\n",
    "    \"\"\"\n",
    "\n",
    "    asset_id: str\n",
    "    phase: PhaseDef\n",
    "    stage: StageDef\n",
    "    name: str\n",
    "    content: Any\n",
    "    created: Optional[datetime] = None\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the `created` attribute with the current datetime if it was not provided.\n",
    "\n",
    "        This ensures that the asset's creation time is recorded upon instantiation,\n",
    "        providing a timestamp for when the asset was initialized.\n",
    "        \"\"\"\n",
    "        self.created = self.created or datetime.now()\n",
    "        self.asset_id = AssetIDGen.get_asset_id(\n",
    "            asset_type=self.__class__.__name__.lower(),\n",
    "            phase=self.phase,\n",
    "            stage=self.stage,\n",
    "            name=self.name,\n",
    "        )\n",
    "\n",
    "    def __getstate__(self):\n",
    "        \"\"\"\n",
    "        Prepares the object's state for serialization.\n",
    "\n",
    "        This method converts the object's attributes into a dictionary\n",
    "        that can be serialized, ensuring compatibility with serialization\n",
    "        libraries and allowing the asset's state to be stored or transmitted.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary representation of the object's state.\n",
    "        \"\"\"\n",
    "        return {field.name: getattr(self, field.name) for field in fields(self)}\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        \"\"\"\n",
    "        Restores the object's state from a serialized format.\n",
    "\n",
    "        This method takes a dictionary representation of the object's state,\n",
    "        applying it to restore the object's attributes, effectively reconstructing\n",
    "        the asset after it has been deserialized.\n",
    "\n",
    "        Args:\n",
    "            state (dict): The state dictionary used to restore the object.\n",
    "        \"\"\"\n",
    "        for key, value in state.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "    @property\n",
    "    def description(self) -> str:\n",
    "        \"\"\"\n",
    "        Provides a detailed description of the asset.\n",
    "\n",
    "        The description includes information such as the phase, stage, class name,\n",
    "        and the creation timestamp formatted in HTTP date format. It offers a\n",
    "        human-readable summary of the asset's key details.\n",
    "\n",
    "        Returns:\n",
    "            str: A detailed description of the asset including its phase, stage,\n",
    "                 class name, and creation time.\n",
    "        \"\"\"\n",
    "        return f\"{self.phase.description} - {self.stage.description} {self.__class__.__name__} created on {dt4mtr.to_HTTP_format(self.created)}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dataset` subclass contains additional attributes, relevant to datasets, such as size, shape and location of the dataset in storage. The `nlp`, and `distributed` attributes determine whether the underlying data are to be materialized as pandas or Spark DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -r 20-119 discover/assets/dataset.py\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from discover.assets.base import Asset\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "@dataclass\n",
    "class Dataset(Asset):\n",
    "    \"\"\"\n",
    "    Represents a dataset, encapsulating content along with metadata like\n",
    "    the number of rows, columns, and size.\n",
    "\n",
    "    Inherits from:\n",
    "        Asset: Base class that provides phase, stage, content, and storage\n",
    "        configuration information.\n",
    "\n",
    "    Attributes:\n",
    "        nlp (bool): Whether the dataset is part of an NLP pipeline\n",
    "        distributed (bool): Whether the dataset contents are persisted in a distributed dataframe structure.\n",
    "        storage_location (Any): Object specifying the storage location for the dataset content payload.\n",
    "        nrows (int): The number of rows in the dataset. Defaults to 0.\n",
    "        ncols (int): The number of columns in the dataset. Defaults to 0.\n",
    "        size (float): The size of the dataset in bytes. Defaults to 0.\n",
    "\n",
    "    Methods:\n",
    "        __post_init__() -> None:\n",
    "            Initializes the `nrows`, `ncols`, and `size` attributes based on\n",
    "            the type of content provided. If the content is a Spark DataFrame,\n",
    "            it calculates the number of rows and columns using Spark methods.\n",
    "            Otherwise, it uses pandas methods for DataFrames.\n",
    "\n",
    "        __eq__(other: object) -> bool:\n",
    "            Checks for equality between two `Dataset` instances. Two datasets\n",
    "            are considered equal if they have the same number of rows, columns,\n",
    "            and size.\n",
    "    \"\"\"\n",
    "\n",
    "    nlp: bool = False\n",
    "    distributed: bool = False\n",
    "    storage_location: Optional[Any] = None\n",
    "    nrows: int = 0\n",
    "    ncols: int = 0\n",
    "    size: float = 0\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the dataset's `nrows`, `ncols`, and `size` attributes.\n",
    "\n",
    "        If the content is a Spark DataFrame, it computes the number of rows\n",
    "        using `count()` and the number of columns from the DataFrame's\n",
    "        `columns` attribute. The size is calculated by mapping each row to its\n",
    "        string representation and summing their lengths.\n",
    "\n",
    "        If the content is a pandas DataFrame, it computes the number of rows\n",
    "        using `shape[0]`, the number of columns using `shape[1]`, and the size\n",
    "        using the `memory_usage(deep=True).sum()` method.\n",
    "\n",
    "        Raises:\n",
    "            AttributeError: If the `content` does not have the expected attributes\n",
    "            for row count, column count, or size calculation.\n",
    "        \"\"\"\n",
    "        super().__post_init__()\n",
    "        if isinstance(self.content, (pd.DataFrame, pd.core.frame.DataFrame)):\n",
    "            self.nrows = self.content.shape[0]\n",
    "            self.ncols = self.content.shape[1]\n",
    "            self.size = self.content.memory_usage(deep=True).sum()\n",
    "        else:\n",
    "            self.nrows = self.content.count()\n",
    "            self.ncols = len(self.content.columns)\n",
    "            self.size = self.content.count()\n",
    "\n",
    "    def __eq__(self, other: object) -> bool:\n",
    "        \"\"\"\n",
    "        Checks for equality between two `Dataset` instances.\n",
    "\n",
    "        Two datasets are considered equal if they have the same number of rows,\n",
    "        columns, and size. This method overrides the equality operator to facilitate\n",
    "        direct comparison between `Dataset` objects.\n",
    "\n",
    "        Args:\n",
    "            other (object): The object to compare against.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the datasets have the same number of rows, columns, and size;\n",
    "                  otherwise, False.\n",
    "        \"\"\"\n",
    "        if not isinstance(other, Dataset):\n",
    "            return NotImplemented\n",
    "        return (\n",
    "            self.nrows == other.nrows\n",
    "            and self.ncols == other.ncols\n",
    "            and self.size == other.size\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Repository\n",
    "The **DatasetRepo** class is responsible for Dataset persistence, ensuring they are stored, retrieved, and persisted efficiently and reliably. It integrates with both centralized (Pandas-based) and distributed (PySpark-based) storage systems, and its core methods include:\n",
    "\n",
    "   - `add()`: Adds a new dataset, writing its content to the appropriate storage and saving its metadata. It includes rollback mechanisms to maintain data consistency if an error occurs.\n",
    "   - `get()`: Retrieves a dataset by ID, including its content and metadata, with error handling for cases where the dataset cannot be found or read.\n",
    "   - `list_all()`, `list_by_phase()`, `list_by_stage()`: Methods to list datasets and filter them based on metadata attributes.\n",
    "   - `remove()`: Removes a dataset and its associated file(s), with options to suppress errors and perform cleanup.\n",
    "   - `exists()`: Checks for the existence of a dataset by its ID.\n",
    "\n",
    "To facilitate efficient IO of large datasets, the underlying data are stored in Parquet files, partitioned by review category for read/write efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -r 19-527 discover/infra/persistence/repo/dataset.py\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "\n",
    "from discover.assets.dataset import Dataset\n",
    "from discover.assets.repo import Repo\n",
    "from discover.core.flow import PhaseDef\n",
    "from discover.infra.persistence.dal.dao.dataset import DatasetDAO\n",
    "from discover.infra.persistence.dal.dao.exception import ObjectNotFoundError\n",
    "from discover.infra.persistence.dal.fao.centralized import (\n",
    "    CentralizedFileSystemFAO as FAOCFS,\n",
    ")\n",
    "from discover.infra.persistence.dal.fao.distributed import (\n",
    "    DistributedFileSystemFAO as FAODFS,\n",
    ")\n",
    "from discover.infra.persistence.dal.fao.location import FileLocationService\n",
    "from discover.infra.persistence.repo.exception import (\n",
    "    DatasetCreationError,\n",
    "    DatasetExistsError,\n",
    "    DatasetIntegrityError,\n",
    "    DatasetIOError,\n",
    "    DatasetNotFoundError,\n",
    "    DatasetRemovalError,\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "class DatasetRepo(Repo):\n",
    "    \"\"\"\n",
    "    Repository class for managing dataset storage and retrieval operations.\n",
    "\n",
    "    This class provides methods to add, retrieve, list, and remove datasets, as well as\n",
    "    read and write datasets to centralized (Pandas) or distributed (PySpark) file systems.\n",
    "    It leverages DAOs (Data Access Objects) for interacting with underlying storage mechanisms.\n",
    "\n",
    "    Args:\n",
    "        dataset_dao (DatasetDAO): Data Access Object for dataset metadata.\n",
    "        fao_cfs (FAOCFS): Data Access Object for centralized file storage (e.g., local filesystem).\n",
    "        fao_dfs (FAODFS): Data Access Object for distributed file storage (e.g., HDFS).\n",
    "        location_service (LocationService): Service that centralizes the assignment of filepaths.\n",
    "        partitioned (bool): Whether dataset files are to be partitioned parquet files. Default is True\n",
    "\n",
    "\n",
    "    Methods:\n",
    "        add(dataset: Dataset) -> None:\n",
    "            Adds a new dataset to the storage system.\n",
    "\n",
    "        get(id: int) -> Optional[Dataset]:\n",
    "            Retrieves a dataset by its ID.\n",
    "\n",
    "        list_all() -> pd.DataFrame:\n",
    "            Returns a DataFrame containing all datasets' metadata.\n",
    "\n",
    "        list_by_phase(phase: PhaseDef) -> pd.DataFrame:\n",
    "            Lists datasets filtered by the specified phase.\n",
    "\n",
    "        list_by_stage(stage: StageDef) -> pd.DataFrame:\n",
    "            Lists datasets filtered by the specified stage.\n",
    "\n",
    "        remove(id: int) -> None:\n",
    "            Removes a dataset and its associated content by its ID.\n",
    "\n",
    "        exists(id: int) -> bool:\n",
    "            Checks if a dataset exists by its ID.\n",
    "\n",
    "    Private Methods:\n",
    "        _read_file(dataset: Dataset) -> Union[pd.DataFrame, pyspark.sql.DataFrame]:\n",
    "            Reads a dataset's content\n",
    "\n",
    "        _read_centralized_file(dataset: Dataset) -> pd.DataFrame:\n",
    "            Reads a dataset's content from a centralized file system (e.g., local filesystem).\n",
    "\n",
    "        _read_distributed_file(dataset: Dataset) -> pyspark.sql.DataFrame:\n",
    "            Reads a dataset's content from a distributed file system (e.g., HDFS).\n",
    "\n",
    "        _write_file(dataset: Dataset) -> None:\n",
    "            Writes a dataset's content.\n",
    "\n",
    "        _write_centralized_file(dataset: Dataset) -> None:\n",
    "            Writes a dataset's content to a centralized file system.\n",
    "\n",
    "        _write_distributed_file(dataset: Dataset) -> None:\n",
    "            Writes a dataset's content to a distributed file system.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_dao: DatasetDAO,\n",
    "        fao_cfs: FAOCFS,\n",
    "        fao_dfs: FAODFS,\n",
    "        location_service: FileLocationService,\n",
    "        partitioned: bool = True,\n",
    "    ) -> None:\n",
    "\n",
    "        self._dataset_dao = dataset_dao\n",
    "        self._fao_cfs = fao_cfs\n",
    "        self._fao_dfs = fao_dfs\n",
    "        self._location_service = location_service\n",
    "        self._partitioned = partitioned\n",
    "        self._logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------- #\n",
    "    #                             DATASET CREATION METHODS                                         #\n",
    "    # -------------------------------------------------------------------------------------------- #\n",
    "    def add(self, dataset: Dataset) -> Optional[Dataset]:\n",
    "        \"\"\"\n",
    "        Adds a new dataset to the repository. Raises an error if the dataset ID already exists.\n",
    "\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset object to be added.\n",
    "\n",
    "        Returns:\n",
    "            dataset (Dataset): The dataset object with storage location added.\n",
    "        Raises:\n",
    "            FileExistsError: If a dataset with the given ID already exists.\n",
    "        \"\"\"\n",
    "        # Ensure dataset doesn't already exist\n",
    "        self._validate_add(dataset=dataset)\n",
    "\n",
    "        # Set the storage location on the dataset object\n",
    "        dataset.storage_location = self._location_service.get_filepath(\n",
    "            asset_type=dataset.__class__.__name__.lower(),\n",
    "            phase=dataset.phase,\n",
    "            stage=dataset.stage,\n",
    "            name=dataset.name,\n",
    "            partition=self._partitioned,\n",
    "        )\n",
    "\n",
    "        # Save dataset contents to file.\n",
    "        try:\n",
    "            self._write_file(dataset=dataset)\n",
    "        except Exception as e:\n",
    "            msg = f\"Exception occurred adding dataset {dataset.asset_id} to the repository.\"\n",
    "            self._logger.exception(msg)\n",
    "            raise DatasetIOError(msg, e) from e\n",
    "\n",
    "        # Save dataset object (metadata and config) to object storage\n",
    "        try:\n",
    "            self._dataset_dao.create(dataset=dataset)\n",
    "        except Exception as e:\n",
    "            msg = f\"Exception occurred while saving dataset {dataset.asset_id} to object storage. Rolling back file persistence.\"\n",
    "\n",
    "            # Rollback the file write to maintain consistency.\n",
    "            try:\n",
    "                self._remove_dataset_file_by_filepath(filepath=dataset.storage_location)\n",
    "            except Exception as e:\n",
    "                fileio_msg = f\"Exception occurred while rolling back dataset {dataset.asset_id} persistence.\"\n",
    "                self._logger.exception(fileio_msg)\n",
    "                raise DatasetIOError(fileio_msg, e) from e\n",
    "\n",
    "            # Raise the original exception\n",
    "            raise DatasetCreationError(msg, e) from e\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------- #\n",
    "    #                             DATASET RETRIEVAL METHODS                                        #\n",
    "    # -------------------------------------------------------------------------------------------- #\n",
    "    def get(\n",
    "        self,\n",
    "        asset_id: str,\n",
    "        distributed: Optional[bool] = None,\n",
    "        nlp: bool = False,\n",
    "    ) -> Optional[Dataset]:\n",
    "        \"\"\"\n",
    "        Retrieves a dataset by its ID.\n",
    "\n",
    "        Args:\n",
    "            asset_id (str): The id of the dataset to retrieve.\n",
    "            distributed (bool): If True, a distributed (Spark) DataFrame is returned.\n",
    "                If None, distributed value is obtained from dataset metadata\n",
    "            nlp (bool): If True, a Spark session for NLP will be used.\n",
    "\n",
    "        Returns:\n",
    "            Optional[Dataset]: The dataset object if found; otherwise, None.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If an error occurs while reading the dataset.\n",
    "        \"\"\"\n",
    "        # Step 1: Obtain the dataset object containing metadata and config.\n",
    "        try:\n",
    "            dataset = self._dataset_dao.read(asset_id=asset_id)\n",
    "        except ObjectNotFoundError:\n",
    "            msg = f\"Dataset {asset_id} does not exist.\"\n",
    "            self._logger.exception(msg)\n",
    "            raise DatasetNotFoundError(msg)\n",
    "        except Exception as e:\n",
    "            msg = f\"Exception occurred while reading the dataset {asset_id} object.\"\n",
    "            self._logger.exception(msg)\n",
    "            raise DatasetIOError(msg, e) from e\n",
    "\n",
    "        # Step 2: Get the dataset contents from file, add to dataset object and return\n",
    "        try:\n",
    "            dataset.content = self._read_file(\n",
    "                dataset=dataset, distributed=distributed, nlp=nlp\n",
    "            )\n",
    "            return dataset\n",
    "        except FileNotFoundError as e:\n",
    "            msg = f\"Exception occurred while reading dataset {dataset.asset_id}. File containing dataset contents was not found at {dataset.storage_location}.\\n{e}\"\n",
    "            self._logger.exception(msg)\n",
    "            raise DatasetIntegrityError(msg)\n",
    "        except Exception as e:\n",
    "            if \"[PATH_NOT_FOUND]\" in str(e):\n",
    "                msg = f\"Exception occurred while reading dataset {dataset.asset_id}. File containing dataset contents was not found at {dataset.storage_location}.\\n{e}\"\n",
    "                self._logger.exception(msg)\n",
    "                raise DatasetIntegrityError(msg)\n",
    "            else:\n",
    "                msg = f\"Exception occurred while reading dataset {dataset.asset_id} contents from file.\"\n",
    "                self._logger.exception(msg)\n",
    "                raise DatasetIOError(msg, e) from e\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------- #\n",
    "    def select(self, asset_id: str, condition: Callable) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Selects and returns rows from the asset data that meet a specified condition.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        asset_id : str\n",
    "            The unique identifier for the asset from which to retrieve data.\n",
    "\n",
    "        condition : Callable\n",
    "            A lambda function or callable expression that filters the DataFrame rows.\n",
    "            This function should accept a DataFrame and return a boolean Series to filter rows\n",
    "            (e.g., `lambda x: x['date'] > some_date`).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            A DataFrame containing only the rows that satisfy the specified condition.\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> select(\"asset_123\", lambda x: x['date'] > '2023-01-01')\n",
    "        \"\"\"\n",
    "        df = self.get(asset_id=asset_id)\n",
    "        return df[condition]\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------- #\n",
    "    def get_dataset_metadata(self, asset_id: str) -> Optional[Dataset]:\n",
    "        \"\"\"\n",
    "        Retrieves a dataset with metadata only.\n",
    "\n",
    "        Args:\n",
    "            asset_id (str): The id of the dataset to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            Optional[Dataset]: The dataset object containing just metadata if found; otherwise, None.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If an error occurs while reading the dataset.\n",
    "        \"\"\"\n",
    "        # Step 1: Obtain the dataset object containing metadata and config.\n",
    "        try:\n",
    "            return self._dataset_dao.read(asset_id=asset_id)\n",
    "        except ObjectNotFoundError:\n",
    "            msg = f\"Dataset {asset_id} does not exist.\"\n",
    "            self._logger.exception(msg)\n",
    "            raise DatasetNotFoundError(msg)\n",
    "        except Exception as e:\n",
    "            msg = f\"Exception occurred while reading the dataset {asset_id} object.\"\n",
    "            self._logger.exception(msg)\n",
    "            raise DatasetIOError(msg, e) from e\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------- #\n",
    "    def update_dataset_metadata(self, dataset: Dataset) -> None:\n",
    "        \"\"\"\n",
    "        Updates the metadata for a dataset.\n",
    "\n",
    "        Args:\n",
    "            asset_id (str): The id of the dataset to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            Optional[Dataset]: The dataset object containing just metadata if found; otherwise, None.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If an error occurs while reading the dataset.\n",
    "        \"\"\"\n",
    "        # Step 1: Obtain the dataset object containing metadata and config.\n",
    "        try:\n",
    "            return self._dataset_dao.update(dataset=dataset)\n",
    "        except Exception as e:\n",
    "            msg = f\"Metadata for dataset {dataset.asset_id} could not be updated.\\n{e}\"\n",
    "            self._logger.exception(msg)\n",
    "            raise DatasetIOError(msg, e) from e\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------- #\n",
    "    def is_consumed(self, asset_id: str) -> bool:\n",
    "        \"\"\"Returns the True if the dataset has been marked as consumed, False otherwise.\n",
    "\n",
    "        Args:\n",
    "            asset_id (str): The dataset asset identifier.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the Dataset has been created, but not yet consumed.\n",
    "        \"\"\"\n",
    "        dataset = self.get_dataset_metadata(asset_id=asset_id)\n",
    "        return dataset.consumed\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------- #\n",
    "    def consumed(self, asset_id: str, consumer: str = None) -> None:\n",
    "        \"\"\"Marks the dataset as having been consumed.\n",
    "\n",
    "        Args:\n",
    "            asset_id (str): The identifier for the dataset asset.\n",
    "            consumer (str): The name of the Task that consumed the dataset.\n",
    "        \"\"\"\n",
    "        dataset = self.get_dataset_metadata(asset_id=asset_id)\n",
    "        dataset.consumed = True\n",
    "        dataset.dt_consumed = datetime.now()\n",
    "        dataset.consumed_by = consumer\n",
    "        self.update_dataset_metadata(dataset=dataset)\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------- #\n",
    "    def _read_file(\n",
    "        self, dataset: Dataset, distributed: Optional[bool] = None, nlp: bool = False\n",
    "    ) -> Union[pd.DataFrame, pyspark.sql.DataFrame]:\n",
    "        \"\"\"\n",
    "        Reads a dataset's content based on its storage configuration.\n",
    "\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset to read.\n",
    "            distributed (Optional[bool]): Indicates whether the dataset should be read\n",
    "                as a distributed dataset (Spark) or centralized (Pandas)\n",
    "            nlp (bool): Whether to obtain data using a NLP spark session.\n",
    "\n",
    "        Returns:\n",
    "            Union[pd.DataFrame, pyspark.sql.DataFrame]: The dataset's content.\n",
    "        \"\"\"\n",
    "        if distributed is None:\n",
    "            distributed = dataset.distributed\n",
    "\n",
    "        if distributed:\n",
    "            return self._read_distributed_file(dataset=dataset, nlp=nlp)\n",
    "        else:\n",
    "            return self._read_centralized_file(dataset=dataset)\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------- #\n",
    "    def _read_centralized_file(self, dataset: Dataset) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Reads a dataset's content from a centralized file system.\n",
    "\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset to read.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The content of the dataset.\n",
    "        \"\"\"\n",
    "        return self._fao_cfs.read(filepath=dataset.storage_location)\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------- #\n",
    "    def _read_distributed_file(\n",
    "        self, dataset: Dataset, nlp: bool = False\n",
    "    ) -> pyspark.sql.DataFrame:\n",
    "        \"\"\"\n",
    "        Reads a dataset's content from a distributed file system.\n",
    "\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset to read.\n",
    "\n",
    "        Returns:\n",
    "            pyspark.sql.DataFrame: The content of the dataset.\n",
    "        \"\"\"\n",
    "        nlp = dataset.nlp or nlp\n",
    "        return self._fao_dfs.read(filepath=dataset.storage_location, nlp=nlp)\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------- #\n",
    "    #                               DATASET LIST METHODS                                           #\n",
    "    # -------------------------------------------------------------------------------------------- #\n",
    "    def list_all(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Lists all datasets' metadata.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing metadata for all datasets.\n",
    "        \"\"\"\n",
    "        return self._dataset_dao.read_all()\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------- #\n",
    "    def list_by_phase(self, phase: PhaseDef) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Lists datasets filtered by the specified phase.\n",
    "\n",
    "        Args:\n",
    "            phase (PhaseDef): The phase to filter datasets by.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing metadata for the filtered datasets.\n",
    "        \"\"\"\n",
    "        return self._dataset_dao.read_by_phase(phase=phase)\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------- #\n",
    "    #                             DATASET REMOVAL METHODS                                          #\n",
    "    # -------------------------------------------------------------------------------------------- #\n",
    "    def remove(self, asset_id: str, ignore_errors: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Removes a dataset and its associated file(s) from the repository.\n",
    "\n",
    "        This method attempts to remove both the dataset object and its related file(s).\n",
    "        If the dataset object does not exist, and `ignore_errors` is set to True, the method will still\n",
    "        search for and remove any related dataset files. If `ignore_errors` is False, it will raise an exception\n",
    "        if the dataset object or file cannot be found or deleted.\n",
    "\n",
    "        Args:\n",
    "            asset_id (str): The id of the dataset to be removed.\n",
    "            ignore_errors (bool): If True, suppresses any exceptions during the removal process\n",
    "                and logs warnings instead of raising errors. Default is False.\n",
    "\n",
    "        Raises:\n",
    "            DatasetRemovalError: If any error occurs while removing the dataset object or file, and `ignore_errors` is False.\n",
    "        \"\"\"\n",
    "        # Obtain the dataset object and storage information from the repository\n",
    "        if self._dataset_dao.exists(asset_id=asset_id):\n",
    "            dataset = self._dataset_dao.read(asset_id=asset_id)\n",
    "            # Delete the dataset file from the repository\n",
    "            self._remove_dataset_file_by_filepath(filepath=dataset.storage_location)\n",
    "            # Delete the dataset object.\n",
    "            self._dataset_dao.delete(asset_id=asset_id)\n",
    "            msg = f\"Removed dataset {asset_id} from the repository.\"\n",
    "            self._logger.info(msg)\n",
    "        # If ignoring errors, issue a warning and search for file remnants by name.\n",
    "        elif ignore_errors:\n",
    "            msg = f\"Warning: Dataset {asset_id} does not exist.\"\n",
    "            self._logger.warning(msg)\n",
    "        # Otherwise throw a DatasetRemovalError\n",
    "        else:\n",
    "            msg = f\"Exception: Dataset {asset_id} does not exist\"\n",
    "            self._logger.exception(msg)\n",
    "            raise DatasetRemovalError(msg)\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------- #\n",
    "    def _remove_dataset_file_by_filepath(self, filepath: str) -> None:\n",
    "        \"\"\"\n",
    "        Removes the dataset file located at the specified filepath if it exists.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): The path of the dataset file to be removed.\n",
    "        \"\"\"\n",
    "        if self._fao_cfs.exists(filepath=filepath):\n",
    "            self._fao_cfs.delete(filepath)\n",
    "            msg = f\"Removed dataset file at {filepath} from repository.\"\n",
    "            self._logger.info(msg)\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------- #\n",
    "    #                             DATASET EXISTENCE METHOD                                         #\n",
    "    # -------------------------------------------------------------------------------------------- #\n",
    "    def exists(self, asset_id: str) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if a dataset exists by its ID.\n",
    "\n",
    "        Args:\n",
    "            asset_id (str): The id to check for existence.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the dataset exists, False otherwise.\n",
    "        \"\"\"\n",
    "        return self._dataset_dao.exists(asset_id=asset_id)\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------- #\n",
    "    def _validate_add(self, dataset: Dataset) -> None:\n",
    "        \"\"\"Ensures dataset object and file doesn't already exist\"\"\"\n",
    "        if self.exists(asset_id=dataset.asset_id):\n",
    "            msg = f\"Unable to add dataset {dataset.asset_id} as it already exists.\"\n",
    "            self._logger.error(msg)\n",
    "            raise DatasetExistsError(msg)\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------- #\n",
    "    #                             DATASET WRITE METHODS                                            #\n",
    "    # -------------------------------------------------------------------------------------------- #\n",
    "    def _write_file(self, dataset: Dataset) -> None:\n",
    "        \"\"\"\n",
    "        Writes a dataset's content.\n",
    "\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset to write.\n",
    "        \"\"\"\n",
    "        if dataset.distributed:\n",
    "            self._write_distributed_file(dataset=dataset)\n",
    "        else:\n",
    "            self._write_centralized_file(dataset=dataset)\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------- #\n",
    "    def _write_centralized_file(self, dataset: Dataset) -> None:\n",
    "        \"\"\"\n",
    "        Writes a dataset's content to a centralized file system.\n",
    "\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset to write.\n",
    "        \"\"\"\n",
    "        self._fao_cfs._write(\n",
    "            filepath=dataset.storage_location,\n",
    "            data=dataset.content,\n",
    "        )\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------- #\n",
    "    def _write_distributed_file(self, dataset: Dataset) -> None:\n",
    "        \"\"\"\n",
    "        Writes a dataset's content to a distributed file system.\n",
    "\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset to write.\n",
    "        \"\"\"\n",
    "        self._fao_dfs._write(\n",
    "            filepath=dataset.storage_location,\n",
    "            data=dataset.content,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "The **Task** class serves as an abstract base class for the tasks such as the **Text Quality Analysis Pipeline** tasks defined earlier. It ensures that every task has a `name` property (accessed by the task_logger decorator) and a `run` method that executes the Task's core logic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -r 20-84 discover/flow/base/task.py\n",
    "\n",
    "import importlib\n",
    "import logging\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "#                                           TASK                                                   #\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "class Task(ABC):\n",
    "    \"\"\"\n",
    "    An abstract base class for defining tasks in a pipeline.\n",
    "    All tasks must implement the `run` method and provide a\n",
    "    `name` property based on the class name.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    name -> str\n",
    "        Returns the name of the task, which is the class name of the task instance.\n",
    "\n",
    "    run(*args, data: Any, **kwargs) -> Any\n",
    "        Abstract method that must be implemented by any subclass. It represents\n",
    "        the main logic of the task. Subclasses should specify the expected inputs\n",
    "        and outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns the name of the task, which is the class name of the task instance.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            The name of the task.\n",
    "        \"\"\"\n",
    "        return self.__class__.__name__\n",
    "\n",
    "    @abstractmethod\n",
    "    def run(self, *args, data: Any, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        The core logic of the task. Must be implemented by any subclass.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        *args : tuple\n",
    "            Positional arguments that the task may require.\n",
    "        data : Any\n",
    "            The input data for the task. The specific type of data will depend\n",
    "            on the implementation of the subclass.\n",
    "        **kwargs : dict\n",
    "            Additional keyword arguments that the task may require.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        Any\n",
    "            The output of the task, as defined by the subclass implementation.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Builder \n",
    "The **TaskBuilder** class is a utility for dynamically constructing task instances from configuration data. It uses a static method, `build`, to read the provided configuration dictionary and create an instance of the specified task class based on the `task_config` dictionary which includes keys such as `module_name` (where the task class is defined), `class_name` (the name of the task class), and `params` (arguments to pass to the class constructor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -r 129-200 discover/flow/base/task.py\n",
    "class TaskBuilder:\n",
    "    \"\"\"\n",
    "    A builder class for constructing task instances from configuration data.\n",
    "\n",
    "    The `TaskBuilder` class provides a static method, `build`, which reads task configuration\n",
    "    data and dynamically creates an instance of the specified task class using the provided\n",
    "    parameters. This allows for flexible and dynamic task instantiation based on configuration.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    build(task_config: dict) -> object\n",
    "        Constructs and returns an instance of a task class using the specified configuration.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> task_config = {\n",
    "    ...     'module_name': 'mypackage.mymodule',\n",
    "    ...     'class_name': 'NormalizationTask',\n",
    "    ...     'params': {'param1': value1, 'param2': value2}\n",
    "    ... }\n",
    "    >>> task_instance = TaskBuilder.build(task_config)\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def build(task_config):\n",
    "        \"\"\"\n",
    "        Builds and returns an instance of a task class based on the provided configuration.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        task_config : dict\n",
    "            A dictionary containing task configuration with the following keys:\n",
    "                - 'phase' (str): The phase associated with the task (e.g., 'preprocessing').\n",
    "                - 'stage' (str): The stage within the phase (e.g., 'normalization').\n",
    "                - 'module_name' (str): The name of the module containing the task class.\n",
    "                - 'class_name' (str): The name of the task class to instantiate.\n",
    "                - 'params' (dict): Additional parameters to pass to the task's constructor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        object\n",
    "            An instance of the specified task class initialized with the provided parameters.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ModuleNotFoundError\n",
    "            If the specified module cannot be found.\n",
    "        AttributeError\n",
    "            If the specified class does not exist in the module.\n",
    "        TypeError\n",
    "            If the class constructor does not accept the provided parameters.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> task_config = {\n",
    "        ...     'module_name': 'mypackage.mymodule',\n",
    "        ...     'class_name': 'NormalizationTask',\n",
    "        ...     'params': {'param1': value1, 'param2': value2}\n",
    "        ... }\n",
    "        >>> task_instance = TaskBuilder.build(task_config)\n",
    "        \"\"\"\n",
    "        module = task_config[\"module\"]\n",
    "        class_name = task_config[\"class_name\"]\n",
    "        params = task_config[\"params\"]\n",
    "        return instantiate_class(\n",
    "            module=module,\n",
    "            class_name=class_name,\n",
    "            params=params,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **TaskBuilder** delegates class instantiation to the `instantiate_class` function, which dynamically imports a module and creates an instance of a specified class using the provided parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -r 86-125 discover/flow/base/task.py\n",
    "def instantiate_class(module: str, class_name: str, params: dict):\n",
    "    \"\"\"\n",
    "    Dynamically imports a module and instantiates a class with the given parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    module : str\n",
    "        The name of the module from which to import the class (e.g., 'mypackage.mymodule').\n",
    "    class_name : str\n",
    "        The name of the class to instantiate from the module.\n",
    "    params : dict\n",
    "        A dictionary of additional parameters to pass to the class constructor.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    object\n",
    "        An instance of the specified class with the provided parameters.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ModuleNotFoundError\n",
    "        If the specified module cannot be found.\n",
    "    AttributeError\n",
    "        If the specified class does not exist in the module.\n",
    "    TypeError\n",
    "        If the class constructor does not accept the provided parameters.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> obj = instantiate_class(\n",
    "    ...     module='mypackage.mymodule',\n",
    "    ...     class_name='Normalizer',\n",
    "    ...     params={'param1': value1, 'param2': value2}\n",
    "    ... )\n",
    "    \"\"\"\n",
    "    module = importlib.import_module(module)\n",
    "    cls = getattr(module, class_name)\n",
    "    return cls(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage\n",
    "Task orchestration occurs in Stage classes, which define a logic unit of work comprised of a series of Task objects. The **Stage** class defines the interface for pipeline Stage subclasses and through its `build` class method, controls the construction and instantiation of Stage objects, leveraging the TaskBuilder to instantiate the Task instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -r 19-159 discover/flow/base/stage.py\n",
    "from __future__ import annotations\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Type\n",
    "\n",
    "from discover.core.flow import PhaseDef, StageDef\n",
    "from discover.core.namespace import NestedNamespace\n",
    "from discover.flow.base.task import Task, TaskBuilder\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "#                                        STAGE                                                     #\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "class Stage(ABC):\n",
    "    \"\"\"Abstract base class for pipeline stages.\n",
    "\n",
    "    This class provides the foundation for creating and executing stages in a data pipeline.\n",
    "    A stage typically involves loading source data, applying a series of tasks to the data,\n",
    "    and saving the processed data to a destination. The stage is configurable with source\n",
    "    and destination settings and a list of tasks to be executed.\n",
    "\n",
    "    Attributes:\n",
    "        _source_config (NestedNamespace): Configuration dictionary for the source dataset.\n",
    "        _destination_config (NestedNamespace): Configuration dictionary for the destination dataset.\n",
    "        _task_builder (Type[TaskBuilder]): The builder class responsible for constructing tasks.\n",
    "        _tasks (List[Task]): A list of tasks to be executed in this stage.\n",
    "        _force (bool): Whether to force execution even if the destination dataset exists.\n",
    "\n",
    "    Methods:\n",
    "        phase (PhaseDef): The phase of the pipeline.\n",
    "        stage (StageDef): The specific stage of the pipeline.\n",
    "        run() -> str: Executes the pipeline stage.\n",
    "        build(stage_config: dict, force: bool = False) -> Stage: Creates and returns a new stage instance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_config: dict,\n",
    "        destination_config: dict,\n",
    "        tasks: List[Task],\n",
    "        task_builder: Type[TaskBuilder] = TaskBuilder,\n",
    "        force: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"Initializes the Stage pipeline with the provided configuration and tasks.\n",
    "\n",
    "        Args:\n",
    "            source_config (dict): Configuration for the source dataset.\n",
    "            destination_config (dict): Configuration for the destination dataset.\n",
    "            tasks (List[Task]): List of tasks to be applied in the pipeline stage.\n",
    "            task_builder (Type[TaskBuilder], optional): The task builder class to use for constructing tasks.\n",
    "                Defaults to `TaskBuilder`.\n",
    "            force (bool, optional): Whether to force execution even if the destination dataset exists.\n",
    "                Defaults to `False`.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If source_config or destination_config are invalid or incomplete.\n",
    "            RuntimeError: For other initialization errors.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._source_config = NestedNamespace(source_config)\n",
    "            self._destination_config = NestedNamespace(destination_config)\n",
    "            self._task_builder = task_builder\n",
    "            self._tasks = tasks\n",
    "            self._force = force\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Missing required configuration key: {str(e)}\") from e\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Failed to initialize the Stage with the provided configuration: {str(e)}\"\n",
    "            ) from e\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def phase(self) -> PhaseDef:\n",
    "        \"\"\"Returns the phase of the pipeline.\n",
    "\n",
    "        This property is meant to categorize the stage within the larger pipeline process.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def stage(self) -> StageDef:\n",
    "        \"\"\"Returns the specific stage within the pipeline.\n",
    "\n",
    "        This property helps identify which stage of the pipeline is being executed.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def run(self) -> str:\n",
    "        \"\"\"Executes the stage and returns the resulting asset ID.\n",
    "\n",
    "        This method orchestrates the execution of the tasks defined in the stage,\n",
    "        processes the source data, and saves the result to the destination.\n",
    "\n",
    "        Returns:\n",
    "            str: The asset ID of the generated dataset.\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If an error occurs during the execution of the stage.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def build(cls, stage_config: dict, force: bool = False) -> Stage:\n",
    "        \"\"\"Creates and returns a new stage instance from the provided configuration.\n",
    "\n",
    "        Args:\n",
    "            stage_config (dict): The configuration dictionary containing source, destination, and tasks details.\n",
    "            force (bool, optional): Whether to force execution even if the destination dataset exists. Defaults to `False`.\n",
    "\n",
    "        Returns:\n",
    "            Stage: A new instance of the Stage class, configured with the provided settings.\n",
    "\n",
    "        Raises:\n",
    "            KeyError: If the required keys are missing from the stage_config.\n",
    "            ValueError: If tasks cannot be built from the configuration.\n",
    "            RuntimeError: If there is an error creating the stage.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tasks = [\n",
    "                cls._task_builder.build(task_config)\n",
    "                for task_config in stage_config[\"tasks\"]\n",
    "            ]\n",
    "            return cls(\n",
    "                source_config=stage_config[\"source_config\"],\n",
    "                destination_config=stage_config[\"destination_config\"],\n",
    "                tasks=tasks,\n",
    "                force=force,\n",
    "            )\n",
    "        except KeyError as e:\n",
    "            raise ValueError(\n",
    "                f\"Missing required configuration key in stage_config: {str(e)}\"\n",
    "            ) from e\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Failed to build stage from configuration: {str(e)}\"\n",
    "            ) from e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation Stage\n",
    "Finally, we have the **DataPrepStage** class, the orchestrator of data preparation activities from loading source data to executing a series of tasks and saving the processed data. The `run` method is where the action occurs.  It loads the source data, applies each task in order, creates a destination dataset, and saves it. Supporting methods include: \n",
    "  - **_load_source_data()**: Loads the source dataset from the repository, ensuring it’s properly configured and renamed if necessary.\n",
    "  - **_create_destination_dataset()**: Constructs a `Dataset` object with the processed data and configuration details.\n",
    "  - **_save_destination_dataset()**: Saves the processed dataset to the repository, raising errors if the operation fails.\n",
    "  - **_endpoint_exists()**: Checks if the dataset endpoint already exists in the repository.\n",
    "  - **_remove_destination_dataset()**: Removes an existing destination dataset from the repository if necessary.\n",
    "\n",
    "The **DataPrepStage** subclasses implement any stage specific variations, such as the `IngestStage` which takes its input from a raw data file, rather than the repository. Yet, the core logic implemented in the **DataPrepStage** defines orchestration policy from ingestion, through data cleaning and feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load -r 22-303 discover/flow/data_processing/data_prep/base/stage.py\n",
    "from typing import List, Type, Union\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "from discover.assets.dataset import Dataset\n",
    "from discover.assets.idgen import AssetIDGen\n",
    "from discover.core.namespace import NestedNamespace\n",
    "from discover.flow.base.task import Task\n",
    "from discover.flow.data_processing.base.stage import DataProcessingStage\n",
    "from discover.infra.service.logging.stage import stage_logger\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "#                                    DATA PREP STAGE                                               #\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "class DataPrepStage(DataProcessingStage):\n",
    "    \"\"\"\n",
    "    A stage class for preparing datasets, handling loading, processing, and saving of data.\n",
    "\n",
    "    This class orchestrates the execution of data preparation tasks, including loading source datasets,\n",
    "    applying a series of tasks, and saving the processed data to a destination repository.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source_config : dict\n",
    "        Configuration for the source dataset, including details like phase, stage, and name.\n",
    "    destination_config : dict\n",
    "        Configuration for the destination dataset, including details like phase, stage, and name.\n",
    "    tasks : List[Task]\n",
    "        A list of tasks to execute as part of the data preparation stage.\n",
    "    force : bool, optional\n",
    "        Whether to force execution if the destination dataset endpoint already exists (default is False).\n",
    "    repo : DatasetRepo, optional\n",
    "        A repository for dataset persistence, injected via dependency injection (default is `DiscoverContainer.repo.dataset_repo`).\n",
    "    **kwargs : dict\n",
    "        Additional keyword arguments for stage configuration.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    _repo : DatasetRepo\n",
    "        The repository instance used for dataset persistence.\n",
    "    _source_asset_id : str\n",
    "        The generated asset ID for the source dataset based on the configuration.\n",
    "    _destination_asset_id : str\n",
    "        The generated asset ID for the destination dataset based on the configuration.\n",
    "    _logger : logging.Logger\n",
    "        Logger instance for logging events related to the data preparation stage.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    run() -> str\n",
    "        Executes the stage by loading the source dataset, applying tasks, and saving the result.\n",
    "    _create_destination_dataset(data: Union[pd.DataFrame, pyspark.sql.DataFrame]) -> Dataset\n",
    "        Creates the destination dataset with the processed data and configuration details.\n",
    "    _load_source_dataset() -> Dataset\n",
    "        Loads the source dataset from the repository using the source asset ID.\n",
    "    _save_destination_dataset(dataset: Dataset) -> None\n",
    "        Saves the processed dataset to the repository using the destination asset ID.\n",
    "    _endpoint_exists(asset_id: str) -> bool\n",
    "        Checks if the dataset endpoint already exists in the repository.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The `DataPrepStage` class leverages dependency injection to retrieve a dataset repository instance.\n",
    "    It ensures that datasets are properly loaded and saved based on the specified configurations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_config: dict,\n",
    "        destination_config: dict,\n",
    "        tasks: List[Task],\n",
    "        asset_idgen: Type[AssetIDGen] = AssetIDGen,\n",
    "        force: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the data preparation stage with source and destination configurations, tasks, and optional parameters.\n",
    "\n",
    "        Args:\n",
    "            source_config (dict): Configuration details for the source dataset.\n",
    "            destination_config (dict): Configuration details for the destination dataset.\n",
    "            tasks (List[Task]): List of tasks to run during this stage.\n",
    "            force (bool, optional): If True, forces execution even if the destination dataset exists (default is False).\n",
    "            repo (DatasetRepo, optional): Repository for dataset persistence, injected via dependency injection.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            source_config=source_config,\n",
    "            destination_config=destination_config,\n",
    "            tasks=tasks,\n",
    "            force=force,\n",
    "        )\n",
    "\n",
    "    @stage_logger\n",
    "    def run(self) -> str:\n",
    "        \"\"\"\n",
    "        Executes the data preparation stage by loading the source dataset, applying tasks, and saving the processed result.\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            str: The asset ID of the destination dataset.\n",
    "\n",
    "        Raises:\n",
    "            DatasetNotFoundError: If the source dataset cannot be loaded from the repository.\n",
    "            DatasetSaveError: If there is an error saving the processed dataset.\n",
    "        \"\"\"\n",
    "        # Obtain the source and destination asset identifiers for repository access\n",
    "        source_asset_id = self._get_asset_id(config=self._source_config)\n",
    "        destination_asset_id = self._get_asset_id(config=self._destination_config)\n",
    "        # Determine whether the endpoint already exists\n",
    "        endpoint_exists = self._dataset_exists(asset_id=destination_asset_id)\n",
    "\n",
    "        # Determine execution path\n",
    "        if self._force:\n",
    "            if endpoint_exists:\n",
    "                self._remove_dataset(asset_id=destination_asset_id)\n",
    "            self._run_task(\n",
    "                source_asset_id=source_asset_id,\n",
    "                destination_asset_id=destination_asset_id,\n",
    "            )\n",
    "        elif endpoint_exists:  # and not forcing execution\n",
    "            # Check if the source data has already been consumed\n",
    "            if not self._is_consumed(asset_id=source_asset_id):\n",
    "                # Updated endpoint with fresh data.\n",
    "                self._update_endpoint(\n",
    "                    source_asset_id=source_asset_id,\n",
    "                    destination_asset_id=destination_asset_id,\n",
    "                )  # Stage specific\n",
    "\n",
    "        else:\n",
    "            self._run_task(\n",
    "                source_asset_id=source_asset_id,\n",
    "                destination_asset_id=destination_asset_id,\n",
    "            )\n",
    "        return destination_asset_id\n",
    "\n",
    "    def _update_endpoint(self, source_asset_id, destination_asset_id):\n",
    "        \"\"\"Updates the endpoint dataset with changes in the source dataset.\n",
    "\n",
    "        Args:\n",
    "            source_asset_id (str): The asset identifier for the input dataset.\n",
    "            destination_asset_id (str): The asset identifier for the output dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        # Obtain the source and destination datasets\n",
    "        source_dataset = self._load_dataset(\n",
    "            asset_id=source_asset_id, config=self._source_config\n",
    "        )\n",
    "        destination_dataset = self._load_dataset(\n",
    "            asset_id=destination_asset_id, config=self._destination_config\n",
    "        )\n",
    "        # Extract the id column and columns created by this process and added to the destination dataset\n",
    "        # prefixed by the stage that created the column.\n",
    "        tqd_cols = [\n",
    "            col\n",
    "            for col in destination_dataset.content.columns\n",
    "            if col.startswith(self.stage_prefix) or col == \"id\"\n",
    "        ]\n",
    "        # Update the destination dataset with updated source dataset content\n",
    "        destination_dataset.content = source_dataset.content.merge(\n",
    "            destination_dataset.content[tqd_cols], how=\"left\", on=\"id\"\n",
    "        )\n",
    "        # Save the destination dataset.\n",
    "        self._save_dataset(dataset=destination_dataset)\n",
    "        # Mark the source dataset metadata as consumed.\n",
    "        source_dataset = self._consumed(dataset=source_dataset)\n",
    "        # Save the source dataset metadata\n",
    "        self._save_dataset_metadata(dataset=source_dataset)\n",
    "\n",
    "    def _run_task(self, source_asset_id: str, destination_asset_id: str) -> None:\n",
    "        \"\"\"Performs the core logic of the Stage, executing tasks in sequence\"\"\"\n",
    "        # Obtain the source dataset\n",
    "        source_dataset = self._load_dataset(\n",
    "            asset_id=source_asset_id, config=self._source_config\n",
    "        )\n",
    "        # Extract the payload\n",
    "        data = source_dataset.content\n",
    "        # Iterate through tasks. Default behavior is that task output is input for subsequent task\n",
    "        for task in self._tasks:\n",
    "            data = task.run(data=data)\n",
    "\n",
    "        # Create the destination dataset\n",
    "        destination_dataset = self._create_dataset(\n",
    "            asset_id=destination_asset_id, config=self._destination_config, data=data\n",
    "        )\n",
    "        self._save_dataset(dataset=destination_dataset)\n",
    "\n",
    "        # Mark the source dataset as having been consumed.\n",
    "        source_dataset = self._consumed(dataset=source_dataset)\n",
    "        # Save the source dataset metadata only. We can do this without integrity errors as\n",
    "        # the dataset content is immutable.\n",
    "        self._save_dataset_metadata(dataset=source_dataset)\n",
    "\n",
    "    def _is_consumed(self, asset_id: str) -> bool:\n",
    "        \"\"\"Returns a boolean indicating whether the data for the given asset_id is fresh.\n",
    "\n",
    "        Args:\n",
    "            asset_id (str): The asset identifier for the dataset\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the dataset is fresh.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self._repo.is_consumed(asset_id=asset_id)\n",
    "        except Exception as e:\n",
    "            msg = f\"Error checking dataset {asset_id} freshness: {str(e)}\"\n",
    "            self._logger.error(msg)\n",
    "            raise RuntimeError(msg)\n",
    "\n",
    "    def _consumed(self, dataset: Dataset) -> Dataset:\n",
    "        \"\"\"Marks the dataset as having been consumed\"\"\"\n",
    "        dataset.consumed = True\n",
    "        dataset.dt_consumed = datetime.now()\n",
    "        dataset.consumed_by = self.__class__.__name__\n",
    "        return dataset\n",
    "\n",
    "    def _load_dataset(\n",
    "        self, asset_id: str, config: NestedNamespace\n",
    "    ) -> Union[pd.DataFrame, DataFrame]:\n",
    "        \"\"\"\n",
    "        Loads the a dataset from the repository.\n",
    "\n",
    "        Args:\n",
    "            asset_id (str): The identifier for the dataset asset.\n",
    "            config (NestedNamespace): Dataset configuration Specifies whether the load the\n",
    "                dataset as a pandas or distributed  (PySpark) DataFrame with or without NLP\n",
    "                dependencies.\n",
    "\n",
    "        Returns:\n",
    "            Union[pd.DataFrame, DataFrame]: Pandas or PySpark DataFrame.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the source dataset cannot be found or loaded.\n",
    "            RuntimeError: If unrecognized error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dataset = self._repo.get(\n",
    "                asset_id=asset_id,\n",
    "                distributed=config.distributed,\n",
    "                nlp=config.nlp,\n",
    "            )\n",
    "            # Marks the dataset as no longer fresh acknowledging the dataset has been\n",
    "            # consumed to avoid unnecessary processing in future executions when data\n",
    "            # hasn't changed.\n",
    "            self._repo.consumed(asset_id=asset_id)\n",
    "\n",
    "            if config.distributed:\n",
    "                # Rename the pandas index column if it exists. This is annoyance that occurs\n",
    "                # when a file saved with pandas indexes is read as a PySpark DataFrame\n",
    "                if \"__index_level_0__\" in dataset.content.columns:\n",
    "                    dataset.content = dataset.content.withColumnRenamed(\n",
    "                        \"__index_level_0__\", \"pandas_index\"\n",
    "                    )\n",
    "            return dataset\n",
    "        except FileNotFoundError as e1:\n",
    "            msg = f\"Dataset {asset_id} not found in the repository.\\n{e1}\"\n",
    "            self._logger.error(msg)\n",
    "            raise FileNotFoundError(msg)\n",
    "        except Exception as e2:\n",
    "            msg = f\"RuntimeError encountered while attempting to load dataset {asset_id}.\\n{e2}\"\n",
    "            self._logger.error(msg)\n",
    "            raise RuntimeError(msg)\n",
    "\n",
    "    def _save_dataset_metadata(self, dataset: Dataset) -> None:\n",
    "        \"\"\"\n",
    "        Saves a dataset metadata to the repository.\n",
    "\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset to be saved.\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If saving the dataset fails.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._repo.update_dataset_metadata(dataset=dataset)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to save dataset metadata\\n{dataset}\\n{str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We began with the following simple, abstract piece of code to configure and execute the **Text Quality Analysis (TQA) Pipeline**.\n",
    "\n",
    "While concise, this initial implementation lacked transparency, discoverability, and reproducibility. To address these shortcomings, we embarked on a odyssey to dissect and understand the essential components of our pipeline orchestration architecture.\n",
    "\n",
    "#### Steps Taken\n",
    "- **Task Definition**: We defined the individual tasks required for the Text Quality Analysis Pipeline, ensuring each task's logic was clear and reproducible.\n",
    "- **Pipeline Construction**: We assembled the TQA Pipeline using the eight core components of our orchestration framework, including stages, tasks, and data management entities. This provided us with a robust, configuration-driven approach to workflow management.\n",
    "\n",
    "We have now come full circle, revisiting the original four lines of code that build and execute the pipeline. However, armed with an understanding of the underlying components and how they interact, the abstract code is no longer a black box, but (we hope) an *understandable* abstraction that effectively hides complexity, without introducing cognitive burden or new friction-inducing complexities. \n",
    "\n",
    "With that, runall in 3,2,...  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# ============================================================================== #\n",
      "#                          Text Quality Analysis Stage                           #\n",
      "# ============================================================================== #\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/john/miniconda3/envs/appvocai/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/john/.ivy2/cache\n",
      "The jars for the packages stored in: /home/john/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6a70032e-bed4-49ad-9247-fa976969d383;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;5.3.3 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-s3;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-kms;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-core;1.12.500 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound software.amazon.ion#ion-java;1.0.2 in central\n",
      "\tfound joda-time#joda-time;2.8.1 in central\n",
      "\tfound com.amazonaws#jmespath-java;1.12.500 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime;1.17.0 in central\n",
      ":: resolution report :: resolve 1852ms :: artifacts dl 72ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-core;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-kms;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-s3;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#jmespath-java;1.12.500 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;5.3.3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime;1.17.0 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\tjoda-time#joda-time;2.8.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\tsoftware.amazon.ion#ion-java;1.0.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcommons-logging#commons-logging;1.2 by [commons-logging#commons-logging;1.1.3] in [default]\n",
      "\tcommons-codec#commons-codec;1.11 by [commons-codec#commons-codec;1.15] in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   83  |   0   |   0   |   5   ||   78  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6a70032e-bed4-49ad-9247-fa976969d383\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 78 already retrieved (0kB/40ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                                    NLPTask                                     \n",
      "                                    -------                                     \n",
      "                          Start Datetime | Mon, 18 Nov 2024 04:18:45\n",
      "pos_ud_ewt download started this may take some time.\n",
      "Approximate size to download 2.2 MB\n",
      "[ | ]pos_ud_ewt download started this may take some time.\n",
      "Approximate size to download 2.2 MB\n",
      "[ / ]Download done! Loading the resource.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:====================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ — ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n",
      "                       Complete Datetime | Mon, 18 Nov 2024 04:19:06\n",
      "                                 Runtime | 21.1 seconds\n",
      "\n",
      "\n",
      "                             ComputeTextQualityTask                             \n",
      "                             ----------------------                             \n",
      "                          Start Datetime | Mon, 18 Nov 2024 04:19:06\n",
      "                       Complete Datetime | Mon, 18 Nov 2024 04:19:07\n",
      "                                 Runtime | 0.89 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=====================================================>  (23 + 1) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                          Text Quality Analysis Stage                           \n",
      "                          ===========================                           \n",
      "                           Stage Started | Mon, 18 Nov 2024 04:18:21\n",
      "                         Stage Completed | Mon, 18 Nov 2024 04:20:33\n",
      "                           Stage Runtime | 2.0 minutes and 12.35 seconds\n",
      "\n",
      "\n",
      "# ============================================================================ #\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Obtain the configuration\n",
    "reader = FlowConfigReader()\n",
    "stage_config = reader.get_stage_config(phase=PhaseDef.DATAPREP, stage=StageDef.TQA)\n",
    "\n",
    "# Build and run Data Ingestion Stage\n",
    "stage = TQAStage.build(stage_config=stage_config, force=FORCE)\n",
    "asset_id = stage.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Transition to the Data Quality Analysis (DQA):\n",
    "With the **Text Quality Analysis (TQA) Pipeline** now complete, we have the linguistic elements that contribute to a holistic assessment of text quality for NLP applications. These enriched text quality measures are determinative inputs for our next stage: the **Data Quality Analysis (DQA)**. \n",
    "\n",
    "In the DQA, we’ll dilate our aperture, integrating sentiments, typographical, and linguistic metrics across several dimensions of data quality, allowing us to uncover areas of concern, and devise further data processing interventions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appvocai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
