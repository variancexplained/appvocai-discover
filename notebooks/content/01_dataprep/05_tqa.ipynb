{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "\n",
    "if \"jbook\" in os.getcwd():\n",
    "    os.chdir(os.path.abspath(os.path.join(\"../..\")))\n",
    "FORCE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Quality Analysis (TQA) \n",
    "Review text quality is an indicator of the content's richness, coherence, and informativeness. In this section, we integrate two complementary quality assessment measures—a lexical/syntactic complexity score and a perplexity-based coherence score—into a weighted sum. This approach provides a balanced evaluation, capturing both the structural diversity and natural language fluency of the reviews.\n",
    "\n",
    "## Syntactic and Lexical Complexity Assessment\n",
    "The lexical and syntactic quality assessment (TQA) evaluates review quality using a composite score derived from multiple syntactic and lexical measures. These measures are computed with specific weights:\n",
    "\n",
    "- **Syntactic Extent Score** (40%): Reflects the richness of content using counts of nouns, verbs, adjectives, and adverbs.\n",
    "- **Syntactic Diversity Score** (20%): Captures variety in language using an entropy-based calculation.\n",
    "- **Syntactic Complexity Score** (10%): Assesses the density of key parts of speech relative to total word count.\n",
    "- **Lexical Complexity Score** (20%): Evaluates text complexity using unique word proportion, special character usage, and word length variation.\n",
    "- **Typography Score** (10%): Incorporates quality signals such as limited digit use, minimal special characters, and proper terminal punctuation.\n",
    "\n",
    "A high Lexical and Syntactic Complexity Score typically indicates a text rich in linguistic features, with varied sentence structures and a well-balanced mix of nouns, verbs, and modifiers (like adjectives and adverbs). This variety is particularly valuable for tasks like Aspect-Based Sentiment Analysis (ABSA), where structural complexity can signal content with nuanced aspects and sentiments.\n",
    "\n",
    "## Coherence - Perplexity-Based Quality Assessment\n",
    "This measure evaluates review quality by applying 14 linguistic and structural filters, each assigned a weight derived from relative perplexity differences between the full dataset and filtered subsets. The filters assess features like adjective presence, punctuation ratios, word repetition, and special character use. Weights are computed to emphasize filters that most reduce perplexity, thus enhancing text fluency and coherence. The final score is a weighted sum of these filter indicators.\n",
    "\n",
    "Lower perplexity implies higher fluency, coherence, and grammatical correctness, which are key indicators of text quality. This component is useful for flagging low-quality or noisy text that may be unpredictable or deviate significantly from standard linguistic norms.\n",
    "\n",
    "## Weighted Scoring Approach\n",
    "To create a balanced quality score, the Syntactic Complexity Score and Perplexity-Based Score are combined with tailored weights that emphasize their respective strengths.\n",
    "\n",
    "- **Lexical and Syntactic Complexity Weight**: Typically given more weight when the task demands detailed and linguistically rich text, such as ABSA, where richer syntactic content improves aspect and sentiment extraction.\n",
    "- **Perplexity-Based Weight**: Often assigned a moderate weight to capture coherence and fluency, ensuring that only grammatically sound and predictable text is prioritized without sacrificing syntactic diversity.\n",
    "\n",
    "The final **Text Quality Score** is a weighted average of these two components, providing a single score that balances both syntactic richness and linguistic fluency. The remainder of this notebook will execute the text quality scoring pipeline, computing and integrating the two quality measures into a final text quality score. The specifics of the measures are provided in {ref}`appendix:tqa`.\n",
    "\n",
    "## Purpose of this Notebook\n",
    "Reproducibility and code transparency are critical principles in scientific research, reinforcing the credibility and trustworthiness of our findings. Clean, repeatable script-based workflows that trace the journey from raw data to clean data and final analyses are essential components of this reproducibility.\n",
    "\n",
    "To this point, our tendency to abstract away *implementation details* in favor of narrative coherence and focus on what *we* believe is important, has been in contravention of these ideas. In this notebook, we lean into code transparency and reproducibility. Our aim here is to describe our text quality analysis methodology in detail, then walk through the process of configuring, constructing, and executing the workflow; thereby, illustrating how configurations, tasks, and stages are materialized into operational pipelines here, and in downstream workflows. By exposing these *details* here, we hope to establish a shared understanding that will allow for abstraction in downstream tasks without sacrificing clarity or trust.\n",
    "\n",
    "Here we go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from discover.container import DiscoverContainer\n",
    "from discover.infra.config.flow import FlowConfigReader\n",
    "from discover.core.flow import DataPrepStageDef\n",
    "from discover.flow.data_prep.tqa.stage import TQAStage\n",
    "from discover.core.flow import PhaseDef, DataPrepStageDef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = DiscoverContainer()\n",
    "container.init_resources()\n",
    "container.wire(\n",
    "    modules=[\n",
    "        \"discover.flow.data_prep.stage\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Quality Analysis Pipeline\n",
    "Here, we configure the pipeline, construct the `TQAStage` object and run it."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Obtain the configuration\n",
    "reader = FlowConfigReader()\n",
    "stage_config = reader.get_stage_config(\n",
    "    phase=PhaseDef.DATAPREP, stage=DataPrepStageDef.TQA\n",
    ")\n",
    "\n",
    "# Build and run Text Quality Analysis Pipeline\n",
    "stage = TQAStage.build(stage_config=stage_config, force=FORCE)\n",
    "asset_id = stage.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viola! How's that for code transparency? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![reproduciblity](../figs/reproducibility.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Reproducibility and code transparency are critical principles in scientific research, reinforcing the credibility and trustworthiness of our findings. Clean, repeatable script-based pipelines that trace the journey from raw data to clean data and final analyses are essential components of this reproducibility.\n",
    "\n",
    "> The trustworthiness of the results of such pipelines rests entirely on their ability to be reproduced with fidelity, which is difficult if pipelines are not documented or recorded minutely and consistently. This difficulty has led to a reproducibility crisis...{cite}`rupprechtImprovingReproducibilityData2020`. \n",
    "\n",
    "To this point, our tendency to abstract away *implementation details* in favor of narrative coherence and focus on what *we* believe is important, has been in contravention of these ideas. Code snippets like that above In this notebook, we lean into code transparency and reproducibility. This notebook is an attemptIn this notebook, we  we lean into code transparency and reproducibility. Here, we describe our text quality analysis methodology in detail, then walk through the process of configuring, constructing, and executing the workflow; thereby, illustrating how configurations, tasks, and stages are materialized into pipelines here, and in downstream workflows. \n",
    "\n",
    "For whom such detail is overly \n",
    "With that rather prolix introduction, \n",
    "\n",
    "By exposing these *details* here, we hope to establish a shared understanding that will allow for abstraction in downstream tasks without sacrificing clarity or trust.\n",
    "\n",
    "Here we go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Quality Analysis Pipeline Tasks\n",
    "The tasks we'll define below execute in three stages:\n",
    "1. **Syntactic and Lexical Complexity Scoring Tasks**: These tasks compute the syntactic/lexical complexity scores for each review.\n",
    "2. **Perplexity-Based Coherence Scoring Tasks**: Coeherence scores for each review are computed here.\n",
    "3. **Text Quality Scoring Task**: Finally, we compute the composite score, combining the above two measures into a weighted sum, reflecting a balanced evaluation of the quality of each review.\n",
    "\n",
    "Here, we go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntactic and Lexical Complexity Scoring Tasks\n",
    "The **Syntactic and Lexical Complexity Score** is computed through a series of tasks:\n",
    "\n",
    "1. **NLPTask**: Leverages SparkNLP to tokenize the review text and assign part-of-speech (POS) tags to each word.\n",
    "2. **ComputeSyntacticStatsTask**: Calculates syntactic metrics, such as raw counts of each POS and the proportions of review text represented by each POS.\n",
    "3. **ComputeLexicalStatsTask**: Computes lexical complexity statistics, including unique word counts, word proportions, and word length metrics.\n",
    "4. **ComputeSyntacticLexicalComplexityScore**: Combines the syntactic and lexical metrics to produce a comprehensive complexity score.\n",
    "\n",
    "Well, I checked StackExchange, and no, these tasks aren't going to define themselves. So, Alex, my SparkNLP coding hat, please!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLPTask\n",
    "The **NLPTask** class utilizes a Spark ML Pipeline to streamline NLP preprocessing on text data within a Spark DataFrame. It begins by assembling raw text into a structured document format using SparkNLP's `DocumentAssembler`. The input is then tokenized, preparing it for POS tagging with a pretrained `PerceptronModel`. Finally, the `Finisher` stage converts the processed NLP annotations into plain lists of tokens and POS tags, making them easily accessible and usable in the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 19-140 discover/flow/data_prep/tqa/task.py\n",
    "import math\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from sparknlp.annotator import PerceptronModel, Tokenizer\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "\n",
    "from discover.flow.base.task import Task\n",
    "from discover.infra.service.logging.task import task_logger\n",
    "from discover.infra.utils.file.io import IOService\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "#                                       NLP TASK                                                   #\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "class NLPTask(Task):\n",
    "    \"\"\"\n",
    "    A class to perform NLP preprocessing on a specified content column in a Spark DataFrame.\n",
    "    This task includes tokenization, POS tagging, and formatting of the output as plain lists.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    column : str\n",
    "        The name of the column containing content data to process (default is \"content\").\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    run(data: DataFrame) -> DataFrame\n",
    "        Executes the NLP pipeline on the provided DataFrame, adding token and POS tag columns.\n",
    "\n",
    "    _build_pipeline() -> Pipeline\n",
    "        Constructs a Spark ML Pipeline with stages for document assembly, tokenization,\n",
    "        POS tagging, and output formatting using a Finisher.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, column: str = \"content\") -> None:\n",
    "        super().__init__()\n",
    "        self._column = column\n",
    "\n",
    "    @task_logger\n",
    "    def run(self, data: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Executes the NLP pipeline on the input DataFrame, applying tokenization and POS tagging,\n",
    "        and returns the transformed DataFrame with additional columns for tokens and POS tags.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : DataFrame\n",
    "            The Spark DataFrame containing the content data column specified during initialization.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        DataFrame\n",
    "            A transformed Spark DataFrame with new columns: 'tokens' and 'pos', containing lists\n",
    "            of tokens and POS tags, respectively.\n",
    "        \"\"\"\n",
    "        pipeline = self._build_pipeline()\n",
    "        return pipeline.fit(data).transform(data)\n",
    "\n",
    "    def _build_pipeline(self) -> Pipeline:\n",
    "        \"\"\"\n",
    "        Builds and returns a Spark ML Pipeline with stages for document assembly, tokenization,\n",
    "        POS tagging, and a Finisher for output formatting.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Pipeline\n",
    "            A configured Spark Pipeline that performs NLP tasks including tokenization, POS tagging,\n",
    "            and result formatting for easy integration into a DataFrame.\n",
    "        \"\"\"\n",
    "        # Assembles raw content data into a Spark NLP document\n",
    "        document_assembler = (\n",
    "            DocumentAssembler().setInputCol(self._column).setOutputCol(\"document\")\n",
    "        )\n",
    "\n",
    "        # Tokenizer splits words for NLP processing\n",
    "        tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"tokens\")\n",
    "\n",
    "        # POS Tagging with a pretrained model\n",
    "        pos = (\n",
    "            PerceptronModel.pretrained(\"pos_ud_ewt\", \"en\")\n",
    "            .setInputCols([\"document\", \"tokens\"])\n",
    "            .setOutputCol(\"pos_tags\")\n",
    "        )\n",
    "\n",
    "        # Finisher converts annotations to plain lists for DataFrame output\n",
    "        finisher = (\n",
    "            Finisher()\n",
    "            .setInputCols([\"tokens\", \"pos_tags\"])\n",
    "            .setOutputCols([\"tp_tokens\", \"tp_pos\"])\n",
    "        )\n",
    "\n",
    "        # Create and return Pipeline with the defined stages\n",
    "        pipeline = Pipeline(\n",
    "            stages=[\n",
    "                document_assembler,\n",
    "                tokenizer,\n",
    "                pos,\n",
    "                finisher,\n",
    "            ]\n",
    "        )\n",
    "        return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Syntactic Complexity Measures\n",
    "This code processes a Spark DataFrame containing lists of part-of-speech (POS) tags for each text entry. First, it counts the total number of POS tags per entry. It then calculates the counts of specific POS types, such as nouns, verbs, adjectives, and adverbs. Next, it computes the proportion of each POS type relative to the total POS count, ensuring these ratios are only calculated when the total count is greater than zero. Finally, the code cleans up by removing any intermediate columns that are no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 145-249 discover/flow/data_prep/tqa/task.py\n",
    "class ComputeSyntacticStatsTask(Task):\n",
    "    \"\"\"\n",
    "    A task to compute Part-of-Speech (POS) statistics for a specified column in a PySpark DataFrame.\n",
    "\n",
    "    This task generates counts and proportions for specific POS tags (nouns, verbs, adjectives, adverbs, determiners)\n",
    "    based on POS tags available in the input DataFrame. These statistics are useful for analyzing the linguistic\n",
    "    characteristics of the text in each row.\n",
    "\n",
    "    Attributes:\n",
    "        column (str): The name of the column containing the text or POS data to analyze. Defaults to \"content\".\n",
    "\n",
    "    Methods:\n",
    "        run(data: DataFrame) -> DataFrame:\n",
    "            Executes the POS statistics calculations on the specified column of the input DataFrame and returns\n",
    "            the DataFrame with the new POS statistics columns.\n",
    "\n",
    "    POS Statistics Columns:\n",
    "        pos_n_nouns (int): The number of noun tags in the text.\n",
    "        pos_n_verbs (int): The number of verb tags in the text.\n",
    "        pos_n_adjectives (int): The number of adjective tags in the text.\n",
    "        pos_n_adverbs (int): The number of adverb tags in the text.\n",
    "        pos_n_determiners (int): The number of determiner tags in the text.\n",
    "        pos_p_nouns (float): The proportion of noun tags relative to the total POS tags.\n",
    "        pos_p_verbs (float): The proportion of verb tags relative to the total POS tags.\n",
    "        pos_p_adjectives (float): The proportion of adjective tags relative to the total POS tags.\n",
    "        pos_p_adverbs (float): The proportion of adverb tags relative to the total POS tags.\n",
    "        pos_p_determiners (float): The proportion of determiner tags relative to the total POS tags.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, column: str = \"content\") -> None:\n",
    "        \"\"\"\n",
    "        Initializes the ComputeSyntacticStatsTask with the specified text or POS column.\n",
    "\n",
    "        Args:\n",
    "            column (str): The name of the column containing the POS data. Defaults to \"content\".\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._column = column\n",
    "\n",
    "    @task_logger\n",
    "    def run(self, data: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Executes the POS statistics calculations on the specified column.\n",
    "\n",
    "        The function calculates the counts and proportions of specific POS tags (nouns, verbs, adjectives, adverbs,\n",
    "        determiners) within each entry of the specified column. The resulting statistics are added as new columns\n",
    "        in the DataFrame.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The input PySpark DataFrame containing the POS tags as a list in the \"tp_pos\" column.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: The input DataFrame with additional POS statistics columns.\n",
    "        \"\"\"\n",
    "\n",
    "        # Assuming `tp_pos` column contains lists of POS tags for each entry\n",
    "        # Step 1: Calculate total POS tag count per entry\n",
    "        data = data.withColumn(\"pos_count\", F.size(\"tp_pos\"))\n",
    "\n",
    "        # Step 1: Calculate counts of specific POS tags (e.g., NOUN, VERB, ADJ)\n",
    "        data = data.withColumn(\n",
    "            \"pos_n_nouns\", F.expr(\"size(filter(tp_pos, x -> x = 'NOUN'))\")\n",
    "        )\n",
    "        data = data.withColumn(\n",
    "            \"pos_n_verbs\", F.expr(\"size(filter(tp_pos, x -> x = 'VERB'))\")\n",
    "        )\n",
    "        data = data.withColumn(\n",
    "            \"pos_n_adjectives\", F.expr(\"size(filter(tp_pos, x -> x = 'ADJ'))\")\n",
    "        )\n",
    "        data = data.withColumn(\n",
    "            \"pos_n_adverbs\", F.expr(\"size(filter(tp_pos, x -> x = 'ADV'))\")\n",
    "        )\n",
    "\n",
    "        # Step 2: Calculate ratios/percentages of specific POS tags\n",
    "        data = data.withColumn(\n",
    "            \"pos_p_nouns\",\n",
    "            F.when(\n",
    "                F.col(\"pos_count\") > 0, F.col(\"pos_n_nouns\") / F.col(\"pos_count\")\n",
    "            ).otherwise(0),\n",
    "        )\n",
    "        data = data.withColumn(\n",
    "            \"pos_p_verbs\",\n",
    "            F.when(\n",
    "                F.col(\"pos_count\") > 0, F.col(\"pos_n_verbs\") / F.col(\"pos_count\")\n",
    "            ).otherwise(0),\n",
    "        )\n",
    "        data = data.withColumn(\n",
    "            \"pos_p_adjectives\",\n",
    "            F.when(\n",
    "                F.col(\"pos_count\") > 0, F.col(\"pos_n_adjectives\") / F.col(\"pos_count\")\n",
    "            ).otherwise(0),\n",
    "        )\n",
    "        data = data.withColumn(\n",
    "            \"pos_p_adverbs\",\n",
    "            F.when(\n",
    "                F.col(\"pos_count\") > 0, F.col(\"pos_n_adverbs\") / F.col(\"pos_count\")\n",
    "            ).otherwise(0),\n",
    "        )\n",
    "\n",
    "        # Drop intermediate column if not needed\n",
    "        data = data.drop(\"pos_count\")\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Lexical Complexity Measures\n",
    "This task calculates various lexical metrics for text data in a PySpark DataFrame. It starts by computing basic character-based statistics, such as the total character count, digit count, and punctuation count, and then calculates their respective proportions. Next, the text is tokenized into words to determine word counts, unique word counts, and the ratio of unique to total words, providing insights into vocabulary richness. The task also measures word length statistics, including the minimum, maximum, mean, and standard deviation, to capture the complexity of the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 253-395 discover/flow/data_prep/tqa/task.py\n",
    "class ComputeLexicalStatsTask(Task):\n",
    "    \"\"\"\n",
    "    A task to compute basic text statistics for a specified column in a PySpark DataFrame.\n",
    "\n",
    "    This task generates various statistics for text data, such as character count, digit and punctuation counts,\n",
    "    word count, unique word count, and word length statistics, which are useful for analyzing the content and structure\n",
    "    of text in each row.\n",
    "\n",
    "    Attributes:\n",
    "        column (str): The name of the column containing the text data to analyze. Defaults to \"content\".\n",
    "\n",
    "    Methods:\n",
    "        run(data: DataFrame) -> DataFrame:\n",
    "            Executes the basic statistics calculations on the specified column of the input DataFrame and returns\n",
    "            the DataFrame with the new statistics columns.\n",
    "\n",
    "    Basic Statistics Columns:\n",
    "        stats_char_count (int): The total number of characters in the text.\n",
    "        stats_digits_count (int): The total number of digits in the text.\n",
    "        stats_digits_proportion (float): The proportion of digits to total characters.\n",
    "        stats_special_chars_count (int): The total number of punctuation marks in the text.\n",
    "        stats_special_chars_proportion (float): The proportion of punctuation marks to total characters.\n",
    "        stats_word_count (int): The total number of words in the text.\n",
    "        stats_unique_word_count (int): The total number of unique words in the text.\n",
    "        stats_unique_word_proportion (float): The proportion of unique words to total words.\n",
    "        stats_word_length_min (int): The minimum word length in the text.\n",
    "        stats_word_length_max (int): The maximum word length in the text.\n",
    "        stats_word_length_mean (float): The mean word length in the text.\n",
    "        stats_word_length_std (float): The standard deviation of word lengths in the text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, column: str = \"content\") -> None:\n",
    "        \"\"\"\n",
    "        Initializes the ComputeLexicalStatsTask with the specified text column.\n",
    "\n",
    "        Args:\n",
    "            column (str): The name of the column containing the text data to analyze. Defaults to \"content\".\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._column = column\n",
    "\n",
    "    @task_logger\n",
    "    def run(self, data: DataFrame) -> DataFrame:\n",
    "        # 1. Character count\n",
    "        data = data.withColumn(\"stats_char_count\", F.length(self._column))\n",
    "\n",
    "        # 2. Digits count\n",
    "        data = data.withColumn(\n",
    "            \"stats_digits_count\",\n",
    "            F.expr(\"regexp_count(content, '[^0-9]')\"),\n",
    "        )\n",
    "\n",
    "        # 3. Digits proportion\n",
    "        data = data.withColumn(\n",
    "            \"stats_digits_proportion\",\n",
    "            F.when(\n",
    "                F.col(\"stats_char_count\") > 0,\n",
    "                F.col(\"stats_digits_count\") / F.col(\"stats_char_count\"),\n",
    "            ).otherwise(0),\n",
    "        )\n",
    "\n",
    "        # 4. Special chars count\n",
    "        data = data.withColumn(\n",
    "            \"stats_special_chars_count\",\n",
    "            F.expr(\"regexp_count(content, r'[^\\\\w\\\\s]')\"),\n",
    "        )\n",
    "\n",
    "        # 5. Special chars proportion\n",
    "        data = data.withColumn(\n",
    "            \"stats_special_chars_proportion\",\n",
    "            F.when(\n",
    "                F.col(\"stats_char_count\") > 0,\n",
    "                F.col(\"stats_special_chars_count\") / F.col(\"stats_char_count\"),\n",
    "            ).otherwise(0),\n",
    "        )\n",
    "\n",
    "        # 8. Split content into words\n",
    "        data = data.withColumn(\"words\", F.split(F.col(self._column), \"\\\\s+\"))\n",
    "\n",
    "        # 9. Word count\n",
    "        data = data.withColumn(\"stats_word_count\", F.size(\"words\"))\n",
    "\n",
    "        # 10. Unique word count\n",
    "        data = data.withColumn(\"unique_words\", F.array_distinct(\"words\"))\n",
    "        data = data.withColumn(\"stats_unique_word_count\", F.size(\"unique_words\"))\n",
    "\n",
    "        # 11. Unique word proportion\n",
    "        data = data.withColumn(\n",
    "            \"stats_unique_word_proportion\",\n",
    "            F.when(\n",
    "                F.col(\"stats_word_count\") > 0,\n",
    "                F.col(\"stats_unique_word_count\") / F.col(\"stats_word_count\"),\n",
    "            ).otherwise(0),\n",
    "        )\n",
    "\n",
    "        # 12. Word Repetition Ratio\n",
    "        data = data.withColumn(\n",
    "            \"stats_word_repetition_ratio\", 1 - F.col(\"stats_unique_word_proportion\")\n",
    "        )\n",
    "\n",
    "        # Drop intermediate columns\n",
    "        data = data.drop(\"words\", \"unique_words\")\n",
    "\n",
    "        # 14. Word length statistics\n",
    "        # Split content into words and calculate word lengths\n",
    "        data = data.withColumn(\n",
    "            \"word_lengths\",\n",
    "            F.expr(\"transform(split(content, '\\\\\\\\s+'), x -> length(x))\"),\n",
    "        )\n",
    "\n",
    "        # Minimum word length\n",
    "        data = data.withColumn(\"stats_word_length_min\", F.array_min(\"word_lengths\"))\n",
    "\n",
    "        # Maximum word length\n",
    "        data = data.withColumn(\"stats_word_length_max\", F.array_max(\"word_lengths\"))\n",
    "\n",
    "        # Mean word length\n",
    "        data = data.withColumn(\n",
    "            \"stats_word_length_mean\",\n",
    "            F.expr(\n",
    "                \"aggregate(transform(word_lengths, x -> CAST(x AS DOUBLE)), CAST(0.0 AS DOUBLE), (acc, x) -> acc + x) / size(word_lengths)\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Standard deviation of word length\n",
    "        data = data.withColumn(\n",
    "            \"stats_word_length_std\",\n",
    "            F.when(\n",
    "                F.size(\"word_lengths\") > 1,\n",
    "                F.sqrt(\n",
    "                    F.expr(\n",
    "                        \"aggregate(transform(word_lengths, x -> CAST(x AS DOUBLE)), CAST(0.0 AS DOUBLE), (acc, x) -> acc + pow(x - stats_word_length_mean, 2)) / size(word_lengths)\"\n",
    "                    )\n",
    "                ),\n",
    "            ).otherwise(0),\n",
    "        )\n",
    "\n",
    "        # Drop intermediate column if not needed\n",
    "        data = data.drop(\"word_lengths\")\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Syntactic/Lexical Complexity Score\n",
    "To evaluate the richness and complexity of text data, this method calculates a comprehensive **SyntacticLexical Complexity Score** composed of three main components:\n",
    "\n",
    "1. **POS Diversity Score**: Using an entropy-based approach, this score quantifies the variety of part-of-speech (POS) tags, such as nouns, verbs, adjectives, and adverbs. It highlights how diverse the grammatical structures are within the text.\n",
    "\n",
    "2. **POS Density Score**: By examining the ratio of total POS tags to the word count, this score captures the intensity of POS usage. It indicates how grammatically dense or varied the language is throughout the text.\n",
    "\n",
    "3. **Lexical Complexity Score**: This metric assesses vocabulary sophistication. It factors in the proportion of unique words, the prevalence of special characters, and the variability in word lengths, offering a detailed look at the text’s lexical intricacy.\n",
    "\n",
    "Together, these scores are combined in a weighted manner to form the overall **SyntacticLexical Complexity Score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 400-579 discover/flow/data_prep/tqa/task.py\n",
    "class ComputeSyntacticLexicalScoresTask(Task):\n",
    "    \"\"\"\n",
    "    A task to compute a Text Quality Assessment (TQA) score based on various components\n",
    "    such as POS count, POS diversity, lexical complexity, POS intensity, and TQA quality checks.\n",
    "\n",
    "    Attributes:\n",
    "        pos_diversity_weight (float): The weight assigned to the POS diversity component.\n",
    "        pos_density_weight (float): The weight assigned to the POS intensity component.\n",
    "        lexical_complexity_weight (float): The weight assigned to the lexical complexity component.\n",
    "        column (str): Column containing review text.\n",
    "        new_column (str): The name of the output column to store the computed TQA score.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pos_diversity_weight: float,\n",
    "        pos_density_weight: float,\n",
    "        lexical_complexity_weight: float,\n",
    "        column: str = \"content\",\n",
    "        new_column: str = \"tqa_syntactic_lexical_score\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the ComputeSyntacticLexicalScoresTask with specified weights and output column name.\n",
    "\n",
    "        Args:\n",
    "            pos_diversity_weight (float): Weight for the POS diversity component.\n",
    "            pos_density_weight (float): Weight for the POS intensity component.\n",
    "            lexical_complexity_weight (float): Weight for the lexical complexity component.\n",
    "            new_column (str): Name of the output column for the TQA score. Defaults to \"enrichment_tqa_score1\".\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._pos_diversity_weight = pos_diversity_weight\n",
    "        self._pos_density_weight = pos_density_weight\n",
    "        self._lexical_complexity_weight = lexical_complexity_weight\n",
    "        self._column = column\n",
    "        self._new_column = new_column\n",
    "\n",
    "    @task_logger\n",
    "    def run(self, data: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Executes the TQA score computation by applying several components as UDFs.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The input PySpark DataFrame containing text data and related features.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: The input DataFrame with additional columns for each component score\n",
    "            and the final TQA score.\n",
    "        \"\"\"\n",
    "\n",
    "        # Define UDFs for each computation\n",
    "\n",
    "        @F.udf(\"float\")\n",
    "        def compute_pos_diversity_score(\n",
    "            content, pos_p_nouns, pos_p_verbs, pos_p_adjectives, pos_p_adverbs\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Computes the POS diversity score using an entropy-based calculation.\n",
    "\n",
    "            Args:\n",
    "                content (str): The text content of the review.\n",
    "                pos_p_nouns (float): Proportion of nouns in the content.\n",
    "                pos_p_verbs (float): Proportion of verbs in the content.\n",
    "                pos_p_adjectives (float): Proportion of adjectives in the content.\n",
    "                pos_p_adverbs (float): Proportion of adverbs in the content.\n",
    "\n",
    "            Returns:\n",
    "                float: The computed POS diversity score.\n",
    "            \"\"\"\n",
    "            if len(content) > 2:\n",
    "                pos_tags = [pos_p_nouns, pos_p_verbs, pos_p_adjectives, pos_p_adverbs]\n",
    "                pos_diversity = -sum(p * math.log(p) for p in pos_tags if p > 0)\n",
    "                return float(pos_diversity * self._pos_diversity_weight)\n",
    "            return 0.0\n",
    "\n",
    "        @F.udf(\"float\")\n",
    "        def compute_pos_density_score(\n",
    "            content,\n",
    "            pos_n_nouns,\n",
    "            pos_n_verbs,\n",
    "            pos_n_adjectives,\n",
    "            pos_n_adverbs,\n",
    "            stats_word_count,\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Computes the POS intensity score based on the number of POS tags relative\n",
    "            to the word count.\n",
    "\n",
    "            Args:\n",
    "                content (str): The text content of the review.\n",
    "                pos_n_nouns (int): Number of nouns in the content.\n",
    "                pos_n_verbs (int): Number of verbs in the content.\n",
    "                pos_n_adjectives (int): Number of adjectives in the content.\n",
    "                pos_n_adverbs (int): Number of adverbs in the content.\n",
    "                stats_word_count (int): Total word count in the content.\n",
    "\n",
    "            Returns:\n",
    "                float: The computed POS intensity score.\n",
    "            \"\"\"\n",
    "            if len(content) > 2 and stats_word_count > 0:\n",
    "                pos_density = (\n",
    "                    pos_n_nouns + pos_n_verbs + pos_n_adjectives + pos_n_adverbs\n",
    "                ) / stats_word_count\n",
    "                return float(pos_density * self._pos_density_weight)\n",
    "            return 0.0\n",
    "\n",
    "        @F.udf(\"float\")\n",
    "        def compute_lexical_complexity_score(\n",
    "            content,\n",
    "            stats_unique_word_proportion,\n",
    "            stats_special_chars_proportion,\n",
    "            stats_word_length_std,\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Computes the lexical complexity score based on unique word proportion,\n",
    "            special character proportion, and word length standard deviation.\n",
    "\n",
    "            Args:\n",
    "                content (str): The text content of the review.\n",
    "                stats_unique_word_proportion (float): Proportion of unique words in the content.\n",
    "                stats_special_chars_proportion (float): Proportion of special characters in the content.\n",
    "                stats_word_length_std (float): Standard deviation of word lengths in the content.\n",
    "\n",
    "            Returns:\n",
    "                float: The computed lexical complexity score.\n",
    "            \"\"\"\n",
    "            if len(content) > 2:\n",
    "                lexical_complexity = (\n",
    "                    0.4 * stats_unique_word_proportion\n",
    "                    + 0.3 * stats_special_chars_proportion\n",
    "                    + 0.3 * stats_word_length_std\n",
    "                )\n",
    "                return float(lexical_complexity * self._lexical_complexity_weight)\n",
    "            return 0.0\n",
    "\n",
    "        # Apply UDFs to create new columns\n",
    "        data = data.withColumn(\n",
    "            \"tqm_pos_diversity_score\",\n",
    "            compute_pos_diversity_score(\n",
    "                F.col(self._column),\n",
    "                F.col(\"pos_p_nouns\"),\n",
    "                F.col(\"pos_p_verbs\"),\n",
    "                F.col(\"pos_p_adjectives\"),\n",
    "                F.col(\"pos_p_adverbs\"),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        data = data.withColumn(\n",
    "            \"tqm_pos_density_score\",\n",
    "            compute_pos_density_score(\n",
    "                F.col(self._column),\n",
    "                F.col(\"pos_n_nouns\"),\n",
    "                F.col(\"pos_n_verbs\"),\n",
    "                F.col(\"pos_n_adjectives\"),\n",
    "                F.col(\"pos_n_adverbs\"),\n",
    "                F.col(\"stats_word_count\"),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        data = data.withColumn(\n",
    "            \"tqm_lexical_complexity_score\",\n",
    "            compute_lexical_complexity_score(\n",
    "                F.col(self._column),\n",
    "                F.col(\"stats_unique_word_proportion\"),\n",
    "                F.col(\"stats_special_chars_proportion\"),\n",
    "                F.col(\"stats_word_length_std\"),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Calculate the TQA score as a weighted combination of components\n",
    "        data = data.withColumn(\n",
    "            self._new_column,\n",
    "            F.col(\"tqm_pos_diversity_score\")\n",
    "            + F.col(\"tqm_lexical_complexity_score\")\n",
    "            + F.col(\"tqm_pos_density_score\"),\n",
    "        )\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we assess the coherence of each review. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Perplexity-Based Coherence Score Pipeline\n",
    "The perplexity-based coherence scores are computed in three steps.\n",
    "\n",
    "1. **ComputePerplexityFilters**: Binary indicators for 14 data quality heuristics are added to the dataset. They indicate the presence of key POS tags that are important in sentiment analysis. We will denote the matrix of these indicators $I$, where $I_i{(\\text{review})}=1$ indicates that the review satisfies the $i^{th}$ filter criteria.  \n",
    "\n",
    "2. **ComputePerplexityWeights**: Each of the 14 filters are applied to the dataset, creating 14 subsets. The average perplexity of each subset is computed, and the perplexity weight reflects the degree to which each filter reduces complexity as follows:\n",
    "$$w_i=\\text{max}\\bigg(0,\\frac{PP_{all}-PP_i}{PP_{all}}\\bigg)$$\n",
    "where $PP_{all}$ is average perplexity of the full dataset, and $P_i$ is the average perplexity of the dataset with the $i^{th}$ filter applied.  \n",
    "\n",
    "3. **ComputeCoherenceScores**: Perplexity-based coherence scores are computed as a weighted sum of the indicator matrix $I$ and the weights $w$ created above as:\n",
    "$$\\text{score}_{review}=\\frac{\\displaystyle\\sum_{i=1}^Fw_iI_i(\\text{review})}{\\displaystyle\\sum_{i=1}^Fw_i}$$ \n",
    "\n",
    "Here's the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Perplexity Filters\n",
    "This task creates binary indicators for 14 filters, each selected from over 50+ filters based on consistent perplexity improvements {cite}`sharmaTextQualityBasedPruning2024`.\n",
    "\n",
    "1. **Linguistic Features**: New columns are created to indicate whether the text contains specific parts of speech:\n",
    "   - **Adjective**: `tqf_has_adjective` checks for at least one adjective.\n",
    "   - **Adverb**: `tqf_has_adverb` checks for at least one adverb.\n",
    "   - **Determiner**: `tqf_has_determiner` checks for at least one determiner.\n",
    "   - **Noun**: `tqf_has_noun` checks for at least one noun.\n",
    "   - **Verb**: `tqf_has_verb` checks for at least one verb.\n",
    "\n",
    "2. **Punctuation and Special Character Checks**:\n",
    "   - **Terminal Punctuation**: `tqf_has_terminal_punctuation` checks if the text ends with a period, exclamation mark, or question mark.\n",
    "   - **High Special Characters Ratio**: `tqf_high_special_chars_ratio` is `True` if special characters make up more than 25% of the text.\n",
    "   - **High Punctuation Ratio**: `tqf_high_punctuation_ratio` is `True` if punctuation makes up more than 25% of the text.\n",
    "\n",
    "3. **Content Length and Structure**:\n",
    "   - **Word Count Range**: `tqf_word_count_range` is `True` if the word count is between 4 and 255 words.\n",
    "   - **Stop Word Match**: `tqf_stop_word_match` checks if at least two common stop words are present in the text.\n",
    "   - **First Letter Capitalized**: `tqf_first_letter_cap` indicates if the first letter of the text is uppercase.\n",
    "   - **Not All Caps**: `tqf_no_all_caps` is `True` if the text is not written entirely in uppercase.\n",
    "\n",
    "4. **Word Repetition and Special Character Checks**:\n",
    "   - **High Word Repetition**: `tqf_high_word_repetition` flags text with a word repetition ratio of 20% or higher.\n",
    "   - **No Special Characters**: `tqf_no_special_chars` is `True` if the text contains no special characters beyond standard punctuation.\n",
    "\n",
    "Finally, the code drops unnecessary columns for tokens and POS tags, returning the enriched DataFrame with these new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 604-714 discover/flow/data_prep/tqa/task.py\n",
    "class ComputePerplexityFiltersTask(Task):\n",
    "    \"\"\"\n",
    "    A task to compute Text Quality Assessment (TQA) statistics for reviews in a PySpark DataFrame.\n",
    "\n",
    "    This task generates various boolean flags based on the presence of certain parts of speech, punctuation patterns,\n",
    "    and statistical ratios in the review text. These flags can be used to assess the quality and characteristics\n",
    "    of each review.\n",
    "\n",
    "    Methods:\n",
    "        run(data: DataFrame) -> DataFrame:\n",
    "            Executes the TQA statistics calculations on the specified columns of the input DataFrame and returns the\n",
    "            DataFrame with the new TQA columns.\n",
    "\n",
    "    TQA Filter Columns:\n",
    "        tqf_has_adjective (bool): True if the review has at least one adjective.\n",
    "        tqf_has_adverb (bool): True if the review has at least one adverb.\n",
    "        tqf_has_determiner (bool): True if the review has at least one determiner.\n",
    "        tqf_has_noun (bool): True if the review has at least one noun.\n",
    "        tqf_has_terminal_punctuation (bool): True if the review contains terminal punctuation (., !, or ?).\n",
    "        tqf_has_verb (bool): True if the review has at least one verb.\n",
    "        tqf_high_digit_ratio (bool): True if the ratio of digits to words is greater than 0.25.\n",
    "        tqf_high_punctuation_ratio (bool): True if the ratio of punctuation to words is greater than 0.25.\n",
    "        tqf_word_count_range (bool): True if the word count is between 3 and 256.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    @task_logger\n",
    "    def run(self, data: DataFrame) -> DataFrame:\n",
    "\n",
    "        # 1. Whether review has at least one adjective\n",
    "        data = data.withColumn(\"tqf_has_adjective\", F.col(\"pos_n_adjectives\") > 0)\n",
    "\n",
    "        # 2. Whether review has at least one adverb\n",
    "        data = data.withColumn(\"tqf_has_adverb\", F.col(\"pos_n_adverbs\") > 0)\n",
    "\n",
    "        # 3. Whether review has at least one determiner\n",
    "        data = data.withColumn(\"tqf_has_determiner\", F.col(\"pos_n_determiners\") > 0)\n",
    "\n",
    "        # 4. Whether review has at least one noun\n",
    "        data = data.withColumn(\"tqf_has_noun\", F.col(\"pos_n_nouns\") > 0)\n",
    "\n",
    "        # 5. Whether the review contains terminal punctuation (., !, or ?)\n",
    "        data = data.withColumn(\n",
    "            \"tqf_has_terminal_punctuation\", F.col(\"content\").rlike(\"[.!?]$\")\n",
    "        )\n",
    "\n",
    "        # 6. Whether review has at least one verb\n",
    "        data = data.withColumn(\"tqf_has_verb\", F.col(\"pos_n_verbs\") > 0)\n",
    "\n",
    "        # 7. Whether special characters to words ratio is greater than 0.25\n",
    "        data = data.withColumn(\n",
    "            \"tqf_high_special_chars_ratio\",\n",
    "            F.col(\"stats_special_chars_proportion\") > 0.25,\n",
    "        )\n",
    "\n",
    "        # 8. Whether punctuation to words ratio is greater than 0.25\n",
    "        data = data.withColumn(\n",
    "            \"tqf_high_punctuation_ratio\",\n",
    "            F.col(\"stats_punctuation_proportion\") > 0.25,\n",
    "        )\n",
    "\n",
    "        # 8. Whether word count is in the range > 3 and < 256\n",
    "        data = data.withColumn(\n",
    "            \"tqf_word_count_range\",\n",
    "            (F.col(\"stats_word_count\") > 3) & (F.col(\"stats_word_count\") < 256),\n",
    "        )\n",
    "\n",
    "        # 9. Stop wprd match\n",
    "        # List of stop words to search for\n",
    "        stop_words = [\"the\", \"be\", \"to\", \"of\", \"and\", \"that\", \"have\", \"with\"]\n",
    "\n",
    "        # Create conditions for each stop word\n",
    "        conditions = [\n",
    "            F.expr(f\"array_contains(split(content, ' '), '{word}')\").cast(\"int\")\n",
    "            for word in stop_words\n",
    "        ]\n",
    "\n",
    "        # Sum the conditions and check if at least 2 stop words are present\n",
    "        data = data.withColumn(\n",
    "            \"tqf_stop_word_match\", F.when(sum(conditions) >= 2, True).otherwise(False)\n",
    "        )\n",
    "\n",
    "        # 10. Create a new column \"tqf_first_letter_cap\" based on the first letter being uppercase\n",
    "        data = data.withColumn(\n",
    "            \"tqf_first_letter_cap\", F.expr(\"substring(content, 1, 1) rlike '^[A-Z]'\")\n",
    "        )\n",
    "\n",
    "        # 11. Create a new column \"tqf_no_all_caps\" based on whether the content is all caps\n",
    "        data = data.withColumn(\"tqf_no_all_caps\", ~F.col(\"content\").rlike(\"^[^a-z]*$\"))\n",
    "\n",
    "        # 12. Create a new column \"tqf_high_word_repetition\" if 'stats_word_repetition_ratio' >= 0.2\n",
    "        data = data.withColumn(\n",
    "            \"tqf_high_word_repetition\", F.col(\"stats_word_repetition_ratio\") >= 0.2\n",
    "        )\n",
    "\n",
    "        # Define the regex pattern for special characters (non-alphanumeric, non-punctuation)\n",
    "        special_chars_pattern = r\"[^a-zA-Z0-9\\s.,!?;:'\\\"()\\-]\"\n",
    "\n",
    "        # Set tqf_no_special_chars to True if content has no special characters\n",
    "        data = data.withColumn(\n",
    "            \"tqf_no_special_chars\", ~F.col(\"content\").rlike(special_chars_pattern)\n",
    "        )\n",
    "\n",
    "        # Delete tokens an pos tags from dataset\n",
    "        data = data.drop(\"tp_tokens\", \"tp_pos\")\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute and Save Perplexity Weights\n",
    "This task computes weights based on the average perplexity of text reviews, leveraging a set of predefined filters. Here's how it works:\n",
    "\n",
    "1. **Check for Existing Weights**: The task first checks if the perplexity weights file already exists at the specified file path (`self._pp_filepath`). If the file exists, the task skips the computation.\n",
    "\n",
    "2. **Calculate Overall Average Perplexity**: The code computes the overall average perplexity (`pp_all`) across all reviews in the input DataFrame.\n",
    "\n",
    "3. **Identify and Sort Filters**: It gathers all columns in the DataFrame that start with a specific prefix (`self._pp_filter_prefix`), which correspond to different text quality filters. These filters are then sorted.\n",
    "\n",
    "4. **Compute Weights for Each Filter**:\n",
    "   - For each filter, the code filters the DataFrame to include only reviews where the filter condition is `True`.\n",
    "   - It then calculates the average perplexity for this subset of reviews (`avg_perplexity_i`).\n",
    "   - The weight for each filter is calculated as `(pp_all - avg_perplexity_i) / pp_all`, reflecting how much the filtered subset differs from the overall average perplexity.\n",
    "\n",
    "5. **Save Weights to File**: The computed weights are converted into a Pandas DataFrame, formatted, and saved to a file using `IOService.write()`.\n",
    "\n",
    "This task generates and saves weights that quantify the impact of various text quality filters on review perplexity, providing a basis for coherence assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 718-791 discover/flow/data_prep/tqa/task.py\n",
    "class ComputePerplexityWeights(Task):\n",
    "    \"\"\"\n",
    "    A class to compute and save perplexity weights for various filters in a PySpark DataFrame.\n",
    "\n",
    "    This task calculates the overall average perplexity and the average perplexity for each\n",
    "    filter column that starts with a specified prefix. It then computes weights for each filter\n",
    "    and writes the results to a specified file.\n",
    "\n",
    "    Attributes:\n",
    "        _column (str): The name of the column containing perplexity values.\n",
    "        _pp_filepath (str): The file path to save the perplexity weights.\n",
    "        _pp_filter_prefix (str): The prefix used to identify filter columns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        column: str = \"dqp_perplexity\",\n",
    "        pp_filepath: str = \"models/tqa/pp_weights.csv\",\n",
    "        pp_filter_prefix: str = \"tqf_\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the ComputePerplexityWeights class with the specified parameters.\n",
    "\n",
    "        Args:\n",
    "            column (str): The name of the column containing perplexity values. Defaults to 'dqp_perplexity'.\n",
    "            pp_filepath (str): The file path to save the perplexity weights. Defaults to 'models/tqa/ppl_weights.csv'.\n",
    "            pp_filter_prefix (str): The prefix used to identify filter columns. Defaults to 'tqf_'.\n",
    "        \"\"\"\n",
    "        self._column = column\n",
    "        self._pp_filepath = pp_filepath\n",
    "        self._pp_filter_prefix = pp_filter_prefix\n",
    "\n",
    "    @task_logger\n",
    "    def run(self, data: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Computes and saves perplexity weights for filter columns in the DataFrame.\n",
    "\n",
    "        The method calculates the overall average perplexity and the average perplexity\n",
    "        for each filter column. It then computes a weight for each filter based on the\n",
    "        difference between the overall and individual perplexity values, and saves the\n",
    "        weights to a specified file.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The input PySpark DataFrame containing the perplexity values and filter columns.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: The input DataFrame (unmodified).\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self._pp_filepath):\n",
    "            # Calculate the overall average perplexity\n",
    "            pp_all = data.agg(F.avg(self._column)).first()[0]\n",
    "            w_i = {}\n",
    "\n",
    "            # Get all columns that start with the filter prefix and sort them\n",
    "            filters = [\n",
    "                col for col in data.columns if col.startswith(self._pp_filter_prefix)\n",
    "            ]\n",
    "            filters = sorted(filters)\n",
    "\n",
    "            # Compute the average perplexity and the weights for each filter\n",
    "            for filter in filters:\n",
    "                filtered_data = data.filter(F.col(filter) == True)  # noqa\n",
    "                avg_perplexity_i = filtered_data.agg(F.avg(self._column)).first()[0]\n",
    "                w_i[filter] = (pp_all - avg_perplexity_i) / pp_all\n",
    "\n",
    "            # Convert the weights dictionary to a DataFrame and write to file\n",
    "            weights = pd.DataFrame.from_dict(\n",
    "                w_i, orient=\"index\", columns=[\"weight\"]\n",
    "            ).reset_index(names=\"filter\")\n",
    "            IOService.write(data=weights, filepath=self._pp_filepath)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Perplexity-Based Coherence Score\n",
    "The ComputeCoherenceScoreTask class calculates a coherence score for text data based on weighted indicators. \n",
    "\n",
    "1. **Initialization**: \n",
    "   - The class is initialized with parameters for the output column name (`new_column`) and the file path to the perplexity weights (`pp_filepath`).\n",
    "   - It loads the weights from the specified file, which are used to compute the coherence score.\n",
    "\n",
    "2. **Loading Weights**:\n",
    "   - The weights are read from a file into a Pandas DataFrame and then converted to a PySpark DataFrame for further processing.\n",
    "   - These weights are collected into a list of dictionaries, making them iterable for constructing the weighted sum expression.\n",
    "\n",
    "3. **Score Calculation**:\n",
    "   - The total weight is computed by summing all individual weights.\n",
    "   - A weighted sum expression is constructed by multiplying each filter indicator (converted to double) by its corresponding weight and normalizing by the total weight.\n",
    "   - The coherence score is added to the DataFrame as a new column, specified by `new_column`.\n",
    "\n",
    "4. **Output**: \n",
    "   - The method returns the modified PySpark DataFrame, now enriched with the computed coherence score.\n",
    "\n",
    "This class calculates a coherence score that reflects the overall text quality by leveraging precomputed perplexity weights and binary filter indicators, providing a quantitative measure of coherence for each text entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 795-851 discover/flow/data_prep/tqa/task.py\n",
    "class ComputeCoherenceScoreTask(Task):\n",
    "    def __init__(\n",
    "        self,\n",
    "        new_column: str = \"tqa_coherence_score\",\n",
    "        pp_filepath: str = \"models/tqa/pp_weights.csv\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the TQATask2 with specified parameters and loads weights for computation.\n",
    "\n",
    "        Args:\n",
    "            ppl_full (float): The perplexity value for normalization.\n",
    "            column (str): The name of the column in the DataFrame containing text data. Defaults to \"content\".\n",
    "            new_column (str): The name of the output column for the computed TQA score. Defaults to \"enrichment_tqa_score2\".\n",
    "            ppl_filepath (str): Path to file containing filtered perplexity scores. Defaults to \"models/tqa/tqa_ppl.csv\".\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._new_column = new_column\n",
    "        self._pp_filepath = pp_filepath\n",
    "\n",
    "    @task_logger\n",
    "    def run(self, data: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Computes the TQA score based on the binary indicators and the assigned weights.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The input PySpark DataFrame containing binary indicators for filters.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: The input DataFrame with an additional column for the computed TQA score.\n",
    "        \"\"\"\n",
    "        # Load perplexity weights\n",
    "        self._weights_pandas = IOService.read(self._pp_filepath).reset_index(drop=True)\n",
    "        # Convert to spark DataFrame\n",
    "        weights_spark = ps.DataFrame(self._weights_pandas).to_spark()\n",
    "\n",
    "        # Collect weights into a list of dictionaries\n",
    "        weights_list = weights_spark.collect()\n",
    "\n",
    "        # Compute the total sum of the weights\n",
    "        total_weight = sum(item[\"weight\"] for item in weights_list)\n",
    "\n",
    "        # Compute the weighted sum of filter indicators\n",
    "        filter_sum_expr = sum(\n",
    "            [\n",
    "                F.col(item[\"filter\"]).cast(\"double\") * F.lit(item[\"weight\"])\n",
    "                for item in weights_list\n",
    "            ]\n",
    "        ) / F.lit(\n",
    "            total_weight\n",
    "        )  # Normalize by the total sum of the weights\n",
    "\n",
    "        # Add the computed TQA score as a new column\n",
    "        data = data.withColumn(self._new_column, filter_sum_expr)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Text Quality Score\n",
    "Finally, we compute the text quality score as a composite of the **Syntactic/Lexical Complexity Score** and the **Perplexity-Based Coherence Score**. We first, scale each score to the range [0,1] using minmax scaling. Then the final score is weighted sum of both scaled scores as follows:\n",
    "\n",
    "$$\\text{score}_{review}=w_1\\times{\\text{SLC}}_{review} + w_2\\times{\\text{PBC}}_{review}$$ \n",
    "where $\\text{SLC}_{review}$ is the **Syntactic/Lexical Complexity Score** and $\\text{PBC}_{review}$ is the **Perplexity-Based Coherence Score**. Weights $w_1$ and $w_2$ are the weights for the **Syntactic/Lexical Complexity Score** and the **Perplexity-Based Coherence Score**, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 855-944 discover/flow/data_prep/tqa/task.py\n",
    "class ComputeTextQualityScore(Task):\n",
    "    \"\"\"\n",
    "    A task to compute a final Text Quality Assessment (TQA) score by normalizing and combining two TQA scores\n",
    "    using specified weights.\n",
    "\n",
    "    Attributes:\n",
    "        new_column (str): Column containing the final text quality score.\n",
    "        tqa_syntactic_weight (float): The weight assigned to the first TQA score. Defaults to 0.4.\n",
    "        tqa_perplexity_weight (float): The weight assigned to the second TQA score. Defaults to 0.6.\n",
    "        _data (DataFrame): The DataFrame holding the data after computation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        new_column: str = \"tqa_score\",\n",
    "        tqa_syntactic_weight: float = 0.4,\n",
    "        tqa_perplexity_weight: float = 0.6,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the ComputeTextQualityScore with specified weights for combining the two TQA scores.\n",
    "\n",
    "        Args:\n",
    "            tqa_syntactic_weight (float): Weight for the first TQA score. Defaults to 0.4.\n",
    "            tqa_perplexity_weight (float): Weight for the second TQA score. Defaults to 0.6.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._new_column = new_column\n",
    "        self._tqa_syntactic_weight = tqa_syntactic_weight\n",
    "        self._tqa_perplexity_weight = tqa_perplexity_weight\n",
    "        self._data = None\n",
    "\n",
    "    @task_logger\n",
    "    def run(self, data: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Normalizes two TQA scores to the range [0, 1] and computes a final TQA score\n",
    "        using the specified weights.\n",
    "\n",
    "        Args:\n",
    "            data (DataFrame): The input PySpark DataFrame containing \"enrichment_tqa_score1\" and \"enrichment_tqa_score2\" columns.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: The PySpark DataFrame with normalized scores and the final combined TQA score.\n",
    "        \"\"\"\n",
    "        # Normalize both scores to [0, 1]\n",
    "        min_max_enrichment_tqa_score1 = data.select(\n",
    "            F.min(\"tqa_syntactic_lexical_score\"), F.max(\"tqa_syntactic_lexical_score\")\n",
    "        ).first()\n",
    "        min_enrichment_tqa_score1, max_enrichment_tqa_score1 = (\n",
    "            min_max_enrichment_tqa_score1\n",
    "        )\n",
    "\n",
    "        min_max_enrichment_tqa_score2 = data.select(\n",
    "            F.min(\"tqa_coherence_score\"), F.max(\"tqa_coherence_score\")\n",
    "        ).first()\n",
    "        min_enrichment_tqa_score2, max_enrichment_tqa_score2 = (\n",
    "            min_max_enrichment_tqa_score2\n",
    "        )\n",
    "\n",
    "        data = data.withColumn(\n",
    "            \"tqa_syntactic_lexical_score\",\n",
    "            (F.col(\"tqa_syntactic_lexical_score\") - min_enrichment_tqa_score1)\n",
    "            / (max_enrichment_tqa_score1 - min_enrichment_tqa_score1),\n",
    "        )\n",
    "\n",
    "        data = data.withColumn(\n",
    "            \"tqa_coherence_score\",\n",
    "            (F.col(\"tqa_coherence_score\") - min_enrichment_tqa_score2)\n",
    "            / (max_enrichment_tqa_score2 - min_enrichment_tqa_score2),\n",
    "        )\n",
    "\n",
    "        # Combine scores using weights\n",
    "        data = data.withColumn(\n",
    "            self._new_column,\n",
    "            self._tqa_syntactic_weight * F.col(\"tqa_syntactic_lexical_score\")\n",
    "            + self._tqa_perplexity_weight * F.col(\"tqa_coherence_score\"),\n",
    "        )\n",
    "\n",
    "        # Compute min and max of the new column\n",
    "        min_value = data.agg(F.min(self._new_column)).collect()[0][0]\n",
    "        max_value = data.agg(F.max(self._new_column)).collect()[0][0]\n",
    "\n",
    "        # Apply Min-Max transformation\n",
    "        data = data.withColumn(\n",
    "            self._new_column,\n",
    "            (F.col(self._new_column) - min_value) / (max_value - min_value),\n",
    "        )\n",
    "        self._data = data\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Text Quality Analysis (TQA) Pipeline\n",
    "### Pipeline Configuration\n",
    "The configuration for the `tqa` pipeline has four principal compoents:\n",
    "1. **`stage_name`**: Specifies the name of the stage for the stage builder.\n",
    "2. **`source_config`**: Specifies the stage input deataset configuration.\n",
    "3. **`destination_config`**: Defines the configuration for the output dataset.\n",
    "4. **`tasks`**: A list of tasks (defined above) and their arguments.\n",
    "\n",
    "We'll begin by defining the input and output dataset configurations. The input dataset, `review` was created in `tqd` stage of the `dataprep` phase. Since we are performing NLP operations on a distributed (PySpark), we set `nlp` and `distributed` to True. This will ensure that the Spark session contains the SparkNLP packages required."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# %load -r 199-214 config/base/flow.yaml\n",
    "tqa:\n",
    "  stage_name: tqa\n",
    "  source_config:\n",
    "    asset_type: dataset\n",
    "    phase: dataprep\n",
    "    stage: tqd\n",
    "    name: review\n",
    "    nlp: True\n",
    "    distributed: True\n",
    "  destination_config:\n",
    "    asset_type: dataset\n",
    "    phase: dataprep\n",
    "    stage: tqa\n",
    "    name: review\n",
    "    nlp: True\n",
    "    distributed: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we add the tasks and their parameters to the pipeline configuration."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# %load -r 199-255 config/base/flow.yaml\n",
    "tqa:\n",
    "  stage_name: tqa\n",
    "  source_config:\n",
    "    asset_type: dataset\n",
    "    phase: dataprep\n",
    "    stage: tqd\n",
    "    name: review\n",
    "    nlp: True\n",
    "    distributed: True\n",
    "  destination_config:\n",
    "    asset_type: dataset\n",
    "    phase: dataprep\n",
    "    stage: tqa\n",
    "    name: review\n",
    "    nlp: True\n",
    "    distributed: True\n",
    "  tasks:\n",
    "  - class_name: NLPTask\n",
    "    module: discover.flow.data_prep.tqa.task\n",
    "    params:\n",
    "      column: content\n",
    "  - class_name: ComputeSyntacticStatsTask\n",
    "    module: discover.flow.data_prep.tqa.task\n",
    "    params:\n",
    "      column: content\n",
    "  - class_name: ComputeLexicalStatsTask\n",
    "    module: discover.flow.data_prep.tqa.task\n",
    "    params:\n",
    "      column: content\n",
    "  - class_name: ComputeSyntacticLexicalScoresTask\n",
    "    module: discover.flow.data_prep.tqa.task\n",
    "    params:\n",
    "      column: content\n",
    "      new_column: tqa_syntactic_lexical_score\n",
    "      pos_diversity_weight: 0.3\n",
    "      pos_density_weight: 0.3\n",
    "      lexical_complexity_weight: 0.4\n",
    "  - class_name: ComputePerplexityFiltersTask\n",
    "    module: discover.flow.data_prep.tqa.task\n",
    "    params: {}\n",
    "  - class_name: ComputePerplexityWeights\n",
    "    module: discover.flow.data_prep.tqa.task\n",
    "    params:\n",
    "      pp_filepath: models/tqa/pp_weights.csv\n",
    "      column: dqp_perplexity\n",
    "      pp_filter_prefix: tqf_\n",
    "  - class_name: ComputeCoherenceScoreTask\n",
    "    module: discover.flow.data_prep.tqa.task\n",
    "    params:\n",
    "      new_column: tqa_coherence_score\n",
    "      pp_filepath: models/tqa/pp_weights.csv\n",
    "  - class_name: ComputeTextQualityScore\n",
    "    module: discover.flow.data_prep.tqa.task\n",
    "    params:\n",
    "      tqa_syntactic_weight: 0.4\n",
    "      tqa_perplexity_weight: 0.6\n",
    "      new_column: tqa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we've configured the eight tasks which we've defined above in our `tqa` configuration. Each specifies the `class_name`, the path for the `module` containing the class, and `params`, a dictionary containing the task's arguments. These arguments include most notably, the weights for the `ComputeSyntacticLexicalScoresTask` and `ComputeTextQualityScore` tasks.  Now that the **Text Quality Analysis Pipeline** is fully specified, we can now build the `DataPrepStage` object that encapsulates the pipeline tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage\n",
    "A Stage object orchestrates the task execution of a pipeline. Exposed in the `Stage` base class below, is the `build` method which takes a stage configuration and constructs a Stage object.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 21-73 discover/flow/base/stage.py\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "\n",
    "from discover.core.flow import PhaseDef, StageDef\n",
    "from discover.core.namespace import NestedNamespace\n",
    "from discover.flow.base.task import Task, TaskBuilder\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "#                                        STAGE                                                     #\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "class Stage(ABC):\n",
    "    \"\"\"Abstract base class for Stage pipelines.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_config: dict,\n",
    "        destination_config: dict,\n",
    "        tasks: List[Task],\n",
    "        force: bool = False,\n",
    "    ) -> None:\n",
    "        self._source_config = NestedNamespace(source_config)\n",
    "        self._destination_config = NestedNamespace(destination_config)\n",
    "        self._tasks = tasks\n",
    "        self._force = force\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def phase(self) -> PhaseDef:\n",
    "        \"\"\"Phase\"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def stage(self) -> StageDef:\n",
    "        \"\"\"Stage\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def run(self) -> str:\n",
    "        \"\"\"Stage execution\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def build(cls, stage_config: dict, force: bool = False) -> Stage:\n",
    "        tasks = [\n",
    "            TaskBuilder.build(task_config) for task_config in stage_config[\"tasks\"]\n",
    "        ]\n",
    "        return cls(\n",
    "            source_config=stage_config[\"source_config\"],\n",
    "            destination_config=stage_config[\"destination_config\"],\n",
    "            tasks=tasks,\n",
    "            force=force,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The build method invokes the TaskBuilder defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 130-200 discover/flow/base/task.py\n",
    "class TaskBuilder:\n",
    "    \"\"\"\n",
    "    A builder class for constructing task instances from configuration data.\n",
    "\n",
    "    The `TaskBuilder` class provides a static method, `build`, which reads task configuration\n",
    "    data and dynamically creates an instance of the specified task class using the provided\n",
    "    parameters. This allows for flexible and dynamic task instantiation based on configuration.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    build(task_config: dict) -> object\n",
    "        Constructs and returns an instance of a task class using the specified configuration.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> task_config = {\n",
    "    ...     'module_name': 'mypackage.mymodule',\n",
    "    ...     'class_name': 'NormalizationTask',\n",
    "    ...     'params': {'param1': value1, 'param2': value2}\n",
    "    ... }\n",
    "    >>> task_instance = TaskBuilder.build(task_config)\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def build(task_config):\n",
    "        \"\"\"\n",
    "        Builds and returns an instance of a task class based on the provided configuration.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        task_config : dict\n",
    "            A dictionary containing task configuration with the following keys:\n",
    "                - 'phase' (str): The phase associated with the task (e.g., 'preprocessing').\n",
    "                - 'stage' (str): The stage within the phase (e.g., 'normalization').\n",
    "                - 'module_name' (str): The name of the module containing the task class.\n",
    "                - 'class_name' (str): The name of the task class to instantiate.\n",
    "                - 'params' (dict): Additional parameters to pass to the task's constructor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        object\n",
    "            An instance of the specified task class initialized with the provided parameters.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ModuleNotFoundError\n",
    "            If the specified module cannot be found.\n",
    "        AttributeError\n",
    "            If the specified class does not exist in the module.\n",
    "        TypeError\n",
    "            If the class constructor does not accept the provided parameters.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> task_config = {\n",
    "        ...     'module_name': 'mypackage.mymodule',\n",
    "        ...     'class_name': 'NormalizationTask',\n",
    "        ...     'params': {'param1': value1, 'param2': value2}\n",
    "        ... }\n",
    "        >>> task_instance = TaskBuilder.build(task_config)\n",
    "        \"\"\"\n",
    "        module = task_config[\"module\"]\n",
    "        class_name = task_config[\"class_name\"]\n",
    "        params = task_config[\"params\"]\n",
    "        return instantiate_class(\n",
    "            module=module,\n",
    "            class_name=class_name,\n",
    "            params=params,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Stage` base class exposes the `build` method, which is responsible for dynamically creating the Stage object its tasks based on a configuration. Here's a breakdown of its functionality:\n",
    "\n",
    "1. **Extract Configuration Details**: \n",
    "   - `module = task_config[\"module\"]`: Retrieves the module name from the `task_config` dictionary.\n",
    "   - `class_name = task_config[\"class_name\"]`: Retrieves the class name from the `task_config` dictionary.\n",
    "   - `params = task_config[\"params\"]`: Retrieves the parameters for the class instantiation from the `task_config` dictionary.\n",
    "\n",
    "2. **Instantiate the Task Class**: \n",
    "   - The `instantiate_class` function is called with the extracted `module`, `class_name`, and `params`. This function dynamically imports the specified module, retrieves the class, and creates an instance of it using the provided parameters.\n",
    "\n",
    "This base class allows for flexible and extensible workflows, where new tasks can be added or modified simply by updating the configuration, without changing the underlying code. It is particularly useful in building configurable data pipelines or frameworks that support various types of tasks.\n",
    "\n",
    "Next, we examine the `instantiate_class`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 29-68 discover/flow/base/task.py\n",
    "def instantiate_class(module: str, class_name: str, params: dict):\n",
    "    \"\"\"\n",
    "    Dynamically imports a module and instantiates a class with the given parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    module : str\n",
    "        The name of the module from which to import the class (e.g., 'mypackage.mymodule').\n",
    "    class_name : str\n",
    "        The name of the class to instantiate from the module.\n",
    "    params : dict\n",
    "        A dictionary of additional parameters to pass to the class constructor.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    object\n",
    "        An instance of the specified class with the provided parameters.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ModuleNotFoundError\n",
    "        If the specified module cannot be found.\n",
    "    AttributeError\n",
    "        If the specified class does not exist in the module.\n",
    "    TypeError\n",
    "        If the class constructor does not accept the provided parameters.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> obj = instantiate_class(\n",
    "    ...     module='mypackage.mymodule',\n",
    "    ...     class_name='Normalizer',\n",
    "    ...     params={'param1': value1, 'param2': value2}\n",
    "    ... )\n",
    "    \"\"\"\n",
    "    module = importlib.import_module(module)\n",
    "    cls = getattr(module, class_name)\n",
    "    return cls(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `instantiate_class` dynamically imports and initializes a class using the `importlib` library. Here's a breakdown of what it does:\n",
    "\n",
    "1. **Dynamic Module Import**: \n",
    "   - `importlib.import_module(module)` is used to dynamically import a module by its name, which is passed as a string to the `module` variable. This allows for the import of modules at runtime rather than at the beginning of the script.\n",
    "\n",
    "2. **Retrieve Class Reference**: \n",
    "   - `getattr(module, class_name)` fetches the class reference from the imported module using the `class_name` string. This retrieves the class definition, allowing the code to reference the class dynamically.\n",
    "\n",
    "3. **Instantiate Class**: \n",
    "   - `cls(**params)` creates an instance of the class, passing `params` as keyword arguments to the class's constructor. This allows for flexible instantiation of classes with varying parameters.\n",
    "\n",
    "With this function, we are able to load and instantiate classes dynamically based on runtime information, such as configurations or external inputs.\n",
    "\n",
    "Inheriting from the `Stage` class, the `DataPrepStage` defines orchestration for the data preparation pipelines. Let's take a look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation Stage\n",
    "The `run` method begins by checking whether the endpoint already exists and evaluating the `force` parameter to decide the appropriate execution path. If the endpoint exists and the `force` parameter is not set, the method simply returns the `asset_id`. However, if execution is forced or the endpoint does not exist, the method proceeds to remove any existing dataset associated with the endpoint. It then sequentially executes each task in the list, using the output of one task as the input for the next. Once all tasks have been executed, the final processed dataset is saved to the repository according to the destination dataset configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 22-204 discover/flow/data_prep/stage.py\n",
    "from typing import List, Union\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from dependency_injector.wiring import Provide, inject\n",
    "\n",
    "from discover.assets.dataset import Dataset\n",
    "from discover.assets.idgen import AssetIDGen\n",
    "from discover.container import DiscoverContainer\n",
    "from discover.core.flow import DataPrepStageDef, PhaseDef\n",
    "from discover.flow.base.stage import Stage\n",
    "from discover.flow.base.task import Task\n",
    "from discover.infra.persistence.repo.dataset import DatasetRepo\n",
    "from discover.infra.service.logging.stage import stage_logger\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "#                                    DATA PREP STAGE                                               #\n",
    "# ------------------------------------------------------------------------------------------------ #\n",
    "class DataPrepStage(Stage):\n",
    "    \"\"\"\n",
    "    A stage class for preparing datasets, handling loading, processing, and saving of data.\n",
    "\n",
    "    The `DataPrepStage` class orchestrates the execution of data preparation tasks,\n",
    "    including loading source datasets, applying a series of tasks, and saving the processed\n",
    "    data to a destination. It uses a repository for dataset persistence and can be configured\n",
    "    to force execution even if the destination dataset already exists.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source_config : dict\n",
    "        Configuration for the source dataset, including details like phase, stage, and name.\n",
    "    destination_config : dict\n",
    "        Configuration for the destination dataset, including details like phase, stage, and name.\n",
    "    tasks : List[Task]\n",
    "        A list of tasks to execute as part of the data preparation stage.\n",
    "    force : bool, optional\n",
    "        Whether to force execution if the destination dataset endpoint already exists (default is False).\n",
    "    repo : DatasetRepo, optional\n",
    "        A repository for dataset persistence, injected via dependency injection (default is `DiscoverContainer.repo.dataset_repo`).\n",
    "    **kwargs : dict\n",
    "        Additional keyword arguments for stage configuration.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    _repo : DatasetRepo\n",
    "        The repository instance used for dataset persistence.\n",
    "    _source_asset_id : str\n",
    "        The generated asset ID for the source dataset based on the configuration.\n",
    "    _destination_asset_id : str\n",
    "        The generated asset ID for the destination dataset based on the configuration.\n",
    "    _logger : logging.Logger\n",
    "        Logger instance for logging events related to the data preparation stage.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    run() -> None\n",
    "        Executes the stage by loading the source dataset, applying tasks, and saving the result.\n",
    "    _create_destination_dataset(data: Union[pd.DataFrame, pyspark.sql.DataFrame]) -> Dataset\n",
    "        Creates the destination dataset with the processed data and configuration details.\n",
    "    _load_source_dataset() -> Dataset\n",
    "        Loads the source dataset from the repository using the source asset ID.\n",
    "    _save_destination_dataset(dataset: Dataset) -> None\n",
    "        Saves the processed dataset to the repository using the destination asset ID.\n",
    "    _endpoint_exists(asset_id: str) -> bool\n",
    "        Checks if the dataset endpoint already exists in the repository.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The `DataPrepStage` class leverages dependency injection to retrieve a dataset repository instance.\n",
    "    It ensures that datasets are properly loaded and saved based on the specified configurations.\n",
    "    \"\"\"\n",
    "\n",
    "    @inject\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_config: dict,\n",
    "        destination_config: dict,\n",
    "        tasks: List[Task],\n",
    "        force: bool = False,\n",
    "        repo: DatasetRepo = Provide[DiscoverContainer.repo.dataset_repo],\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            source_config=source_config,\n",
    "            destination_config=destination_config,\n",
    "            tasks=tasks,\n",
    "            force=force,\n",
    "        )\n",
    "        self._repo = repo\n",
    "\n",
    "        self._destination_asset_id = AssetIDGen.get_asset_id(\n",
    "            asset_type=self._destination_config.asset_type,\n",
    "            phase=PhaseDef.from_value(value=self._destination_config.phase),\n",
    "            stage=DataPrepStageDef.from_value(value=self._destination_config.stage),\n",
    "            name=self._destination_config.name,\n",
    "        )\n",
    "\n",
    "        self._logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n",
    "\n",
    "    @property\n",
    "    def phase(self) -> PhaseDef:\n",
    "        return PhaseDef.from_value(value=self._destination_config.phase)\n",
    "\n",
    "    @property\n",
    "    def stage(self) -> PhaseDef:\n",
    "        return DataPrepStageDef.from_value(value=self._destination_config.stage)\n",
    "\n",
    "    @stage_logger\n",
    "    def run(self) -> str:\n",
    "        \"\"\"Executes the stage by loading the source dataset, applying tasks, and saving the result.\n",
    "\n",
    "        Returns:\n",
    "            asset_id (str): Returns the asset_id for the asset created.\n",
    "        \"\"\"\n",
    "        if (\n",
    "            self._endpoint_exists(asset_id=self._destination_asset_id)\n",
    "            and not self._force\n",
    "        ):\n",
    "            return self._destination_asset_id\n",
    "        else:\n",
    "            if self._repo.exists(asset_id=self._destination_asset_id):\n",
    "                self._repo.remove(asset_id=self._destination_asset_id)\n",
    "\n",
    "            data = self._load_source_data()\n",
    "\n",
    "            for task in self._tasks:\n",
    "                data = task.run(data=data)\n",
    "\n",
    "            dataset = self._create_destination_dataset(data=data)\n",
    "\n",
    "            self._save_destination_dataset(dataset=dataset)\n",
    "\n",
    "            return self._destination_asset_id\n",
    "\n",
    "    def _endpoint_exists(self, asset_id: str) -> bool:\n",
    "        \"\"\"Checks if the dataset endpoint already exists in the repository.\"\"\"\n",
    "        return self._repo.exists(asset_id=asset_id)\n",
    "\n",
    "    def _load_source_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Loads the source dataset from the repository using the source asset ID.\"\"\"\n",
    "        source_asset_id = AssetIDGen.get_asset_id(\n",
    "            asset_type=self._source_config.asset_type,\n",
    "            phase=PhaseDef.from_value(value=self._source_config.phase),\n",
    "            stage=DataPrepStageDef.from_value(value=self._source_config.stage),\n",
    "            name=self._source_config.name,\n",
    "        )\n",
    "        dataset = self._repo.get(\n",
    "            asset_id=source_asset_id,\n",
    "            distributed=self._source_config.distributed,\n",
    "            nlp=self._source_config.nlp,\n",
    "        )\n",
    "\n",
    "        if self._source_config.distributed:\n",
    "            # Rename the pandas index column if it exists\n",
    "            if \"__index_level_0__\" in dataset.content.columns:\n",
    "                dataset.content = dataset.content.withColumnRenamed(\n",
    "                    \"__index_level_0__\", \"pandas_index\"\n",
    "                )\n",
    "        return dataset.content\n",
    "\n",
    "    def _create_destination_dataset(\n",
    "        self, data: Union[pd.DataFrame, pyspark.sql.DataFrame]\n",
    "    ) -> Dataset:\n",
    "        \"\"\"Creates the destination dataset with the processed data and configuration details.\"\"\"\n",
    "        return Dataset(\n",
    "            phase=PhaseDef.from_value(self._destination_config.phase),\n",
    "            stage=DataPrepStageDef.from_value(self._destination_config.stage),\n",
    "            name=self._destination_config.name,\n",
    "            content=data,\n",
    "            nlp=self._destination_config.nlp,\n",
    "            distributed=self._destination_config.distributed,\n",
    "        )\n",
    "\n",
    "    def _remove_destination_dataset(self) -> None:\n",
    "        \"\"\"Removes the destination dataset from the repository.\"\"\"\n",
    "        self._repo.remove(asset_id=self._destination_asset_id)\n",
    "\n",
    "    def _save_destination_dataset(self, dataset: Dataset) -> None:\n",
    "        \"\"\"Saves the processed dataset to the repository using the destination asset ID.\"\"\"\n",
    "        self._repo.add(dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we defined the tasks comprising the **Text Quality Analysis Pipeline** and constructed the pipeline stage responsible for its orchestration. Starting with its configuration, we constructed a Stage object, which invoked a Task builder that dynamically instantiated the Task classes based upon configuration and added them to the `DataPrepStage` object. The run method will execute the pipeline and persist the results in the dataset repository. \n",
    "\n",
    "This process is encapsulated in the following code cell, which will now run the **Text Quality Analysis Pipeline**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the configuration\n",
    "reader = FlowConfigReader()\n",
    "stage_config = reader.get_stage_config(\n",
    "    phase=PhaseDef.DATAPREP, stage=DataPrepStageDef.TQA\n",
    ")\n",
    "\n",
    "# Build and run Data Ingestion Stage\n",
    "stage = TQAStage.build(stage_config=stage_config, force=FORCE)\n",
    "asset_id = stage.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Transition to the Data Quality Analysis (DQA):\n",
    "With the **Text Quality Analysis (TQA) Pipeline** now complete, we have the linguistic elements that contribute to a holistic assessment of text quality for NLP applications. These enriched text quality measures are determinative inputs for our next stage: the **Data Quality Analysis (DQA)**. \n",
    "\n",
    "In the DQA, we’ll dilate our aperture, integrating sentiments, typographical, and linguistic metrics across several dimensions of data quality, allowing us to uncover areas of concern, and devise further data processing interventions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appvocai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
