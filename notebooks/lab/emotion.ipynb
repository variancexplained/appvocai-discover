{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from discover.infra.utils.file.io import IOService\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from discover.infra.service.data.generator import DataBatchGenerator\n",
    "\n",
    "pandarallel.initialize(nb_workers=18, progress_bar=False, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"SamLowe/roberta-base-go_emotions-onnx\"\n",
    "file_name = \"onnx/model_quantized.onnx\"\n",
    "fp = \"workspace/dev/dataset/01_dataprep/appvocai_discover-01_dataprep-03_tqa-review-dataset.parquet\"\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The ONNX file onnx/model_quantized.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n"
     ]
    }
   ],
   "source": [
    "model = ORTModelForSequenceClassification.from_pretrained(model_id, file_name=file_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = IOService.read(fp)\n",
    "gen = DataBatchGenerator(data=df, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapped Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruncatingTokenizer:\n",
    "    def __init__(self, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, text, *args, **kwargs):\n",
    "        # Force truncation, padding, and max_length\n",
    "        return self.tokenizer(\n",
    "            text,\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "wrapped_tokenizer = TruncatingTokenizer(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the pipeline outside the loop\n",
    "tokenizer_kwargs = {\"padding\": True, \"truncation\": True, \"max_length\": 512}\n",
    "\n",
    "onnx_classifier = pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=wrapped_tokenizer,\n",
    "    top_k=3,\n",
    "    function_to_apply=\"sigmoid\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_emotion(text):\n",
    "    return onnx_classifier([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize list to store all batch results\u001b[39;00m\n\u001b[1;32m      2\u001b[0m all_results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_batches\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Parallel apply to detect emotion in the batch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     batch \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mparallel_apply(detect_emotion)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Collect processed batch\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "# Initialize list to store all batch results\n",
    "all_results = []\n",
    "\n",
    "for batch in tqdm(gen, total=gen.n_batches):\n",
    "    # Parallel apply to detect emotion in the batch\n",
    "    batch = batch[\"content\"].parallel_apply(detect_emotion)\n",
    "\n",
    "    # Collect processed batch\n",
    "    all_results.append(batch)\n",
    "\n",
    "# Concatenate all results and assign to the original DataFrame\n",
    "df[\"emotion\"] = pd.concat(all_results, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appvocai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
