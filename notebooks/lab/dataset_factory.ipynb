{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discover.asset.dataset.builder import DatasetBuilder, DatasetPassportBuilder\n",
    "from discover.setup import auto_wire_container\n",
    "from discover.core.dtypes import DFType\n",
    "from discover.infra.utils.file.fileset import FileFormat\n",
    "from discover.asset.dataset.config import DatasetConfigfig\n",
    "from discover.core.flow import PhaseDef, StageDef\n",
    "\n",
    "# Wire container\n",
    "container = auto_wire_container()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = \"tests/data/dirty_reviews.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = container.io.repo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Obtain a spark session from the spark session pool\n",
    "spark_session_pool = container.spark.session_pool()\n",
    "spark = spark_session_pool.spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Configuration\n",
    "Set the configuration for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_config = DatasetConfig(\n",
    "    phase=PhaseDef.DATAPREP,\n",
    "    stage=StageDef.RAW,\n",
    "    name=\"dirty_reviews\",\n",
    "    file_format=FileFormat.PARQUET,\n",
    "    filepath=FP,\n",
    "    asset_type=\"dataset\",\n",
    "    dftype=DFType.SPARK,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Passport Builder\n",
    "Construct the dataset passport, which will contain the dataset metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "passport = (\n",
    "    DatasetPassportBuilder()\n",
    "    .phase(PhaseDef.DATAPREP)\n",
    "    .stage(StageDef.RAW)\n",
    "    .name(\"dirty_reviews\")\n",
    "    .build()\n",
    "    .passport\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Builder\n",
    "Construct the dataset from the configuration and passport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                            AppVoCAI Dataset Summary                            \n",
      "                       Number of Reviews | 50\n",
      "                     Number of Reviewers | 48\n",
      "              Number of Repeat Reviewers | 1 (2.1%)\n",
      "         Number of Influential Reviewers | 3 (6.2%)\n",
      "                          Number of Apps | 30\n",
      "                 Average Reviews per App | 1.7\n",
      "                    Number of Categories | 16\n",
      "                       Min Review Length | 2\n",
      "                       Max Review Length | 439\n",
      "                   Average Review Length | 122.76\n",
      "                    Date of First Review | 1/16/2021 0:02\n",
      "                     Date of Last Review | 9/9/2020 18:27\n"
     ]
    }
   ],
   "source": [
    "dataset = (\n",
    "    DatasetBuilder()\n",
    "    .from_file(ds_config)\n",
    "    .passport(passport)\n",
    "    .to_parquet()\n",
    "    .spark(spark)\n",
    "    .build()\n",
    "    .dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete from Repo if Exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if repo.exists(asset_id=dataset.asset_id):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                            AppVoCAI Dataset Summary                            \n",
      "                       Number of Reviews | 50\n",
      "                     Number of Reviewers | 48\n",
      "              Number of Repeat Reviewers | 1 (2.1%)\n",
      "         Number of Influential Reviewers | 3 (6.2%)\n",
      "                          Number of Apps | 30\n",
      "                 Average Reviews per App | 1.7\n",
      "                    Number of Categories | 16\n",
      "                       Min Review Length | 2\n",
      "                       Max Review Length | 439\n",
      "                   Average Review Length | 122.76\n",
      "                    Date of First Review | 1/16/2021 0:02\n",
      "                     Date of Last Review | 9/9/2020 18:27\n",
      "\n",
      "\n",
      "                        DatasetPassport                         \n",
      "                        asset_id | dataset_dataprep_raw_dirty_reviews_v0.0.4\n",
      "                      asset_type | Dataset\n",
      "                           phase | Data Preparation Phase\n",
      "                           stage | Raw Data Stage\n",
      "                            name | dirty_reviews\n",
      "                     description | Dataset dirty_reviews created in the Data Preparation Phase - Raw Data Stage on 2025-01-21 at 02:02:59\n",
      "                         creator | User (not identified)\n",
      "                         created | 2025-01-21T02:02:59.465176\n",
      "                         version | v0.0.4\n",
      "                          source | None\n",
      "                          parent | None\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.summary\n",
    "print(dataset.passport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[01/21/2025 02:26:22 AM] [ERROR] [discover.infra.persist.file.fao.FAO] [read] : Unable to read spark dataframe. Spark session is None. When reading spark dataframes a spark session must be provided.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unable to read spark dataframe. Spark session is None. When reading spark dataframes a spark session must be provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m repo \u001b[38;5;241m=\u001b[39m container\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mrepo()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m## repo.add(asset=dataset, dftype=DFType.SPARK)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m ds2 \u001b[38;5;241m=\u001b[39m \u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43masset_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masset_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdftype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDFType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSPARK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dataset \u001b[38;5;241m==\u001b[39m ds2\n",
      "File \u001b[0;32m~/projects/appvocai-discover/discover/infra/persist/repo/dataset.py:117\u001b[0m, in \u001b[0;36mDatasetRepo.get\u001b[0;34m(self, asset_id, spark, dftype)\u001b[0m\n\u001b[1;32m    114\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dao\u001b[38;5;241m.\u001b[39mread(asset_id\u001b[38;5;241m=\u001b[39masset_id)\n\u001b[1;32m    115\u001b[0m dftype \u001b[38;5;241m=\u001b[39m dftype \u001b[38;5;129;01mor\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mdftype\n\u001b[0;32m--> 117\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdftype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdftype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28msetattr\u001b[39m(dataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_dataframe\u001b[39m\u001b[38;5;124m\"\u001b[39m, df)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/projects/appvocai-discover/discover/infra/persist/repo/dataset.py:153\u001b[0m, in \u001b[0;36mDatasetRepo._get_data\u001b[0;34m(self, filepath, dftype, spark)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_data\u001b[39m(\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    134\u001b[0m     filepath: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    135\u001b[0m     dftype: DFType,\n\u001b[1;32m    136\u001b[0m     spark: Optional[SparkSession] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    137\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    138\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieves a file from the repository using the specified data structure and format.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m        ValueError: If an unsupported data structure is specified.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fao\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdftype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdftype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/appvocai-discover/discover/infra/persist/file/fao.py:150\u001b[0m, in \u001b[0;36mFAO.read\u001b[0;34m(self, filepath, dftype, file_format, spark)\u001b[0m\n\u001b[1;32m    148\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to read spark dataframe. Spark session is None. When reading spark dataframes a spark session must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39merror(msg)\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reader\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    152\u001b[0m     filepath\u001b[38;5;241m=\u001b[39mfilepath,\n\u001b[1;32m    153\u001b[0m     spark\u001b[38;5;241m=\u001b[39mspark,\n\u001b[1;32m    154\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to read spark dataframe. Spark session is None. When reading spark dataframes a spark session must be provided."
     ]
    }
   ],
   "source": [
    "## repo.add(asset=dataset, dftype=DFType.SPARK)\n",
    "ds2 = repo.get(asset_id=dataset.asset_id, dftype=DFType.SPARK)\n",
    "assert dataset == ds2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appvocai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
