{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABSA Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Orkhan/llama-2-7b-absa\"\n",
    "# load model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output(result, user_prompt):\n",
    "    interpreted_input = (\n",
    "        result[0][\"generated_text\"].split(\"### Assistant:\")[0].split(\"### Human:\")[1]\n",
    "    )\n",
    "    new_output = (\n",
    "        result[0][\"generated_text\"].split(\"### Assistant:\")[1].split(\")\")[0].strip()\n",
    "    )\n",
    "\n",
    "    new_output.split(\"## Opinion detected:\")\n",
    "\n",
    "    aspect_opinion_sentiment = new_output\n",
    "\n",
    "    aspects = aspect_opinion_sentiment.split(\"Aspect detected:\")[1].split(\"##\")[0]\n",
    "    opinions = aspect_opinion_sentiment.split(\"Opinion detected:\")[1].split(\n",
    "        \"## Sentiment detected:\"\n",
    "    )[0]\n",
    "    sentiments = aspect_opinion_sentiment.split(\"## Sentiment detected:\")[1]\n",
    "\n",
    "    aspect_list = [aspect.strip() for aspect in aspects.split(\",\") if \",\" in aspects]\n",
    "    opinion_list = [\n",
    "        opinion.strip() for opinion in opinions.split(\",\") if \",\" in opinions\n",
    "    ]\n",
    "    sentiments_list = [\n",
    "        sentiment.strip() for sentiment in sentiments.split(\",\") if \",\" in sentiments\n",
    "    ]\n",
    "    phrases = [\n",
    "        opinion + \" \" + aspect for opinion, aspect in zip(opinion_list, aspect_list)\n",
    "    ]\n",
    "\n",
    "    output_dict = {\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"interpreted_input\": interpreted_input,\n",
    "        \"aspects\": aspect_list,\n",
    "        \"opinions\": opinion_list,\n",
    "        \"sentiments\": sentiments_list,\n",
    "        \"phrases\": phrases,\n",
    "    }\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "def process_prompt(user_prompt, model):\n",
    "    edited_prompt = \"### Human: \" + user_prompt + \".###\"\n",
    "    pipe = pipeline(\n",
    "        task=\"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=len(tokenizer.encode(user_prompt)) * 4,\n",
    "    )\n",
    "    result = pipe(edited_prompt)\n",
    "\n",
    "    output_dict = process_output(result, user_prompt)\n",
    "    return result, output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discover.infra.utils.file.io import IOService\n",
    "\n",
    "\n",
    "fp = \"workspace/test/dataset/01_dataprep/appvocai_discover-01_dataprep-05_clean-review-dataset.parquet\"\n",
    "df = IOService.read(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def apply_to_dataframe(reviews, base_model):\n",
    "    \"\"\"\n",
    "    Processes a list of reviews and returns a DataFrame in the desired structure.\n",
    "\n",
    "    Args:\n",
    "        reviews (list of dict): A list where each dict contains `id` and `text` keys.\n",
    "        base_model (object): The base model used for text processing.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with `review_id`, `aspects`, `opinions`, and `sentiments`.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for review in tqdm(reviews):\n",
    "        review_id = review[\"id\"]\n",
    "        _, output_dict = process_prompt(review[\"content\"], base_model)\n",
    "\n",
    "        # Unpack the aspects, opinions, and sentiments into individual rows\n",
    "        for aspect, opinion, sentiment in zip(\n",
    "            output_Dict[\"aspects\"], output_Dict[\"opinions\"], output_Dict[\"sentiments\"]\n",
    "        ):\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"id\": review_id,\n",
    "                    \"aspects\": aspect,\n",
    "                    \"opinions\": opinion,\n",
    "                    \"sentiments\": sentiment,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Create the DataFrame\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Assuming `base_model` is already defined\n",
    "df = apply_to_dataframe(df, base_model)\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appvocai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
