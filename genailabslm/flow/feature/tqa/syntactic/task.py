#!/usr/bin/env python3
# -*- coding:utf-8 -*-
# ================================================================================================ #
# Project    : GenAI-Lab-SLM                                                                       #
# Version    : 0.1.0                                                                               #
# Python     : 3.10.14                                                                             #
# Filename   : /genailabslm/flow/feature/tqa/syntactic/task.py                                     #
# ------------------------------------------------------------------------------------------------ #
# Author     : John James                                                                          #
# Email      : john@variancexplained.com                                                           #
# URL        : https://github.com/variancexplained/genai-lab-slm                                   #
# ------------------------------------------------------------------------------------------------ #
# Created    : Sunday January 19th 2025 11:53:03 am                                                #
# Modified   : Saturday January 25th 2025 04:41:08 pm                                              #
# ------------------------------------------------------------------------------------------------ #
# License    : MIT License                                                                         #
# Copyright  : (c) 2025 John James                                                                 #
# ================================================================================================ #
"""Syntactic Text Quality Analysis Task Module"""
import logging

import pyspark.sql.functions as F
from genailabslm.flow.base.task import Task
from genailabslm.infra.service.logging.task import task_logger
from pyspark.sql import DataFrame, Row
from pyspark.sql.types import ArrayType, FloatType, IntegerType

# ------------------------------------------------------------------------------------------------ #
#                                    PHRASE COUNTS                                                 #
# ------------------------------------------------------------------------------------------------ #


class PhraseCount(Task):
    """Counts phrases extracted by a Spark NLP Chunker.

    This class takes a DataFrame with a column containing arrays of phrases (e.g.,
    noun phrases, verb phrases) generated by a Spark NLP Chunker and adds a new
    column with the count of those phrases for each row. Optionally, the counts
    can be log-normalized.

    Attributes:
        _column (str): The name of the column containing the phrase arrays.
        _new_column (str): The name of the new column to store the phrase counts.
        _normalized (bool): Whether to log-normalize the counts (default: True).
        _logger (logging.Logger): Logger instance for the class.

    Args:
        column (str): The name of the column containing the phrase arrays.
        new_column (str): The name of the new column to store the phrase counts.
        normalized (bool, optional): Whether to log-normalize the counts. Defaults to True.

    Raises:
        ValueError: If the specified `column` is not found in the DataFrame.
        TypeError: If the specified `column` is not an array type.

    """

    def __init__(
        self,
        column: str,
        new_column: str,
        normalized: bool = True,
        **kwargs,
    ) -> None:
        super().__init__(**kwargs)
        self._column = column
        self._new_column = new_column
        self._normalized = normalized
        self._kwargs = kwargs

        self._logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

    @task_logger
    def run(self, data: DataFrame) -> DataFrame:
        """Counts phrases and adds the counts to a new column.

        This method counts the number of phrases in the specified column of the
        input DataFrame and adds a new column containing these counts. The counts
        can be optionally log-normalized by adding 1 to the count before taking
        the logarithm.

        Args:
            data (DataFrame): A Spark DataFrame containing the phrase arrays.

        Returns:
            DataFrame: The input DataFrame with an additional column containing
                the phrase counts.

        """

        if self._column not in data.columns:
            raise ValueError(f"Column '{self._column}' not found in DataFrame.")

        if not isinstance(
            data.schema[self._column].dataType, ArrayType
        ):  # Check if column is array. Not strictly necessary but good practice
            raise TypeError(f"Column '{self._column}' is not an array type.")

        data = data.withColumn(
            self._new_column, F.size(F.col(self._column))
        ).withColumn(self._new_column, F.col(self._new_column).cast(IntegerType()))

        if self._normalized:
            data = data.withColumn(self._new_column, F.log(F.col(self._new_column) + 1))

        return data


# ------------------------------------------------------------------------------------------------ #
class NounPhraseCount(PhraseCount):
    def __init__(self, column, new_column, normalized=True, **kwargs):
        super().__init__(column, new_column, normalized, **kwargs)


# ------------------------------------------------------------------------------------------------ #
class AdjectiveNounPairCount(PhraseCount):
    def __init__(self, column, new_column, normalized=True, **kwargs):
        super().__init__(column, new_column, normalized, **kwargs)


# ------------------------------------------------------------------------------------------------ #
class AspectVerbPairCount(PhraseCount):
    def __init__(self, column, new_column, normalized=True, **kwargs):
        super().__init__(column, new_column, normalized, **kwargs)


# ------------------------------------------------------------------------------------------------ #
class AdverbPhraseCount(PhraseCount):
    def __init__(self, column, new_column, normalized=True, **kwargs):
        super().__init__(column, new_column, normalized, **kwargs)


# ------------------------------------------------------------------------------------------------ #
#                            ASPECT AND RELATED VERB COUNTS                                        #
# ------------------------------------------------------------------------------------------------ #
class AspectVerbCount(Task):

    def __init__(
        self,
        column: str = "tqa_syntactic_dependencies",
        new_column: str = "tqa_syntactic_aspect_related_verb_count",
        normalized: bool = True,
        **kwargs,
    ) -> None:
        super().__init__(**kwargs)
        self._column = column
        self._new_column = new_column
        self._normalized = normalized
        self._kwargs = kwargs

        self._logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

    @task_logger
    def run(self, data: DataFrame) -> DataFrame:

        @F.udf(returnType=FloatType())
        def count_aspect_related_verbs(annotations: Row, normalized: bool) -> float:
            """
            Counts aspect-related verbs based on dependency parsing results.

            Args:
                annotations: A Row object containing token, pos, and dependency annotations.
                normalized: Whether to apply log normalization.

            Returns:
                The count of aspect-related verbs (float), or 0.0 if input is invalid.
            """
            if not annotations:
                return 0.0

            try:
                tokens = annotations["tqa_syntactic_token"]
                pos_tags = annotations["tqa_syntactic_pos"]
                dependencies = annotations["tqa_syntactic_dependencies"]

                if not tokens or not pos_tags or not dependencies:
                    return 0.0

                aspect_verbs = 0
                num_tokens = len(tokens)
                for dep in dependencies:
                    if dep is None:
                        continue
                    if (
                        hasattr(dep, "relation")
                        and hasattr(dep, "governor")
                        and hasattr(dep, "dependent")
                    ):
                        if dep.relation in [
                            "nsubj",
                            "dobj",
                            "iobj",
                            "advcl",
                            "xcomp",
                            "ccomp",
                            "acl",
                        ]:  # More relevant dependencies for ABSA
                            governor_index = dep.governor - 1  # 0-based indexing
                            dependent_index = dep.dependent - 1  # 0-based indexing

                            if (
                                0 <= governor_index < num_tokens
                                and 0 <= dependent_index < num_tokens
                            ):
                                governor_pos = pos_tags[governor_index]
                                dependent_pos = pos_tags[dependent_index]

                                if dependent_pos.startswith(
                                    "NN"
                                ) and governor_pos.startswith("VB"):
                                    aspect_verbs += 1
                                elif governor_pos.startswith(
                                    "NN"
                                ) and dependent_pos.startswith("VB"):
                                    aspect_verbs += 1
                    else:
                        print(f"Malformed dependency object: {dep}")
                        continue
                if normalized:
                    return F.log(aspect_verbs + 1)
                else:
                    return aspect_verbs

            except (IndexError, TypeError, AttributeError, KeyError) as e:
                print(f"Error processing annotations: {e}. Annotations: {annotations}")
                return 0.0
            except Exception as e:
                print(f"An unexpected error occurred: {e}. Annotations: {annotations}")
                return 0.0

        data = data.withColumn(
            "tqa_aspect_related_verb_count",
            count_aspect_related_verbs(
                F.struct(
                    *[
                        F.col(x + ".result")
                        for x in [
                            "tqa_syntactic_token",
                            "tqa_syntactic_pos",
                            "tqa_syntactic_dependencies",
                        ]
                    ]
                ),
                F.lit(self._normalized),  # Pass the normalization flag as a literal
            ),
        )

        return data


# ------------------------------------------------------------------------------------------------ #
#                                      POS COUNTS                                                  #
# ------------------------------------------------------------------------------------------------ #
class POSCount(Task):
    """Counts occurrences of a specific POS tag in a DataFrame column.

    This class takes a DataFrame with a column containing arrays of POS tags
    and adds a new column with the count of a specified POS tag. The counts
    can be optionally log-normalized.

    Attributes:
        _column (str): The name of the column containing the POS tag arrays.
        _new_column (str): The name of the new column to store the POS tag counts.
        _pos (str): The POS tag to count (e.g., "NN", "VB", "JJ").
        _normalized (bool): Whether to log-normalize the counts (default: True).
        _logger (logging.Logger): Logger instance for the class.

    Args:
        column (str): The name of the column containing the POS tag arrays.
        new_column (str): The name of the new column to store the POS tag counts.
        pos (str): The POS tag to count (e.g., "NN", "VB", "JJ").
        normalized (bool, optional): Whether to log-normalize the counts. Defaults to True.

    Raises:
        ValueError: If the specified `column` is not found in the DataFrame.
        TypeError: If the specified `column` is not an array type.
    """

    def __init__(
        self, column: str, new_column: str, pos: str, normalized: bool = True, **kwargs
    ):
        super().__init__(**kwargs)
        self._column = column
        self._new_column = new_column
        self._pos = pos
        self._normalized = normalized
        self._logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

    @task_logger
    def run(self, data: DataFrame) -> DataFrame:
        """Counts POS tags and adds the counts to a new column.

        This method counts the occurrences of the specified POS tag in the
        given column of the input DataFrame and adds a new column containing
        these counts. The counts can be optionally log-normalized.

        Args:
            data (DataFrame): A Spark DataFrame containing the POS tag arrays.

        Returns:
            DataFrame: The input DataFrame with an additional column containing
                the POS tag counts.
        """
        if self._column not in data.columns:
            raise ValueError(f"Column '{self._column}' not found in DataFrame.")

        if not isinstance(data.schema[self._column].dataType, ArrayType):
            raise TypeError(f"Column '{self._column}' is not an array type.")

        # Count occurrences of the specific POS tag
        data = data.withColumn(
            self._new_column,
            F.aggregate(
                F.col(self._column),
                F.lit(0),
                lambda acc, x: acc
                + F.when(x.result.startswith(self._pos), 1).otherwise(0),
            ).cast(IntegerType()),
        )

        if self._normalized:
            data = data.withColumn(self._new_column, F.log(F.col(self._new_column) + 1))

        return data


# ------------------------------------------------------------------------------------------------ #
class NounCount(POSCount):
    def __init__(self, column, new_column, pos, normalized=True, **kwargs):
        super().__init__(column, new_column, pos, normalized, **kwargs)


# ------------------------------------------------------------------------------------------------ #
class VerbCount(POSCount):
    def __init__(self, column, new_column, pos, normalized=True, **kwargs):
        super().__init__(column, new_column, pos, normalized, **kwargs)


# ------------------------------------------------------------------------------------------------ #
class AdverbCount(POSCount):
    def __init__(self, column, new_column, pos, normalized=True, **kwargs):
        super().__init__(column, new_column, pos, normalized, **kwargs)


# ------------------------------------------------------------------------------------------------ #
class AdjectiveCount(POSCount):
    def __init__(self, column, new_column, pos, normalized=True, **kwargs):
        super().__init__(column, new_column, pos, normalized, **kwargs)
